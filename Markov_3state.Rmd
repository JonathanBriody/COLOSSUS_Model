---
title: 'Exploratory Cost-Effectiveness Analysis of Treatment Based on New Diagnostic Tests vs Standard of Care in Microsatellite Stable RAS Mutant Metastatic Colorectal Cancer Patients'
author: "Jonathan Briody (1) | Kathleen Bennett (1) | Lesley Tilson (2)"
output:
  html_document: default
  pdf_document: default
editor_options: 
  markdown: 
    wrap: sentence
bibliography: references.bib
---

(1) Data Science Centre, RCSI University of Medicine and Health Sciences, Dublin, Ireland
(2) National Centre for Pharmacoeconomics, St James Hospital, Dublin 8, Ireland

**Co-authors:**

*Annette feels Rodrigo should be on this paper as he suggested considering the Angiopredict study. Annette said that as Matias Ebert, a gastroenterologist in Germany, is on both COLOSSUS and Angiopredict, he should be on the paper. This collaboration may also help us to get some of the cost data that we've been waiting on from Germany.*

*Annette feels that all the PIs on Angiopredict should be co-authors on this paper Matthias Ebert as mentioned above, Jochen Prehn and Bauke Ylstra. I would probably like to add Qiushi Chen, dependent on how helpful his thoughts on parametric survival analysis actually turn out to be.*

IF THIS BECOMES THE BEVACIZUMAB STUDY WE CAN INCOPORATE SOME OF THE BELOW:

<https://www.angiopredict.com/go/about_us>

Smeets, D., Miller, I. S., O'Connor, D. P., Das, S., Moran, B., Boeckx, B., ... & Byrne, A. T.
(2018).
Copy number load predicts outcome of metastatic colorectal cancer patients receiving bevacizumab combination therapy.
*Nature communications*, *9*(1), 1-16.

# **Abstract \# For your write-up on Bevacizumab, and particularly for the part indicating it's not cost effective, you should use these 3 studies: Kristin, E., Endarti, D., Khoe, L. C., Taroeno-Hariadi, K. W., Trijayanti, C., Armansyah, A., & Sastroasmoro, S. (2021). Economic evaluation of adding bevacizumab to chemotherapy for metastatic colorectal cancer (mCRC) patients in Indonesia. Asian Pacific Journal of Cancer Prevention: APJCP, 22(6), 1921. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8418847/pdf/APJCP-22-1921.pdf> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., El-Rayes, B. F., & Flowers, C. R. (2015). First-and second-line bevacizumab in addition to chemotherapy for metastatic colorectal cancer: a United States--based cost-effectiveness analysis. Journal of Clinical Oncology, 33(10), 1112. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4881313/pdf/zlj1112.pdf> Goldstein, D. A., Chen, Q., Ayer, T., Chan, K. K., Virik, K., Hammerman, A., ... & Hall, P. S. (2017). Bevacizumab for Metastatic Colorectal Cancer: A Global Cost‐Effectiveness Analysis. The Oncologist, 22(6), 694-699. <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5469597/pdf/onco12164.pdf>:**

> I'll initially draft this as informed by:
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> Clinical colorectal cancer, 13(4), 219-225.
> <https://www.sciencedirect.com/science/article/abs/pii/S1533002814000978>
>
> **[TICK HERE:]**
>
> and:
>
> Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> Journal of Medical Economics, 24(1), 339-344.
> <https://www.tandfonline.com/doi/pdf/10.1080/13696998.2021.1888743>
>
> **[TICK HERE:]**
>
> and:
>
> Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
> (2019).
> Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
> Journal of Medical Economics, 22(2), 163-168.
> <https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>
>
> **[TICK HERE:]**
>
> and here:
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Ramalingam, S. S., ... & Flowers, C. R.
> (2015).
> Necitumumab in metastatic squamous cell lung cancer: establishing a value-based cost.
> JAMA oncology, 1(9), 1293-1300.
>
> Paper saved here: <file:///C:/Users/Jonathan/Dropbox/PC/Downloads/13%20Goldstein.pdf> and supplementary material saved here: <file:///C:/Users/Jonathan/Dropbox/PC/Downloads/Neci%20supplemental%20material%20071815.pdf>
>
> **[TICK HERE:]**
>
> and not forgetting I have some relevant stuff in Grammarly
>
> **[TICK HERE:]**
>
> Then I'll have a look at the other open journal articles in my browser windows
>
> **[TICK HERE:]**

**Background:** Colorectal cancer is the third most common cancer in Europe.
Microsatellite stable RAS mutant metastatic colorectal cancer (MSS RAS mt mCRC) [some details about this specific cancer]....
**Objectives:** The objective of this study was to compare the cost-effectiveness of a personalised medicine approach (evaluation of predictive biomarkers for targeted therapy) with standard of care treatment FOLFOX (5-FU, leucovorin, and oxaliplatin) in patients with MSS RAS mt mCRC in Germany, Ireland and Spain.

**Materials and Methods:** We developed a three-state Markov model with monthly cycles to estimate the incremental cost-effectiveness ratio (ICER) of biomarkers for targeted therapies versus FOLFOX for patients with MSS RAS mt mCRC from a healthcare payer perspective in the countries under study (Germany, Ireland and Spain).
Progression risks, cause-specific mortality and probabilities of adverse events were taken from the literature.
Costs for treatment administration and management of adverse events were derived from published data and survey of consortium hospitals in the participating countries.
Utilities applied to calculate the quality-adjusted life years (QALYs) were obtained from a review of the literature [I may want to return to this to update this with the country specific utility plan myself and Kathleen made].
To test the robustness of the results one-way sensitivity analysis and probabilistic sensitivity analysis was applied.
To test the robustness of the results, parameter values were varied individually to assess the sensitivity of the model to individual parameters or specific sets of parameters, and simultaneously, sampling multiple sets of parameter values from a priori--defined probability distributions.

**Results:** Predictive biomarkers with targeted therapy provided X QALYS at a cost of €X,000 compared with standard of care (FOLFOX without testing).
Compared to standard of care, the ICER of predictive biomarkers with targeted therapy provided was EUR X,000/QALY from a European health care perspective, which was above the threshold used to define cost-effectiveness in the countries studied, which provided X QALYS at a cost of €X,000.
The incremental cost-effectiveness ratio (ICER) was €X,000 per QALY.
The ICER was below the cost-effectiveness threshold (\<€X,000 per QALY) in all univariate and multivariate sensitivity analyses.
One-way sensitivity analysis demonstrated that the initial results were generally robust.
Cost-effectiveness acceptability curves from the probabilistic sensitivity analysis show that the probability that predictive biomarkers with targeted therapy was cost-effective was 100% when the threshold was \<€X,000 per QALY.

**Conclusion:** When compared to standard of care, predictive biomarkers with targeted therapy is cost effective for patients with MSS RAS mt mCRC in Germany, Ireland and Spain at a €X,000 per QALY threshold.
However, because of the exploratory nature of this analysis, the cost effectiveness profile of these biomarkers should be evaluated in further comparative studies.

**Keywords:** Colorectal neoplasms, cost effectiveness analysis, Quality-adjusted life years, Markov model, mCRC, precision medicine, biomarkers, assays,

JEL CLASSIFICATION CODES

I19, I1, I, I11, I1, I,

[*Possible Journal Homes:*]{.underline}

Value in Health: BEST POSSIBLE HOME, HIGHEST RATED JOURNAL, FOCUSED ON CEA, ETC.,

Journal of Medical Economics <https://www.tandfonline.com/doi/pdf/10.1080/13696998.2021.1888743>

Clinical Colorectal Cancer <https://www-sciencedirect-com.proxy.library.rcsi.ie/science/article/pii/S1533002814000978>

Studies in Health Technology andInformatics

Search "New Journal: Oxford Open Economics" in my gmail, it's a new Journal and thus may be interested in this work.

We can increase our publication numbers by publishing a short summary of our study in: PharmacoEconomics & Outcomes News - per the description "Stay up to date with the exciting world of health economics. Solve the problem of reading and monitoring the vast volumes of literature necessary to stay informed on the latest issues in pharmacoeconomics and outcomes research by reading our concise news summaries, compiled in an easy-to-read format." <https://www.springer.com/journal/40274>

**Correspondence:**

Jonathan Briody, PhD, RCSI University of Medicine and Health Sciences, Beaux Lane House, Lower Mercer St, Dublin 2, Ireland.
Email: [jonathanbriody\@rcsi.ie](mailto:jonathanbriody@rcsi.ie){.email}

**Funding Information:**

This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 754923.
The material presented and views expressed here are the responsibility of the author(s) only.
The EU Commission takes no responsibility for any use made of the information set out.

\newpage

# Introduction

We used the World Health Organization's Choosing Interventions that are Cost--Effective (WHO-CHOICE) suggestion that "interventions for a given country or region that cost less than three times average per capita income per disability-adjusted life-year (DALY) averted are considered costeffective; and those that exceed this level are considered not cost-effective" (estimation from CAPMAS)29.
The three times GDP per capita in Egypt in 2017 was EGP106,000, which corresponds to an incremental cost-effectiveness ratio (ICER) threshold of USD 41,372 (2017)30.
Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
(2019).
Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
Journal of Medical Economics, 22(2), 163-168.
<https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>

"The WTP threshold was set as three times the GDP per capita in 2017, which was \$10,800/QALY in Indonesia."

Kristin, E., Endarti, D., Khoe, L. C., Taroeno-Hariadi, K. W., Trijayanti, C., Armansyah, A., & Sastroasmoro, S.
(2021).
Economic evaluation of adding bevacizumab to chemotherapy for metastatic colorectal cancer (mCRC) patients in Indonesia.
*Asian Pacific Journal of Cancer Prevention: APJCP*, *22*(6), 1921.

<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Evidence%20Synthesis/Economic%20Models/Kristin%20et%20al_2021_Economic%20Evaluation%20of%20Adding%20Bevacizumab%20to%20Chemotherapy%20for%20Metastatic.pdf>

three times the GDP per capita of China Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China tandfonline.com/doi/pdf/10.1080/13696998.2021.1888743

Maybe for \*\*Germany\*\*, where they don't appear to have a cost-effectiveness threshold, I can use the GDP per capita approach I've read about (somewhere) or some range of this.
Rough description of this here: Marseille, E., Larson, B., Kazi, D. S., Kahn, J. G., & Rosen, S.
(2014).
Thresholds for the cost--effectiveness of interventions: alternative approaches.
Bulletin of the World Health Organization, 93, 118-124.
<https://www.who.int/bulletin/volumes/93/2/14-138206/en/> Health Opportunity cost may also factor into this: Ochalek, J., Lomas, J., & Claxton, K.
(2018).
Estimating health opportunity costs in low-income and middle-income countries: a novel approach and evidence from cross-country data.
BMJ Global Health, 3(6).
<https://gh.bmj.com/content/3/6/e000964>

This study also didnt have thresholds, much like Germany doesnt: Franken, M. D., Van Rooijen, E. M., May, A. M., Koffijberg, H., van Tinteren, H., Mol, L., ... & van Oijen, M. G. H.
(2017).
Cost-effectiveness of capecitabine and bevacizumab maintenance treatment after first-line induction treatment in metastatic colorectal cancer.
European Journal of Cancer, 75, 204-212.
<https://sci-hub.ru/10.1016/j.ejca.2017.01.019>

With almost 2 million new cases diagnosed annually, colorectal cancer is the third most commonly diagnosed cancer in men, the second most commonly diagnosed in women [@vabi2021], and the second leading cause of cancer deaths worldwide [@nwaokorie2021].
Following advances in primary and adjuvant treatments, survival time continues to improve in colorectal cancer, with a 5-year survival rate of 64%, however, this is not reflected in metastatic colorectal cancer (mCRC), which has a dramatically lower 5-year survival rate of just 12% [@siegel2019].

As one of the most commonly mutated oncogenes in cancer, RAS mutations appear in around half of mCRC patients, and in microsatellite stable (MSS) mCRC are associated with more aggressive tumor biology and poorer prognosis [@patelli2021].
Current front-line treatment for mCRC does not differ significantly from colorectal cancer regardless of mutational status, comprising of 5-fluoruracil (5-FU)-based standard of care chemotherapy (e.g. FOLFOX, FOLFIRI, FOLFOXFIRI) in combination with bevacizumab.
However, there is emerging evidence that targeted therapy may prolong overall survival for mCRC patients, as previously seen in non-metastatic colorectal cancer [@xie2020].

Cancerous cells may be addressed by targeted therapy which directly inhibits cell proliferation, differentiation, and migration.
Similarly, the micro-environment of the tumor, including it's local blood vessels and immune cells, may be modified in order to disrupt growth and to improve immune surveillance and response [@tiwari2018].
Beginning with Cetuximab in 2004, numerous targeted therapies for colorectal cancer have successfully come to market in the last two decades (Bevacizumab, Panitumumab, Ziv-aflibercept, Regorafenib, Ramucirumab, Pembrolizumab, Bivolumabm Ipilimumab, etc.,).
The number of pathways offering possible opportunities for targeted therapy is similarly extensive, including epidermal growth factor receptor (EGFR), vascular endothelial growth factor/vascular endothelial growth factor receptor (VEGF/VEGFR), and hepatocyte growth factor/mesenchymal--epithelial transition factor (HGF/c-MET), among others [@xie2020].

*DRUG X* is a *DETAILS ABOUT DRUG X HERE* that *HOW DRUG X WORKS HERE* [citation to support this].
This experimentally identified targeted agent is in preclinical status or in phase I trials, but has been proven to be efficient in clinical studies [amend if this isnt the case], and ... [citations to support this].
The results of X demonstrate a median overall survival (OS) benefit of X months (Y months vs. Z months) and a median progression-free survival (PFS) benefit of X months for DRUG X when compared with placebo (X months vs. X months).

> Fruquintinib is a VEGFR inhibitor that blocks new blood vessel growth associated with tumor proliferation5 .
> In 2018, it was approved to treat Chinese patients with mCRC who had experienced failure of second-line treatment.
> The results of the FRESCO trial demonstrated a median overall survival (OS) benefit of 2.7 months (9.3 months vs. 6.6 months) and a median progression-free survival (PFS) benefit of 1.9 months for fruquintinib when compared with placebo (3.7 months vs. 1.8 months)6.
>
> \- Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.

Because targeted therapy is most associated with prolonged survival, rather than cure, the cost--effectiveness of such therapy must be understood when compared to current standard of care chemotherapy, which is assumed to be significantly less costly than producing targeted agents and testing necessary genetic markers, particularly under the provision of multiple targeted agents [@dekker2019].

Limited data exist regarding these novel therapies, with no research comparing the cost-effectiveness of these targeted therapies with standard of care, thus, we developed a general decision-analytic framework applied to mCRC to model the cost-effectiveness of biomarkers for targeted therapies in mCRC.
We constructed a Markov model using a hypothetical cohort of patients with mCRC to conduct an exploratory cost-effectiveness analysis of targeted treatment based on new diagnostic tests vs standard of care in MSS RAS mt mCRC patients.

# Materials and Methods

#### Model Structure

We constructed Markov models to calculate the lifetime costs and outcomes of each treatment strategy.
As per Figure 1, in this model we established the following health states: (1) first line treatment prior to progression; (2) second line treatment following progression; (3) death.
We applied published estimates of costs associated with each health state in the countries studied [CITE].
All cost estimates were in 2022 Euros [maybe discuss how you converted to this?
HIQA has something on this I think].
To apply country-specific state utility values we...DETAIL HERE THE APPROACH WE TOOK AS DISCUSSED WITH KATHLEEN [CITE].
The model was programmed in R 4.1.2 using RStudio 2021.09.2+382 and was informed by the state-transition model framework developed by the Decision Analysis in R for Technologies in Health (DARTH) workgroup [@alarid-escudero2021] [@jalal2017] [@krijkamp2018] [@krijkamp2020].
The Markov model simulated the health state transitions following a diagnosis of metastatic cancer.
A cycle length of 1 month was chosen, per the time between administration of chemotherapy doses.
A 5-year time horizon was also chosen, this was appropriate to estimate life expectancy in this patient population [CITE].
For standard of care, we considered 2 lines of treatment, first line (FOLFOX) and second line (FOLFIRI).
For the comparator we also considered 2 lines of treatment (NEW THERAPY), then (FOLFIRI).
In both instances we made use of treatment-specific utility values.
At the end of each cycle, patients remained in their current health state, or transitioned to a more progressive state.
To reflect that transitions between states may occur at any point within each cycle, we applied a half-cycle correction to calculating life-years (LYs) {I AM DEFINITELY NOT GOING TO DO THIS, I JUST PUT IT IN HERE AS SOMETHING TO THINK ABOUT, CYCLES ARE ONLY A MONTH SO A CORRECTION OF 2 WEEKS IS PRETTY STUPID}.
Within cycle correction here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\\September-Workshop-main\\September-Workshop-main\\R\\Functions.R

> [A three-state Markov model (Figure 1) with monthly cycle could also have been used to represent the progression of mCRC, including PFS, progressive disease (PD), and death.
> All patients entered the model either receiving FOLFOX therapy or NOVEL therapy.
> After treatment discontinuation due to progession, best supportive care (BSC) was modeled.
> \*\*--\> So, here even doing that they still got a treatment that needs to be modelled in PFS].\*\*
>
> Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.

The intention-to-treat (ITT) population we modeled in this study is MSS RAS mt mCRC patients in Germany, Ireland and Spain, from a healthcare payer perspective in these countries.
Subsequently the analysis only included direct medical costs.
Effectiveness is reflected in life-years (LYs) and quality adjusted life years (QALYS).
Per the countries studied, a discount rate of 4% per annum was applied to cost input and health output parameters [citation here].
The primary outcome of the model was the total direct costs, LYs, QALYS, and the incremental cost-effectiveness ratio (ICER).

#### Patients and Treatment Regimens

In the absence of a randomised controlled trial making direct comparisons between standard of care and the proposed novel therapy, it was necessary to perform an indirect comparison using the data from two published studies [CITE].
We made use of published results from one study to estimate the clinical course for patients treated under standard of care and another to estimate outcomes for patients under novel therapy [@goldstein2014].
The standard of care arm was based on a study be ..et al., {FILL IN THE BLANKS HERE ON THE STUDY YOU BASED THIS ON} who compared...[cite].
The novel therapy arm was based on a X, Y, Z, study by ....et al., While these 2 studies did not have identical target populations...

> The PK arm was based on a phase II nonrandomized study by Capitain et al., who compared PK FOLFOX with BSA FOLFOX.9 The BSA arm was based on a phase III randomized study by Tournigand et al.14 Although the 2 studies did not have identical target populations, the survival outcomes of the baseline BSA arm of the 2 studies are comparable (Table 1).9,14,15 Moreover, the median PFS and OS were also similar to the FOLFOX arm in a similar study.15
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> Given that there was no single RCT with all treatment arms, it was necessary to perform an indirect comparison using the data from the FRESCO trial and the CONCUR trial6, 11.
> The baseline treatment for the indirect comparison was BSC taken from the FRESCO trial.
> The clinical effectiveness in patients in the BSC arm was also available from the CONCUR trial.
> This, however, was not considered appropriate for the baseline treatment because the survival curves reported by the CONCUR trial were distorted in the paper and were not suitable for recreating individual patient data.
> However, the hazard ratios (HRs) calculated by original data would not be affected by the distorted figures.
> The implicit assumption underlying the indirect comparison was that the baseline patient characteristics in the two studies were reasonably similar, which was shown to be true (Appendix Table 1).

[I THINK WE CAN TAKE THEIR APPROACH IN OUR MODEL, DO DIGITISING AND PARAMETRIC SURVIVAL ANALYSIS FOR STANDARD OF CARE, THEN TAKE A HAZARD RATIO FROM ANNETTE AND THE COLOSSUS STUDY FOR THE NEW INTERVENTION, THERE'S A LITTLE MORE ON HR'S BELOW and the following R code applies hazard ratios after doing parametric survival analysis, similar to what I plan to do: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\ISPOR WEBINAR Health Economic Modelling in R\\ISPOR_webinar_R-master\\ISPOR_webinar_R-master\\oncologySemiMarkov_illustration.R].

> The FRESCO trial was used to model the probabilities of remaining in the PFS state or experiencing disease progression in the fruquintinib arm.
> This research recreating individual patients data (IPD) from Kaplan--Meier (KM) survival curves in FRESCO by the method published by Guyot et al.12.
> To predict the events of death and progression beyond the observation period, several parametric models were fit in different distributions, including exponential, Weibull, Gompertz, generalized gamma log-normal and log-logistic.
> After comparing the goodness-of-fit of different parametric models by the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) (Appendix Table 2), assessing the conformities of parametric models to the original KM survival curves in the FRESCO trial and considering about proportional hazard model used for the indirect comparison, the Weibull distribution model was chosen for the base case analysis (Appendix Figure 1 and Appendix Figure 2).
>
> We used the PFS curves to estimate the transition probability from PFS state to PFS state and the transition probability from PFS state to another states (PD and death) and used the OS curve to estimate the overall mortality.
> The model assumed the transition probability from PFS state to death was estimated by the background mortality obtained from Chinese government data, which based on actuarial estimates of mortality among the general population in China.
> According to the background mortality and the transition probability from PFS state to another states (PD and death), we calculated the transition probability from PFS state and PD.
> Moreover, the transition probability from PD state to PD state could calculated by overall mortality and background mortality.
>
> We also calculated the transition probabilities in the BSC arm by a Weibull-distribution hazard function using the survival data from the FRESCO trial.
> The model assumed that the survival curves of BSC both in FRESCO and CONCUR would be identical if the patients in BSC arm was identical, and the HR of regorafenib versus BSC would not change even if the patients receiving BSC in CONCUR became exactly same as the patients receiving BSC in FRESCO.
> Hence,we adjusted the transition probabilities of the regorafenib arm using the transition probabilities of the BSC arm modeled from the FRESCO trial and the HR of regorafenib versus BSC taken from the CONCUR trial.
>
> Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.

> There is a discussion on indirect comparisons and how best to do these in this short piece: Martin-Broto, J.
> (2015).
> Indirect comparisons in cost-effectiveness analysis: are we being naïve?.
> Clinical and Translational Oncology, 17(1), 85-86.
> <https://sci-hub.se/10.1007/s12094-014-1256-9>
>
> This also talks about indirect comparisons Hollmann, S., Alloul, K., Attard, C., & Kavan, P. (2014).
> An Indirect Treatment Comparison and Cost-Effectiveness Analysis Comparing Folfirinox with Nab-Paclitaxel Plus Gemcitabine for First-Line Treatment for Patients with Metastatic Pancreatic Cancer.
> Annals of Oncology, 25, ii11.
> <https://sci-hub.se/10.1093/annonc/mdu164.18>
>
> This applies an indirect comparison in more detail: Tamoschus, D., Draexler, K., Chang, J., Ngai, C., Madin-Warburton, M., & Pitcher, A.
> (2017).
> Cost-effectiveness analysis of regorafenib for gastrointestinal stromal tumour (GIST) in Germany.
> Clinical Drug Investigation, 37(6), 525-533.
> <https://sci-hub.se/10.1007/s40261-017-0514-3>
>
> I'm not even sure if I would classify what we are doing as an indirect comparison, as we don't have data for the novel therapy at all (that I know of), but I'll read the above articles and then make a call on that.
> Likely I'll make the call to follow the approach of: Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.
> and use a HR, but I'll see.
>
> The following also shows how to do the above: Purmonen, T., Martikainen, J. A., Soini, E. J., Kataja, V., Vuorinen, R. L., & Kellokumpu-Lehtinen, P. L.
> (2008).
> Economic evaluation of sunitinib malate in second-line treatment of metastatic renal cell carcinoma in Finland.
> Clinical therapeutics, 30(2), 382-392.
> - <https://sci-hub.se/10.1016/j.clinthera.2008.02.013>
>
> ***Or we could try:***
>
> The transition probabilities were derived from the median time to radiologic progression and median OS obtained from previously published studies18,20 following recommended methods21.
> - Per: Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
> (2019).
> Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
> Journal of Medical Economics, 22(2), 163-168.
> <https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>
>
> I think the below also takes transition probabilities just from the literature, although I'd have to read it and see:
>
> Pierse, T., O'Neill, S., Dinneen, S. F., & O'Neill, C.
> (2021).
> A simulation study of the economic and health impact of a diabetes prevention programme in Ireland.
> *Diabetic Medicine*, *38*(6), e14540.

#### Transition Probabilities

> ##### Life Tables:
>
> Following significant review of the literature on CEA of mCRC it's pretty clear that it's extremely uncommon for published studies to make use of life tables, most likely because of the short life-span of mCRC patients.
> However, if I did need to make use of Life Tables for this model, I would utilise the following:
>
> WRITING:
>
> Non mCRC death was captured as a competing risk in the model.
> We use life tables in each country studied to estimate the background mortality, these data were derived from [CITE WHERE YOU GOT THE LIFE TABLES] for GIVE THE YEAR. The starting age of the cohort was X years, which is the median age of incident mCRC diagnosis, as reported by [SOME TRUST WORTHY CANCER REGISTRATION BODY].
>
> CODE:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.3 Practical exercise R
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\\August_24\\4_cSTM - time-dependent models_material
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\\August_25\\3_cSTM - history dependence_material
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.3 Practical exercise R
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A3_Presenting Simulation Results\\A3.3 Practical exercise R

# Parametric Survival Analysis:

Some more R code on parametric survival analysis here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Applied Cost Effectiveness Modeling with R\\rcea-rscripts\\05-psm

This study: Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Ramalingam, S. S., ... & Flowers, C. R.
(2015).
Necitumumab in metastatic squamous cell lung cancer: establishing a value-based cost.
JAMA oncology, 1(9), 1293-1300.
saved here: C:/Users/Jonathan/Dropbox/PC/Downloads/coi150070.pdf and particulary it's supplementary material, saved here: <file:///C:/Users/Jonathan/Dropbox/PC/Downloads/supplementary%20materials.pdf> explains how to use PFS and OS curves to generate transition probabilities from the first state into the second and from the first state into death, so, it tells you how to create both by taking the PFS probabilities and OS probabilities from eachother to get the progressing probability and using the OS probability to get the death probability, it also talks about AIC, BIC, etc.,

Something to consider in this is the following point:

"Model Validation We performed internal model validations demonstrating that the overall survival curves generated by the Markov model simulation closely approximated those presented in the SQUIRE trial14 and the fitted survivalmodels (see eFigure 3 and eFigure 4 in the Supplement)."

And I talked to **Qiushi Chen** about this in detail in email (I also described to Kathleen in email), so look to that conversation on how he approached this to make sure I am not making any mistakes.

This talks about AIC as well: Briggs, A., Sculpher, M., Dawson, J., Fitzpatrick, R., Murray, D., & Malchau, H.
(2003).
Modelling the cost-effectiveness of primary hip replacement: how cost-effective is the Spectron compared to the Charnley prosthesis.
CHE Technical Paper Series, 28.
<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Training%20Resources/Decision%20Modelling%20-%20Advanced%20Course/A2_Making%20Models%20Probabilistic/A2.1.2%20Distributions%20for%20parameters/gammadist/interquartile%20range.pdf>

A GREAT EXPLANATION OF PARAMETRIC SURVIVAL ANALYSIS, MATHEMATICAL NOTATION AND CODE CAN BE FOUND HERE: <https://rpubs.com/mbounthavong/survival_analysis_in_r> ALSO SAVED HERE: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\GitHub\\COLOSSUS_Model\\rpubs-com-mbounthavong-survival_analysis_in_r

Info here too: <https://devinincerti.com/2019/06/18/parametric_survival.html>

Survival Curves in R: <https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html> <https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_survival/BS704_Survival_print.html>

Progression and mortality probabilities were derived from the PFS and OS curves of the X study [CITE].
Data points were extracted from survival plots using Engauge Digitizer software, version 12.2, and these data points were used to fit parametric survival models [@mitchell2022].

To produce estimates of progression and mortality outside of the follow-up time in X study several parametric models were fit to a number of distributions, including exponential, Weibull, Gompertz, generalized gamma log-normal and log-logistic.
Statistical analyses demonstrated a good fit was provided for both curves by Weibull models, according to the Akaike information criterion (AIC), the Bayesian information criterion (BIC), Schwarz Bayesian criterion and visual inspection of the relative conformity of parametric models to the original Kaplan-Meier survival curves (Table A1 in the appendix, Figure A1 in the appendix).
Weibull models have been demonstrated to be more suitable for modeling events occurring in mCRC [@goldstein2014a], and support the application of proportional hazard models [@guan2021] such that the Weibull distribution model was chosen for the base case analysis.
Therefore, we estimated the transition probabilities for each cycle based on the fitted Weibull survival models.

The cycle length studied was 2 weeks, corresponding to the time between chemotherapy administrations.
Because PFS curves capture patients under progression and death, we estimate the progression risks, that is the transition probability from first to second line treatment, by subtracting the mortality risks \$r\_{i}\$ from these transition probabilities \$p\_{i}\$.
Thus, the probability of progression is = max(\$p\_{i}\$ - \$r\_{i}\$, 0), *i* = 1,2,\\cdots, per [@goldstein2015a].

Transition from first to second line treatment in each cycle was calculated based on the [@wang2020] [USE THIS PAPER TO DESCRIBE THE PARAMETRIC SURVIVAL ANALYSIS] formulation: 1−exp(\$*λ*[t−1]\^{γ}\$ − \$*λ*t\^{γ}\$), where the current stage in the model is t, and *λ* and *γ* are the estimated scale and shape parameters respectively.
The transition probability of second-line treatment into death is as informed by [@wen2016].

# 00 Set up general state of play

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = T)
# To knit this document every time it is run, you can change `eval` to `TRUE` in the above.

# If you wanted to compare two R files and see what the differences are between them, you can do so with this:

#install.packages("diffr")
#library(diffr)
# diffr("Markov_3state_Andy.Rmd", "Markov_3state.Rmd", contextSize = 500, minJumpSize = 500, width = NULL, wordWrap)
# The above options complicate things and then you have to scroll for ages to the right to get the second file, the below is much more tidy:
# diffr("Markov_3state_Andy.Rmd", "Markov_3state.Rmd")
```

```{r}
rm(list = ls())  
# clear memory (removes all the variables from the work space)
options(scipen=999) 
# turns scientific notation off
# options(scipen=0) turns it back on, per: https://stackoverflow.com/questions/5352099/how-can-i-disable-scientific-notation
```

# 01 Load all packages

```{r}
# Joshua's Package Manager:

# Package names

packages <- c("pacman", "flexsurv", "MASS", "dplyr", "devtools", "scales", "ellipse", "ggplot2", "lazyeval", "igraph", "ggraph", "reshape2", "knitr", "stringr", "diagram", "survival", "lubridate", "ggsurvfit", "gtsummary", "tidycmprsk", "magrittr", "dplyr", "diffr")

# Install packages not yet installed

installed_packages <- packages %in% rownames(installed.packages())

if (any(installed_packages == FALSE)) {

  install.packages(packages[!installed_packages])

}

# Packages loading

invisible(lapply(packages, library, character.only = TRUE))

p_load_gh("DARTH-git/dampack", "DARTH-git/darthtools")
```

```{r}

# Check whether the required packages are installed and, if not, install the missing packages
# # - the 'pacman' package is used to conveniently install other packages
# if (!require("pacman")) install.packages("pacman"); library(pacman)
# p_load("flexsurv", "MASS", "dplyr", "devtools", "scales", "ellipse", "ggplot2", "lazyeval", "igraph", "ggraph", "reshape2", "knitr", "stringr", "diagram")   
# p_load_gh("DARTH-git/dampack", "DARTH-git/darthtools")
```

# 02 Individual Data for Parametric Code

```{r}
# Load the individual patient data for the time-to-progression (TTP) that you recovered by digitising published survival curves

# df_TTP <- read.csv(file='PFS.csv', header=TRUE)
# save.image("C:/Users/Jonathan/OneDrive - Royal College of Surgeons in Ireland/COLOSSUS/R Code/GitHub/COLOSSUS_Model/df_TTP.RData")
# df_TTD <- read.csv(file='OS.csv', header=TRUE)
# save.image("C:/Users/Jonathan/OneDrive - Royal College of Surgeons in Ireland/COLOSSUS/R Code/GitHub/COLOSSUS_Model/df_TTD.RData")

load(file = "df_TTP.RData")
# Load the individual patient data for the time-to-death (TTD) that you recovered by digitising published survival curves
load(file = "df_TTD.RData")
# At the moment I don't have time-to-death (TTD) data, and creating a fake dataset by duplicating time-to-progression (TTP) won't work as R will recognise the duplication and refuse to load the data, but when I DO have time-to-death (TTD) data I'll load it in here.

# Here's how to remove the BVZ rows from the excel files we have: https://techcommunity.microsoft.com/t5/excel/deleting-rows-that-contain-specific-content/m-p/2084473

# At the moment this is just nonsense data and needs to be replaced when I have figured out the issues described at the end of this section.

# When I do have data, the data needs to be set up to include a column for the time (in the example this is time in years) and a status indicator whether the time corresponds to an event, i.e. progression (status = 1), or to the last time of follow up, i.e. censoring (status = 0),

# "What is Censoring? Censoring in a study is when there is incomplete information about a study participant, observation or value of a measurement. In clinical trials, it's when the event doesn't happen while the subject is being monitored or because they drop out of the trial." - https://www.statisticshowto.com/censoring/#:~:text=What%20is%20Censoring%3F,drop%20out%20of%20the%20trial.

# https://en.wikipedia.org/wiki/Survival_analysis#:~:text=or%20q%20%3D%200.99.-,Censoring,is%20common%20in%20survival%20analysis.

# At the bottom of the parametric survival code I think about how time from individual patient data may be changed to match the time of our cycles.

# I think the digitiser will give me the time in the context of the survival curves I am digitising, i.e., time in weeks, or time in months or time in years.

# Then I will have to set-up my time accordingly in the R code so that my cycle length is at the same level as the individual patient data.

# That is, in Koen's example:

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\ISPOR WEBINAR Health Economic Modelling in R\ISPOR_webinar_R-master

# the data includes a column for the time (in years)
# t_cycle <- 1/4      # cycle length of 3 months (in years)                                # n_cycle <- 60       # number of cycles (total runtime 15 years)
  

# So I would have my colum for time, in the [TIME] the graph I was digitising used.
# Then I would create my cycle length of X weeks (in [TIME] the graph I was digitising used)
# Then I would have my number of cycles that would add up to give me a total runtime of how long I want to run the model for.
# So, above Koen wanted to run the model for 15 years, but his cycles were in 3 months, or each cycle was a quarter of a year, so 60 quarters, or 60 3 month cycles, is 15 years.
```

# 03 Parametric Survival Analysis Model Plan

```{r}

# We will implement a semi-Markov model with time-dependent transition probabilities, via a parametric survival model fitted to some individual patient data for the time-to-progression (TTP) and time-to-death (TTD) for standard of care.

# A hazard ratio for the new intervention therapy vs. the standard of care will then be applied to obtain transition probabilities for the new experimental strategy.

# Let's first review the survival curves and see if they match the study:

# Simple survival curves:

# km_fit_PFS <- survfit(Surv(time, status) ~ 1, data=df_TTP)
# plot(km_fit_PFS)
# 
# km_fit_OS <- survfit(Surv(time, status) ~ 1, data=df_TTD)
# plot(km_fit_OS)

# My understanding of survival curves is supported by: https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html

#install.packages(c("survival", "lubridate", "ggsurvfit", "gtsummary", "tidycmprsk"))
#remotes::install_github("zabore/condsurv")
#remotes::install_github("zabore/ezfun")
# library(survival)
# library(lubridate)
# library(ggsurvfit)
# library(gtsummary)
# library(tidycmprsk)
# #library(condsurv)
# #install.packages("magrittr") # package installations are only needed the first time you use it
# #install.packages("dplyr")    # alternative installation of the %>%
# library(magrittr) # needs to be run every time you start R and want to use %>%
# library(dplyr)    # alternatively, this also loads %>%


# We want to make sure the data we're feeding in matches the data from the publication in Table 4d and Supplementary Table 10d


# A really good intro to survival curves is here: https://shariq-mohammed.github.io/files/cbsa2019/1-intro-to-survival.html


survfit2(Surv(time, status) ~ 1, data = df_TTP) %>% ggsurvfit() +
  labs(
    x = "Days",
    y = "Progression Free survival probability"
  ) + 
  add_risktable()


km_fit_PFS <- survfit(Surv(time, status) ~ 1, data=df_TTP)
summary(km_fit_PFS, times = c(0,150,300,450, 600, 750, 900, 1050, 1200, 1350)) # This will stop just before the last patient is censored in the data, also, n.event means number of events that happened at this time.


survfit2(Surv(time, status) ~ 1, data = df_TTD) %>% ggsurvfit() +
  labs(
    x = "Days",
    y = "Overall survival probability"
  ) + 
  add_risktable()


km_fit_OS <- survfit(Surv(time, status) ~ 1, data=df_TTD)
summary(km_fit_OS, times = c(0,300,600,900,1200,1500,1800,1810,1820,1830,1840,1850,1860,1870,1880,1890,1891,1892,1893,1894,1895,1896,1897,1898,1899,1900,1901,2000,2010))


# The PFS and OS curves I create, match those from the publication, so we know the data we are feeding in is correct.

# So, the problem must be in how I am generating probabilities in my model.



# I calculate the probability of not progressing at the different time periods in the data from the publication, to contrast this to the probabilities I create of not progressing at different time periods once I've created probabilities in the R code, the probability of survival at each time period will be reported under the "survival" heading, per: https://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html 

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 0)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 150)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 300)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 450)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 600)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 750)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 900)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 1050)

summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 1200)



# I can repeat this for TTD:


summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 0)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 150)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 300)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 450)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 600)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 750)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 900)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 1050)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 1200)

summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 1350)


summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 1500)





# Do the Weibull probabilities match the probabilities created directly by the Kaplan-Meier:

# Here's how I checked if the Weibull probabilities match the probabilities created directly by the Kaplan-Meier, basically, I take the Kaplan-Meier probabilities at a few different cycles directly from the data as below, the probability of survival at each time period will be reported under the "survival" heading:

# I do this first for TTP:

# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 0)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 14)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 28)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 42)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 56)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 70)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 84)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 98)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTP), times = 112)



# Then, I calculate the probability of staying in the PFS state, which is what the above gives me. To do this, I create all the other probabilities as zero because I just want to look at whats going on with my Weibull probabilities for the PFS curve, I also put it in the first slot, as the way it works is that it needs to be multiplied by 1, and m_M_SoC has a cohort trace with 1 in in the first slot, ready to be matrix multiplied by m_P_SoC with it's probabilities. It's 1- because the probabilities created above from the PFS curve are probabilities of staying in PFS, not the probabilities of moving from PFS to OS.

# m_P_SoC["PFS", "PFS",]<- (1 -p_PFSOS_SoC)
# m_P_SoC["PFS", "OS",]<- 0
# m_P_SoC["PFS", "Dead",]<-0
# 
# # Setting the transition probabilities from OS
# m_P_SoC["OS", "OS", ] <- 0
# m_P_SoC["OS", "Dead", ]        <- 0
# 
# 
# # Setting the transition probabilities from Dead
# m_P_SoC["Dead", "Dead", ] <- 0


# So here I once again create the Markov cohort trace by looping over all cycles
# - note that the trace can easily be obtained using matrix multiplications
# - note that now the right probabilities for the cycle need to be selected, like I explained above. (this is all just the comments written from where I actually do the analysis).
# for(i_cycle in 1:(n_cycle-1)) {
#   m_M_SoC[i_cycle + 1, ] <- m_M_SoC[i_cycle, ] %*% m_P_SoC[ , , i_cycle]
#   m_M_Exp[i_cycle + 1, ] <- m_M_Exp[i_cycle, ] %*% m_P_Exp[ , , i_cycle]
# }


# head(m_M_SoC)  # print the first few lines of the matrix for standard of care (m_M_SoC)
# head(m_M_Exp)  # print the first few lines of the matrix for experimental treatment(m_M_Exp)

# looking at the cohort trace -> m_M_SoC, you see that the proportions in the PFS state from 100 are basically identical to the probabilities of being in those states created from the Kaplan Meier, again starting from 100% of people under study, thus, the Weibull probabilities created match the Kaplan-Meier.

# They can be a few % off but that's OK, as it's fitting a Weibull to the Kaplan Meier curves, so they're not going to be identical.

# m_M_SoC


# I can repeat this for TTD:
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 0)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 14)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 28)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 42)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 56)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 70)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 84)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 98)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 112)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 126)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 158)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 172)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 184)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 198)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 212)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 224)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 238)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 252)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 266)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 280)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 294)
# 
# summary(survfit(Surv(time, status) ~ 1, data = df_TTD), times = 308)


# Here, like above, I put the p_PFSD_SoC bit into the first part of the probability matrix, even though it concerns people going from first line treatment or PFS to dead, because it needs to be multiplied by the 1 that exists in the cohort:

# 
# m_P_SoC["PFS", "PFS",]<- 1- p_PFSD_SoC
# m_P_SoC["PFS", "OS",]<- 0
# m_P_SoC["PFS", "Dead",]<-0
# 
# # Setting the transition probabilities from OS
# m_P_SoC["OS", "OS", ] <- 0
# m_P_SoC["OS", "Dead", ]        <- 0
# 
# 
# # Setting the transition probabilities from Dead
# m_P_SoC["Dead", "Dead", ] <- 0


# So here I once again create the Markov cohort trace by looping over all cycles
# - note that the trace can easily be obtained using matrix multiplications
# - note that now the right probabilities for the cycle need to be selected, like I explained above.
# for(i_cycle in 1:(n_cycle-1)) {
#   m_M_SoC[i_cycle + 1, ] <- m_M_SoC[i_cycle, ] %*% m_P_SoC[ , , i_cycle]
#   m_M_Exp[i_cycle + 1, ] <- m_M_Exp[i_cycle, ] %*% m_P_Exp[ , , i_cycle]
# }
# 
# 
# head(m_M_SoC)  # print the first few lines of the matrix for standard of care (m_M_SoC)
# head(m_M_Exp)  # print the first few lines of the matrix for experimental treatment(m_M_Exp)
# 
# #m_M_SoC
# m_M_SoC

# This also answers the question of how exactly probabilities are created, the m_P_SoC is something that, when multiplied by 1, (when at the start 100% of our cohort are in the PFS state) gives us the first probability from that 100%, and the next part or row of that m_P_SoC probability is then multiplied by the proportion who are in the different states in the next row for the cohort (given their movements last time having been multiplied by m_P_SoC) which gives us the second probability multiplied by the proportion of that 1 (or 100%) in each state, and so on, and so forth, for each wave.








```

# Time-to-Progression (TTP):

# 04 Parametric Survival Analysis itself:

```{r}

# We use the 'flexsurv' package to fit several commonly used parametric survival distributions.

# The data needs to be set up to include a column for the time (in years) and a status indicator whether the time corresponds to an event, i.e. progression (status = 1), or to the last time of follow up, i.e. censoring (status = 0).


# It looks like Koen is applying the flexsurvreg formula to individuals who experience progression (i.e. ~1):

head(df_TTP)

l_TTP_SoC_exp      <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "exp")
l_TTP_SoC_gamma    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "gamma")
l_TTP_SoC_gompertz <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "gompertz")
l_TTP_SoC_llogis   <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "llogis")
l_TTP_SoC_lnorm    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "lnorm")
l_TTP_SoC_weibull  <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "weibull")


```

# 05 Inspecting the fits:

```{r}

# And this would make sense per the below diagram - which looks at the proportion of individuals who have the event, i.e. progression.

# Inspect fit based on visual fit
colors <- rainbow(6)
plot(l_TTP_SoC_exp,       col = colors[1], ci = FALSE, ylab = "Event-free proportion", xlab = "Time in days", las = 1)
lines(l_TTP_SoC_gamma,    col = colors[2], ci = FALSE)
lines(l_TTP_SoC_gompertz, col = colors[3], ci = FALSE)
lines(l_TTP_SoC_llogis,   col = colors[4], ci = FALSE)
lines(l_TTP_SoC_lnorm,    col = colors[5], ci = FALSE)
lines(l_TTP_SoC_weibull,  col = colors[6], ci = FALSE)
legend("right",
       legend = c("exp", "gamma", "gompertz", "llogis", "lnorm", "weibull"),
       col    = colors,
       lty    = 1,
       bty    = "n")


# Koen says "# Weibull has the best visual and numerical fit" but I don't see what it's visually being compared to in this graph, I will have to learn about this in the C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv folder.

# The time is in years which makes sense, the data you are drawing from is in years so that's what you want to make comparisons to.


```

```{r}

# Compare the fit numerically based on the AIC
(v_AIC <- c(
  exp      = l_TTP_SoC_exp$AIC,
  gamma    = l_TTP_SoC_gamma$AIC,
  gompertz = l_TTP_SoC_gompertz$AIC,
  llogis   = l_TTP_SoC_llogis$AIC,
  lnorm    = l_TTP_SoC_lnorm$AIC,
  weibull  = l_TTP_SoC_weibull$AIC
))

# Weibull has the best visual and numerical fit

# I will have to learn what a good value of AIC is, and, because flexsurvreg provides AIC I will have to see what other numerical measures, such as BIC, it provides.

```

# 06 Saving the survival parameters for use in the model:

```{r}

# Saving the survival parameters ----

# The 'flexsurv' package return the coefficients, which need to be transformed for use in the base R functions, but that will be done when the coefficients actually are used, for the time being we will just save the survival parameters from the distribution we decide to use. 

# NB, if we are not going with Weibull then we may have to save something specific to the distribution that is not shape or scale - we can look into this if we don't use Weibull.

l_TTP_SoC_weibull
# Calling a flexsurvreg parameter like this allows you to see here that Weibull is the shape and the scale, so if we do go with another distribution we can see what it's version of shape and scale are and use these instead.

l_TTP_SoC_weibull$coefficients

coef_weibull_shape_SoC <- l_TTP_SoC_weibull$coefficients["shape"]
coef_weibull_scale_SoC <- l_TTP_SoC_weibull$coefficients["scale"]

```

# Time-to-Dead (TTD):

# 07 Parametric Survival Analysis itself:

```{r}

# We use the 'flexsurv' package to fit several commonly used parametric survival distributions.

# The data needs to be set up to include a column for the time (in years) and a status indicator whether the time corresponds to an event, i.e. progression (status = 1), or to the last time of follow up, i.e. censoring (status = 0).


# It looks like Koen is applying the flexsurvreg formula to individuals who experience progression (i.e. ~1):

# I duplicate the time to progression data frame as time to dead here, I'll REALLY need to make sure to delete this line when I have actual time to dead data or else I'll be running my analysis on the time to progression data twice and thinking I'm running it on time to dead:

# df_TTD <- df_TTP

head(df_TTD)

l_TTD_SoC_exp      <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "exp")
l_TTD_SoC_gamma    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "gamma")
l_TTD_SoC_gompertz <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "gompertz")
l_TTD_SoC_llogis   <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "llogis")
l_TTD_SoC_lnorm    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "lnorm")
l_TTD_SoC_weibull  <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "weibull")

```

# 08 Inspecting the fits:

```{r}

# And this would make sense per the below diagram - which looks at the proportion of individuals who have the event, i.e. progression.

# Inspect fit based on visual fit
colors <- rainbow(6)
plot(l_TTD_SoC_exp,       col = colors[1], ci = FALSE, ylab = "Event-free proportion", xlab = "Time in days", las = 1)
lines(l_TTD_SoC_gamma,    col = colors[2], ci = FALSE)
lines(l_TTD_SoC_gompertz, col = colors[3], ci = FALSE)
lines(l_TTD_SoC_llogis,   col = colors[4], ci = FALSE)
lines(l_TTD_SoC_lnorm,    col = colors[5], ci = FALSE)
lines(l_TTD_SoC_weibull,  col = colors[6], ci = FALSE)
legend("right",
       legend = c("exp", "gamma", "gompertz", "llogis", "lnorm", "weibull"),
       col    = colors,
       lty    = 1,
       bty    = "n")


# Koen says "# Weibull has the best visual and numerical fit" but I don't see what it's visually being compared to in this graph, I will have to learn about this in the C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv folder.

# The time is in years which makes sense, the data you are drawing from is in years so that's what you want to make comparisons to.

```

```{r}

# Compare the fit numerically based on the AIC
(v_AIC <- c(
  exp      = l_TTD_SoC_exp$AIC,
  gamma    = l_TTD_SoC_gamma$AIC,
  gompertz = l_TTD_SoC_gompertz$AIC,
  llogis   = l_TTD_SoC_llogis$AIC,
  lnorm    = l_TTD_SoC_lnorm$AIC,
  weibull  = l_TTD_SoC_weibull$AIC
))

# Weibull has the best visual and numerical fit

# I will have to learn what a good value of AIC is, and, because flexsurvreg provides AIC I will have to see what other numerical measures, such as BIC, it provides.

```

# 09 Saving the survival parameters for use in the model:

```{r}

# Saving the survival parameters ----

# The 'flexsurv' package return the coefficients, which need to be transformed for use in the base R functions, but that will be done when the coefficients actually are used, for the time being we will just save the survival parameters from the distribution we decide to use. 

# NB, if we are not going with Weibull then we may have to save something specific to the distribution that is not shape or scale - we can look into this if we don't use Weibull.

l_TTD_SoC_weibull
# Calling a flexsurvreg parameter like this allows you to see here that Weibull is the shape and the scale, so if we do go with another distribution we can see what it's version of shape and scale are and use these instead.

l_TTD_SoC_weibull$coefficients

coef_TTD_weibull_shape_SoC <- l_TTD_SoC_weibull$coefficients["shape"]
coef_TTD_weibull_scale_SoC <- l_TTD_SoC_weibull$coefficients["scale"]
```

[**How to extrapolate the parametric models beyond the KM curve**]{.underline}

1 How to deal with time in parametric survival analysis, that is, when I digitise for a certain period of time, how do I convert it to the time period I want to look at in my own analysis?
[there may be a clue to this by how they handle time here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\Parametric Survival Analysis\\flexsurv\\THE FLEXSURV EXAMPLE CODE EXPLAINED.txt OS_TimeYears\<-AVAL\*(1/12) it looks like they wanted overal survival time in years, and so converted AVAL by 1/12]

2 How to extrapolate the parameteric models beyond the KM curve they came from.

3 What does the flexsurvreg code mean?

Use the below (and above quote section) for digitising, engauge, parametric survival modelling, etc., There is helpful R code on doing this here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\ISPOR WEBINAR Health Economic Modelling in R and I think I should go through all the R code I have with a fine toothcomb to look for further resources in R on how to do this.
Because this R code specificaly has: "\# 2. SURVIVAL ANALYSIS survival analysis of individual patient data"

> Read the below in tangent with:
>
> Cost-Effectiveness Analysis of Regorafenib for Metastatic Colorectal Cancer Daniel A. Goldstein, Bilal B. Ahmad, Qiushi Chen, Turgay Ayer, David H. Howard, Joseph Lipscomb, Bassel F. El-Rayes, and Christopher R. Flowers <https://sci-hub.se/10.1200/JCO.2015.61.9569>
>
> and also this article: Peng, Z., Hou, X., Huang, Y., Xie, T., & Hua, X.
> (2020).
> Cost-effectiveness analysis of fruquintinib for metastatic colorectal cancer third-line treatment in China.
> BMC cancer, 20(1), 1-8.
> <https://link.springer.com/article/10.1186/s12885-020-07486-w>
>
> and:
>
> Ewara, E. M., Zaric, G. S., Welch, S., & Sarma, S.
> (2014).
> Cost-effectiveness of first-line treatments for patients with KRAS wild-type metastatic colorectal cancer.
> Current Oncology, 21(4), 541-550.
> <https://sci-hub.st/10.3747/co.21.1837>
>
> Progression Risk and Mortality Progression risks and cause-specific mortalities for each arm were derived from the PFS and OS curves of the corresponding study.
> Engauge Digitizer16 was used to extract the data points from each PFS and OS plot, and then used to fit parametric survival models.
> Statistical analyses demonstrated that Weibull and log-logistic models provided a good fit for all curves according to the Akaike information criterion, Schwarz Bayesian criterion,17 and visual inspection (Supplemental Table 1 in the online version).
> Weibull models permit the hazard rate to increase over time and are thus more suitable for modeling events occurring early during follow-up periods.
> We therefore selected the Weibull model for all survival curves in this analysis.
> Next, we estimated the risk for each cycle based on the fitted survival models.
> In particular, because OS is defined as the period between the start of treatment and death, we computed the cause-specific mortality rates, p1(t), at cycle t based on the fitted Weibull OS model, OS(t), using the formula:
>
> p1ðtÞ ¼ Pðt T t þ 1Þ PðT tÞ ¼ ðOSðtÞ  OSðt  1ÞÞ OSðtÞ : (1)
>
> PFS is the period between the start and progression of disease or death.
> We computed the combined risks p2(t) based on the Weibull PFS model using Formula 1 and computed p2(t)  p1(t) as the estimate of the progression risks.
> Estimates of mortality and progression risk beyond the follow-up time in the clinical trials were extrapolated based on the fitted survival models.
> The overall mortality was defined according to the maximum value of cause-specific mortality and the background mortality.
> We use the US life tables to estimate the background mortality for each age group separately.18 **In each run of the model simulation, the initial age was sampled from the distribution of age at diagnosis for mCRC in the Surveillance, Epidemiology, and End Results database since the year 2000. [So, rather than picking a starting age, it looks like they randomly sampled an age for competing background mortality].**
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> I discussed parametric survival analysis with Andy here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.3 Practical exercise R

**[So, I think my takeway is that I need to do digitising and maybe parametric survival analysis on some existing published data - per C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Parametric Survival Analysis and the rest of my York training material THEN I'll need to include Age specific all cause mortality per: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH because I don't have my own clinical data, these are both good sources to consider seeing as they also didnt have their own data, I'll also have to think about the age of the cohort if I include competing mortality, as below:].**

> **All done below:**
>
> [*"Starting age of the cohort was 60 years, which is the median age of incident breast cancer cases as reported by The Netherlands Cancer Registration [28]." per: Kip, M., Monteban, H., & Steuten, L. (2015). Long-term cost--effectiveness of Oncotype DX® versus current clinical practice from a Dutch cost perspective. Journal of comparative effectiveness research, 4(5), 433-445. C:/Users/Jonathan/Dropbox/PC/Downloads/kip2015%20(1).pdf*]{.underline}
>
> [*"Nonbreast cancer death was captured as a competing risk in the model. These data were derived from The Netherlands Cancer Registration and based on Dutch female life tables for 2007--2009 [28,33]. Kip, M., Monteban, H., & Steuten, L. (2015). Long-term cost--effectiveness of Oncotype DX® versus current clinical practice from a Dutch cost perspective. Journal of comparative effectiveness research, 4(5), 433-445." [file:///C:/Users/Jonathan/Dropbox/PC/Downloads/kip2015%20(1).pdf](file:///C:/Users/Jonathan/Dropbox/PC/Downloads/kip2015%20(1).pdf)*]{.underline}
>
> [*Patients had a starting age of 56 years Salem, A., Men, P., Ramos, M., Zhang, Y. J., Ustyugova, A., & Lamotte, M. (2021). Cost--effectiveness analysis of empagliflozin compared with glimepiride in patients with Type 2 diabetes in China. Journal of Comparative Effectiveness Research, 10(6), 469-480.*]{.underline}

#### Utility Estimates

To present health outcomes in terms of QALYS, we adjusted survival time by quality of life using a Health Related Quality of Life (HR-QOL) score on a 0-1 utility scale.
The quality of life utilities of a cycle spent in each of the model health states has been estimated to be 0.85 for the first line health state, and 0.65 for the second line health state in the published literature on quality of life utility of patients with mCRC without complications [@ramsey2000] FIND NEW UTILITIES ELSEWHERE [We want to be clear, per Craig, D., McDaid, C., Fonseca, T., Stock, C., Duffy, S., & Woolacott, N.
(2009).
Are adverse effects incorporated in economic models?
An initial review of current practice.
*Health Technology Assessment (Winchester, England)*, *13*(62), 1-71., that our utilities come from patients without adverse events, otherwise the disutilty of these adverse events have already been counted, so we want utility before any adverse events occur, and then we can decrease this with disutility for these adverse events per the studies AND MAYBE EXPLAIN MY PLAN WITH KATHLEEN HERE TO GET COUNTRY SPECIFIC UTILITY VALUES].
We assumed in the model that utility was only related to health state, and not to therapy.

#### Probability of Adverse events:

> We established the probability of Grade 3 and 4 adverse events (AEs) from the trials used for the models.15,20 Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> To do this, they basically went to the studies by:
>
> Capitain et al, 2012
>
> Hochster et al, 2008
>
> Bennouna et al, 2013
>
> And went to the tables on "Incidence of Grade X and Grade Y Adverse Events", and took the percentages reported in these studies to use in their own table under "Adverse Event Incidence"
>
> Then, according to: [[https://en.wikipedia.org/wiki/Incidence\_(epidemiology](https://en.wikipedia.org/wiki/Incidence\_(epidemiology))](https://en.wikipedia.org/wiki/Incidence_(epidemiology)) because we have explicitly an "incidence" we are allowed use this as a probability in our analysis, so, having filled in the "Adverse Event Incidence" section in our Table on model parameters like they do, we can then use these "incidence" values as probabilities.
>
> Remember that Leukopenia = Febrile Neutropenia.

Adverse events were added as a health state in the Markov model, per [@nebuloni2013] [@goulart2011a].
The probability, and disutility, of Grade 3/4 adverse events (AEs) was informed by the literature [@hochster2008] [@aballéa2007].
[PROBABBILITY:Leukopenia = 0.040 Diarrhea = 0.310 Vomiting = 0.310, --\> POTENTIALLY I SHOULD TAKE THE PROBABILITIES UNDER BVZ FOR AE'S FROM HERE TOO AND USE THEM IN THE MODEL. DISUTILITY:Leukopenia = 45% LOWER PFS UTILITY Diarrhea = 36% LOWER PFS UTILITY Vomiting = 19% LOWER PFS UTILITY].
Duration-adjusted disutility was subtracted from the baseline utility to calculate the overall utility of each health state, we assumed the duration of each adverse event was 7 days, and that treatment-induced complications are mutually exclusive [@goldstein2015] [@refaat2014].

[*AEs can reduce patients' utility so we applied disutility estimates for these temporary health states to each major AE based on data from Aballea at al.21 These included febrile neutropenia, vomiting, and diarrhea. We assumed the duration of each AE was 5 days. For each AE, the average disutility was weighted by the incidence reported in the clinical study.*]{.underline}

[*Disutilities associated with adverse events (AEs), including fatigue, hand-foot syndrome, diarrhea, and hypertension, were obtained from published data in the literature13. The duration of AEs was estimated based on Expert Consultation. From the opinions of many doctors in China, regardless of patients in the arm of fruquintinib or regorafenib, hand-foot syndrome would last for 14 days and the disutility was 0.116. Hypertension would last for five days and the disutility was 0. Diarrhea would last for five days and the disutility was 0.1038 . The duration-adjusted disutility was subtracted from the baseline utility to calculate the overall utility of each health state.*]{.underline}

> [Baseline quality of life utility of Chinese patients with T2D without complications was 0.876 [[**19**](https://www.futuremedicine.com/reader/content/17e4aa15614/10.2217/cer-2020-0284/format/epub/EPUB/xhtml/index.xhtml#B19)] and BMI-related disutility applied per BMI point increase was 0.0061 [[**20**](https://www.futuremedicine.com/reader/content/17e4aa15614/10.2217/cer-2020-0284/format/epub/EPUB/xhtml/index.xhtml#B20)]. For acute events, a disutility was subtracted from the base utility, leading to a final utility score recorded for the patient experiencing the event. In case, a patient suffered different multiple events (e.g., a myocardial infarction [MI] and a stroke), the model followed the minimum approach, recording the utility value of the health state with the lowest individual score, while still applying a disutility related to adverse events. The full list of utilities and disutilities can be retrieved from [**Supplementary Table 2**](https://www.futuremedicine.com/doi/suppl/10.2217/cer-2020-0284).]{.underline}
>
> [Salem, A., Men, P., Ramos, M., Zhang, Y. J., Ustyugova, A., & Lamotte, M. (2021). Cost--effectiveness analysis of empagliflozin compared with glimepiride in patients with Type 2 diabetes in China. Journal of Comparative Effectiveness Research, 10(6), 469-480.]{.underline}
>
> [*If a patient suffered multiple adverse events, I think they just applied the disutility that was the worst to their baseline utility, rather than trying to combine disutilities - which would probably be messy.*]{.underline}
>
> [*We assumed that treatment-induced complications---grade 3 or 4---are mutually exclusive---a patient can have only 1 complication at a time*]{.underline}
>
> [*Refaat, T., Choi, M., Gaber, G., Kiel, K., Mehta, M., Gradishar, W., & Small Jr, W. (2014). Markov model and cost-effectiveness analysis of bevacizumab in HER2-negative metastatic breast cancer. American journal of clinical oncology, 37(5), 480-485. <https://sci-hub.se/10.1097/COC.0b013e31827e4e9a>*]{.underline}
>
> [*This is an alternative approach that may be cleaner, assume patients can only have one adverse event per cycle, and that way you don't have to find out which is worse and apply that one.*]{.underline}

[*Lots of adverse events in studies to consider here:*]{.underline}

[*"The frequencies of occurrence of AEFIs were expressed through probabilities, incidence, rates, or relative risk mentioned in 35 of the included studies [27--34,36,38-- 46,49,51,52,55--57,59,61,63,64,68,72,73,75--78]."*]{.underline}

[*- Fens, T., de Boer, P. T., van Puijenbroek, E. P., & Postma, M. J. (2021). Inclusion of Safety-Related Issues in Economic Evaluations for Seasonal Influenza Vaccines: A Systematic Review. Vaccines 2021, 9, 111.*]{.underline}

[*https://sci-hub.se/10.3390/vaccines9020111*](https://sci-hub.se/10.3390/vaccines9020111){.uri}

###### [*Adverse Events Occurring:*]{.underline}

> [*Have a look at my emails to Kathleen and Daniel re: Adverse events, utilities during adverse events, etc.,*]{.underline}
>
> [*"The frequencies of occurrence of AEFIs were expressed through probabilities, incidence, rates, or relative risk mentioned in 35 of the included studies [27--34,36,38-- 46,49,51,52,55--57,59,61,63,64,68,72,73,75--78]."*]{.underline}
>
> [*- Per:*]{.underline}
>
> [*Fens, T., de Boer, P. T., van Puijenbroek, E. P., & Postma, M. J. (2021). Inclusion of Safety-Related Issues in Economic Evaluations for Seasonal Influenza Vaccines: A Systematic Review. Vaccines 2021, 9, 111.*]{.underline}
>
> [*https://sci-hub.se/10.3390/vaccines9020111*](https://sci-hub.se/10.3390/vaccines9020111){.uri}
>
> To convert reported incidence rates to probabilities read:
>
> <https://sphweb.bumc.bu.edu/otlt/mph-modules/ep/ep713_diseasefrequency/EP713_DiseaseFrequency5.html>
>
> Also saved here:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.1.3 Key concepts in time-dependent transition probabilities\\Relationship of Incidence Rate to Cumulative Incidence (Risk).pdf
>
> The HERC Material here:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\HERC Cost Effectiveness Analysis Course\\Estimating Transition Probabilities
>
> Probability converting is here per the York course:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.1.3 Key concepts in time-dependent transition probabilities\\notes
>
> This may also be helpful
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Foundation Course\\F2_Decision Trees\\F2.1.2 Probabilities & conditioning\\Probabilities and Conditioning Notes

[*These guys add adverse events as a State in the Markov model, so I could probably just do that:*]{.underline}

[*Nebuloni, D. R., Mak, M. P., Souza, F. H., Saragiotto, D. F., Júlio, T., De Castro Jr, G., ... & Hoff, P. M. (2013). Modified FLOX as first-line chemotherapy for metastatic colorectal cancer patients in the public health system in Brazil: Effectiveness and cost-utility analysis. Molecular and Clinical Oncology, 1(1), 175-179. <https://sci-hub.st/10.3892/mco.2012.12>*]{.underline}

[*Goulart, B., & Ramsey, S. (2011). A trial-based assessment of the cost-utility of bevacizumab and chemotherapy versus chemotherapy alone for advanced non-small cell lung cancer. Value in Health, 14(6), 836-845. [\<https://sci-hub.se/10.1016/j.jval.2011.04.004\>](https://sci-hub.se/10.1016/j.jval.2011.04.004){.uri}*]{.underline}

> [*The duration-adjusted disutility was subtracted from the baseline utility to calculate the overall utility of each health state.*]{.underline}
>
> [*Cost-Effectiveness Analysis of Regorafenib for Metastatic Colorectal Cancer Daniel A. Goldstein, Bilal B. Ahmad, Qiushi Chen, Turgay Ayer, David H. Howard, Joseph Lipscomb, Bassel F. El-Rayes, and Christopher R. Flowers <https://sci-hub.se/10.1200/JCO.2015.61.9569>*]{.underline}
>
> [*Progression Risk and Mortality Progression risks and cause-specific mortalitie*]{.underline}

###### Range around things:

> The below, and most of Table 4 in Goldstein, Daniel A., Qiushi Chen, Turgay Ayer, David H. Howard, Joseph Lipscomb, R. Donald Harvey, Bassel F. El-Rayes, and Christopher R. Flowers.
> 2014.
> "Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colorectal Cancer." Clinical Colorectal Cancer 13 (4): 219--25.
> [\<https://doi.org/10.1016/j.clcc.2014.09.007.\>](https://doi.org/10.1016/j.clcc.2014.09.007.){.uri}
>
> seems to suggest I can determine my upper and lower limit simply as 20% of my point estimate, most, if not all, of the time.
> Also, Goldstein take the 20% approach and apply this to the Beta distribution for utilities, so it must be possible to apply to both the Beta distribution and the normal distribution application I saw in the study from Eygpt.
>
> "All model inputs were varied 20% below and above the base case for costs, and confidence intervals or standard errors extracted from published sources were used for the probabilities, adverse events, and utilities (Table 1)."
>
> The transition probabilities had a max of 20% over the point estimate, a min of 20% under the point estimate, and a Gamma distribution for transition probabilities.
>
> \- Per: Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
> (2019).
> Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
> Journal of Medical Economics, 22(2), 163-168.
> [\<https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432\>](https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432){.uri}
>
> The below takes the max and min as 20% of the mean for costs, utility, applying a gamma and beta distribution respectively (negative beta for disutilities that were negative percentages).
> For the discount factor it looks like they just took a minimum of no discount factor and a maximum of 5% and applied a uniform distribution.
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> ***I like that Goldstein et al., report the actual distribution they derived from their Value, Minimum and Maximum, I think I'll review the other papers by Goldstein and co-authors to see if they repeat that elsewhere, as it supports checking your methods against their own.***
>
> Transition probabilities used in the analysis
>
> The transition probabilities, had a min 20% below the base case value and a max 20% above this.
>
> Wu, Q., Wang, X., Zhang, M., Liao, W., Wang, F., & Li, Q.
> (2020).
> Cost-effectiveness analysis of bevacizumab plus paclitaxel versus bevacizumab plus capecitabine for HER2-negative locally recurrent or metastatic breast cancer.
> Oncology Research and Treatment, 43(4), 153-159.
> [\<https://www.karger.com/Article/Pdf/505932\>](https://www.karger.com/Article/Pdf/505932){.uri}
>
> 20% costs variation here: Sharp, L., Tilson, L., Whyte, S., O'Ceilleachair, A., Walsh, C., Usher, C., ... & Comber, H.
> (2012).
> Cost-effectiveness of population-based screening for colorectal cancer: a comparison of guaiac-based faecal occult blood testing, faecal immunochemical testing and flexible sigmoidoscopy.
> British journal of cancer, 106(5), 805-816.
> <https://www.nature.com/articles/bjc2011580.pdf>
>
> 20% costs variability Goulart, B., & Ramsey, S.
> (2011).
> A trial-based assessment of the cost-utility of bevacizumab and chemotherapy versus chemotherapy alone for advanced non-small cell lung cancer.
> *Value in Health*, *14*(6), 836-845.
>
> **Costing Data in Spain:**

Rivera, F., Valladares, M., Gea, S., & López-Martínez, N.
(2017).
Cost-effectiveness analysis in the Spanish setting of the PEAK trial of panitumumab plus mFOLFOX6 compared with bevacizumab plus mFOLFOX6 for first-line treatment of patients with wild-type RAS metastatic colorectal cancer.
Journal of Medical Economics, 20(6), 574-584.
<https://sci-hub.st/10.1080/13696998.2017.1285780>

#### Sensitivity Analysis

I should do this in tangent with the two papers using 20%.

`` {# {r setup, include=FALSE} # knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = T) # # To knit this document every time it is run, you can change `eval` to `TRUE` in the above. ``

```{r}
# rm(list = ls())  
# clear memory (removes all the variables from the work space)
```

# 01 Load packages

`{# {r} # if (!require('pacman')) install.packages('pacman'); library(pacman)  # # use this package to conveniently install other packages # # load (install if required) packages from CRAN # p_load("diagram", "dampack", "reshape2") # # library(devtools) # devtools is necessary to install from github. # # install_github("DARTH-git/darthtools", force = TRUE) # Uncomment if there is a newer version # p_load_gh("DARTH-git/darthtools")`

# 02 Load functions

```{r}
# all functions are in the darthtools package

# There is a functions RMD for the PSA stuff below, instead of calling it in: ## 08.2 Load PFS-PFSer Markov model function, I could just place it here, and place all the necessary packages above and then I would only ever need 1 R Markdown document for the entire study. Will think about doing this.


```

# To use dampack, review this before you start:

<https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/psa_generation.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/voi.html>

<https://cran.r-project.org/web/packages/dampack/dampack.pdf>

<https://syzoekao.github.io/CEAutil/#43_one-way_sensitivity_analysis>

<https://hesim-dev.github.io/hesim/articles/cea.html>

also saved as pdf files here:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS\_Model

And:

"An Introductory Tutorial on Cohort State-Transition Models in R Using a Cost-Effectiveness Analysis" <https://arxiv.org/pdf/2001.07824.pdf> [also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS\_Model\An Introductory Tutorial on Cohort StateTransition.pdf]

The dampack package was based on the following textbook:

[file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine\_%20integrating%20evidence%20and%20values.pdf](file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine_%20integrating%20evidence%20and%20values.pdf)

If I find any parts of the package confusing I can email:

Maintainer: Greg Knowlton \<knowl193\@umn.edu\>

The official website is here: <https://cran.r-project.org/web/packages/dampack/>

IT'S ALSO VERY IMPORTANT TO CONSIDER WHAT BEARING THE INCLUSION OF ADDITIONAL MARKOV BUBBLES WILL HAVE ON THE FUNCTIONS USED HERE, WHICH ASSUMEDLY WERE BUILT WITH A 3 STATE MODEL IN MIND, AND NOW WE HAVE ADDITIONAL TUNNEL STATES.

# 03 Input model parameters

Great utility values for first and second line treatment and adverse events here: Cost-effectiveness analysis of selective first-line use of biologics for unresectable RAS wild-type left-sided metastatic colorectal cancer saved here: <file:///C:/Users/Jonathan/Dropbox/Grammarly%20files/curroncol-26-04843.pdf>

```{r}
## General setup


# Why ordering, V_names_states v_tc_SoC, v_tc_Exp,  v_tu_SoC and v_tu_Exp matters:

# The ordering of V_names_states has an influence on the tornado diagram and the reported cost-effectiveness results.

# So, I need to ensure I correctly order V_names_states all the way through from the start.

# m_P_Exp and m_P_SoC both use v_names_states to set up the names of the rows and columns of their matrices. 

# As does m_M_SoC and m_M_Exp.

# Then m_M_SoC and  m_M_Exp use the row column names to fill in the 100% of the cohort (or 1) in PFS and the 0% of the cohort in AE1, AE2, AE3, OS and DEAD in wave 1. m_P_Exp and m_P_SoC also both use the row and column names to fill in the transition probabilities between PFS and OS, etc.,

# Then m_M_SoC and m_M_Exp are matrix multiplied by m_P_Exp and m_P_SoC:

# for(i_cycle in 1:(n_cycle-1)) {
#  m_M_SoC[i_cycle + 1, ] <- m_M_SoC[i_cycle, ] %*% m_P_SoC[ , , i_cycle]
#  m_M_Exp[i_cycle + 1, ] <- m_M_Exp[i_cycle, ] %*% m_P_Exp[ , , i_cycle]
# }

# The way this matrix multiplication works is that the value in box 1 for m_M_SoC is multiplied by the value in box 1 for m_P_SoC and so on. If box 1 for m_M_SoC and m_P_SoC is the 1 (i.e. 100% of people in the PFS state when this all starts), multiplied by the PFS state probabilities, i.e., the probability of going from the PFS state into other states like so:

#          PFS       AE1       AE2       AE3          OS        Dead
# PFS  0.974925 0.0194985 0.0194985 0.0194985 0.005074995 0.020000000

# Then things will multiply correctly and we're multiplying PFS transition probabilities by the 100% of the cohort in the PFS state. 

# Even, if ordering v_names_states  <- c("PFS", "AE1", "AE2", "AE3", "OS", "Dead")   is not in the order as above, i.e., v_names_states  <- c("OS", "AE1", "AE2", "AE3", "PFS", "Dead") , then the m_M_SoC will have 0, 0, 0, 0, 1, 0, because everyone still starts in the PFS state, and m_P_SoC will have  "OS", "AE1", "AE2", "AE3", "PFS", "Dead", with the right probabilities put in the right spots, so when you matrix multiply it things will multiply out fine.

# However, this is not the case for ordering costs and utilities:  


# v_tc_SoC <- m_M_SoC %*% c(c_F_SoC, c_AE1, c_AE2, c_AE3, c_P, c_D)
# v_tc_Exp <- m_M_Exp %*% c(c_F_Exp, c_AE1, c_AE2, c_AE3, c_P, c_D)


# v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)
# v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)

# As you can see, in both cases the ordering is set manually by how we enter things in the concatenated brackets, so in the case above where ordering v_names_states  <- c("PFS", "AE1", "AE2", "AE3", "OS", "Dead") is ordered differently, i.e., v_names_states  <- c("OS", "AE1", "AE2", "AE3", "PFS", "Dead") when we matrix multiply costs and utilities by m_M_SoC  and m_M_Exp above we will be multiplying the utility of being in the progression free state u_F by the matrix of individuals in the OS state, which will clearly be a smaller number of individuals in the first few waves, and multiplying the utility of the OS state (u_P) by the larger number of individuals actually in the PFS state <- c("OS", "AE1", "AE2", "AE3", "PFS", "Dead"). We'll be doing the same thing with costs. What this will mean is more people getting the OS costs and OS utility and fewer people getting the PFS costs and the PFS utility, which will in turn have consequences for the cost-effectiveness analysis results with more OS costs and more OS utility being considered in the equation that compares costs and utilities.

# I've confirmed all of the things I say about by changing the ordering first of v_names_states, and then of the cost and utility concatenations. Changing the ordering of v_names_states did nothing to the CEA results or Tornado diagram provided I changed the ordering of utilities and costs to match this, changing the ordering of utilities and costs changed both unless I changed them to be in an order that matched the changed ordering of v_names_states.






# At the bottom of the parametric survival code I think about how time from individual patient data may be changed to match the time of our cycles.

# I think the digitiser will give me the time in the context of the survival curves I am digitising, i.e., time in weeks, or time in months or time in years.

# Then I will have to set-up my time accordingly in the R code so that my cycle length is at the same level as the individual patient data.

# That is, in Koen's example:

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\ISPOR WEBINAR Health Economic Modelling in R\ISPOR_webinar_R-master

# the data includes a column for the time (in years)
# t_cycle <- 1/4      # cycle length of 3 months (in years)                                # n_cycle <- 60       # number of cycles (total runtime 15 years)
  

# So I would have my colum for time, in the [TIME] the graph I was digitising used.
# Then I would create my cycle length of X weeks (in [TIME] the graph I was digitising used)
# Then I would have my number of cycles that would add up to give me a total runtime of how long I want to run the model for.
# So, above Koen wanted to run the model for 15 years, but his cycles were in 3 months, or each cycle was a quarter of a year, so 60 quarters, or 60 3 month cycles, is 15 years.

# REALISE HERE THEAT P_PD ISNT THE PROBABILITY OF PROGRESSION TO DEAD, BUT OF PFS TO DEAD, OF FIRST LINE TO DEAD, BECAUSE OUR APD CURVES ONLY EVER DESCRIBE FIRST LINE TREATMENT, BE THAT FIRST LINE SOC TREATMENT OR FIRST LINE EXP TREATMENT.

# Here we define all our model parameters, so that we can call on these parameters later during our model:

t_cycle <- 14      # cycle length of 2 weeks (in [[days]] - this is assuming the survival curves I am digitising will be in [[days]] if they are in another period I will have to represent my cycle length in that period instead).                                  
n_cycle        <- 143                            
# We set the number of cycles to 143 to reflect 2,000 days from the Angiopredict study (5 Years, 5 Months, 3 Weeks, 1 Day) broken down into fortnightly cycles
v_names_cycles  <- paste("cycle", 0:n_cycle)    
# So here, we just name each cycle by the cycle its on, going from 0 up to the number of cycles there are, here 67
v_names_states  <- c("PFS", "OS", "Dead")  
# These are the health states in our model, PFS, Adverse Event 1, Adverse Event 2, Adverse Event 3, OS, Death.
n_states        <- length(v_names_states)        
# We're just taking the number of health states from the number of names we came up with, i.e. the number of names to reflect the number of health states 

# Strategy names
v_names_strats     <- c("Standard of Care",         
                     "Experimental Treatment")
               # store the strategy names
n_str           <- length(v_names_strats)           
# number of strategies



# TRANSITION PROBABILITIES: Time-To-Transition - TTP:


# Time-dependent transition probabilities are obtained in four steps
# 1) Defining the cycle times
# 2) Obtaining the event-free (i.e. survival) probabilities for the cycle times for SoC
# 3) Obtaining the event-free (i.e. survival) probabilities for the cycle times for Exp based on a hazard ratio
# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. survival) probabilities

# 1) Defining the cycle times
(t <- seq(from = 0, by = t_cycle, length.out = n_cycle + 1))

# I think here we're saying, at each cycle how many of the time periods our individual patient data is measured at have passed? Here our individual patient data is in days, so we have 0 in cycle 0, 14 (or two weeks) in cycle 1, and so on.

# Having established that allows us to obtain the transition probabilities for the time we are interested in for our cycles from this different period individual patient data, so where the individual patient data is in days and our cycles are in fortnight or half months, this allows us to obtain transition probabilities for these fortnights.

# 2) Obtaining the event-free (i.e. survival) probabilities for the cycle times for SoC
# S_FP_SoC - survival of progression free to progression, i.e. not going to progression, i.e. staying in progression free.
# Note that the coefficients [that we took from flexsurvreg earlier] need to be transformed to obtain the parameters that the base R function uses


S_FP_SoC <- pweibull(
  q     = t, 
  shape = exp(coef_weibull_shape_SoC), 
  scale = exp(coef_weibull_scale_SoC), 
  lower.tail = FALSE
)

head(cbind(t, S_FP_SoC))

#        t  S_FP_SoC
# [1,] 0.0 1.0000000
# [2,] 0.5 0.9948214
# [3,] 1.0 0.9770661
# [4,] 1.5 0.9458256
# [5,] 2.0 0.9015175
# [6,] 2.5 0.8454597


# Having the above header shows that this is probability for surviving in the F->P state, i.e., staying in this state, because you can see in time 0 100% of people are in this state, meaning 100% of people hadnt progressed and were in PFS, if this was instead about the progressed state (i.e. OS), there should be no-one in this state when the model starts, as everyone starts in the PFS state, and it takes a while for people to reach the OS state.


# 3) Obtaining the event-free (i.e. survival) probabilities for the cycle times for Experimental treatment (aka the novel therapy) based on a hazard ratio.
# So here we basically have a hazard ratio for the novel therapy that says you do X much better under the novel therapy than under standard of care, and we want to apply it to standard of care from our individual patient data to see how much improved things would be under the novel therapy.
# (NB - if we ultimately decide not to use a hazard ratio, I could probably just create my transition probabilities for the experimental therapy from individual patient data that I have digitised from patients under this novel therapy).
# Here our hazard ratio is 0.6, I can change that for our hazard ratio.
# - note that S(t) = exp(-H(t)) and, hence, H(t) = -ln(S(t))
# that is, the survival function is the expoential of the negative hazard function, per:
# https://faculty.washington.edu/yenchic/18W_425/Lec5_survival.pdf
# and: 
# https://web.stanford.edu/~lutian/coursepdf/unit1.pdf
# Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv
# And to multiply by the hazard ratio it's necessary to convert the survivor function into the hazard function, multiply by the hazard ratio, and then convert back to the survivor function, and then these survivor functions are used for the probabilities.
HR_FP_Exp <- 0.68
H_FP_SoC  <- -log(S_FP_SoC)
H_FP_Exp  <- H_FP_SoC * HR_FP_Exp
S_FP_Exp  <- exp(-H_FP_Exp)

head(cbind(t, S_FP_SoC, H_FP_SoC, H_FP_Exp, S_FP_Exp))


head(cbind(t, S_FP_SoC, H_FP_SoC))



    # I want to vary my probabilities for the one-way sensitivity analysis, particularly for the tornado       plot of the deterministic sensitivity analysis. 
    
    # The problem here is that df_params_OWSA doesnt like the fact that a different probability for each       cycle (from the time-dependent transition probabilities) gives 122 rows (because there are 60 cycles,      two treatment strategies and a probability for each cycle). It wants the same number of       rows as      there are probabilities, i.e., it would prefer a probability of say 0.50 and then a max and a      min     around that.
    
    # To address this, I think I can apply this mean, max and min to the hazard ratios instead, knowing        that when run_owsa_det is run in the sensitivity analysis it calls this function to run and in this        function the hazard ratios generate the survivor function, and then these survivor functions are used      to generate the probabilities (which will be cycle dependent).
    
    # This is fine for the hazard ratio for the experimental strategy, I can just take:
    
    # HR_FP_Exp as my mean, and:
    
    # Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
    # Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp
    
    # For min and max.
    
    # For standard of care there was no hazard ratio, because we took these values from the survival curves     directly, and didnt vary them by a hazard ratio, like we do above.
    
    # To address this, I create a hazard ratio that is exactly one.
    
    # hazard ratio

    # A measure of how often a particular event happens in one group compared to how often it happens in       another group, over time. In cancer research, hazard ratios are often used in clinical trials to           measure survival at any point in time in a group of patients who have been given a specific treatment      compared to a control group given another treatment or a placebo. A hazard ratio of one means that         there is no difference in survival between the two groups. A hazard ratio of greater than one or less      than one means that survival was better in one of the groups. https://www.cancer.gov/publications/dictionaries/cancer-terms/def/hazard-ratio

    # Thus, I can have a hazard ratio where the baseline value of it gives you the survival curves, and        thus the probabilities, from the actual survival curves we are drawing from, and where the min and max     will be 1 +/- 0.20, which will give us probabilities that are 20% higher or lower than the probabilities from the actual survival curves that we are drawing from in the parametric survival analysis to get transitions under standard of care.
    
    # To do this, I just have to add a hazard ratio to the code that creates the transition probabilities      under standard of care as below, then I can add that hazard ratio, and it's max and min, to the            deterministic sensitivity analysis and vary all the probabilities by 20%.
        

    # So here we basically have a hazard ratio that is equal to 1, so it leaves things unchanged for           patients, and we want to apply it to standard of care from our individual patient data to leave things     unchanged in this function, but allow things to change in the sensitivity analysis.
      
    # Here our hazard ratio is 1, things are unchanged.

# So, first we create our hazard ratio == 1
HR_FP_SoC <- 1

# (I'm creating the below as new parameters, i.e. putting "nu" infront of them, in case keeping the name the same causes a problem for when I want to use them in the deterministic sensivity analysis, i.e., if I generate a parameter from itself - say var_name = var_name exactly, then there may be some way in which R handles code that won't let this work, or will take one parameter before the other, or something and stop the model from executing correctly).

# Then, we create our hazard function for SoC:
NU_S_FP_SoC <- S_FP_SoC
NU_H_FP_SoC  <- -log(NU_S_FP_SoC)
# Then, we multiply this hazard function by our hazard ratio, which is just 1, but which gives us the      opportunity to apply a hazard ratio to standard of care in our code and thus to have a hazard ratio for     standard of care for our one way deterministic sensitivity analysis and tornado diagram.
NUnu_H_FP_SoC  <- NU_H_FP_SoC * HR_FP_SoC
# Again, I was worried that with overlap when creating parameters I would have a problem with the deterministic sensivity analysis so I call it NU again to make it a "new" parameter again.
NU_S_FP_SoC  <- exp(-NUnu_H_FP_SoC)

head(cbind(t, NU_S_FP_SoC, NUnu_H_FP_SoC))



# NU_H_FP_SoC  <- -log(NU_S_FP_SoC)
# # Then, we multiply this hazard function by our hazard ratio, which is just 1, but which gives us the      opportunity to apply a hazard ratio to standard of care in our code and thus to have a hazard ratio for     standard of care for our one way deterministic sensitivity analysis and tornado diagram.
# NU_H_FP_SoC  <- NU_H_FP_SoC * HR_FP_SoC
# # 
# NU_S_FP_SoC  <- exp(-NU_H_FP_SoC)
# 
# head(cbind(t, NU_S_FP_SoC, NU_H_FP_SoC))












# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. survival) probabilities

# Now we can take the probability of being in the PFS state at each of our cycles, as created above, from 100% (i.e. from 1) in order to get the probability of NOT being in the PFS state, i.e. in order to get the probability of moving into the progressed state, or the OS state.
 
p_PFSOS_SoC <- p_PFSOS_Exp <- rep(NA, n_cycle)

# First we make the probability of going from progression-free (F) to progression (P) blank (i.e. NA) for all the cycles in standard of care and all the cycles under the experimental strategy.

for(i in 1:n_cycle) {
  p_PFSOS_SoC[i] <- 1 - NU_S_FP_SoC[i+1] /  NU_S_FP_SoC[i]
  p_PFSOS_Exp[i] <- 1 - S_FP_Exp[i+1] / S_FP_Exp[i]
}




# If I ever wanted to round my probabilities to 2 decimal places, I can do this as below, but Koen's code actually makes probabilities that sum perfectly to 1, so there's no longer any need to do this.

# round(p_PFSOS_SoC, digits=2)
# round(p_PFSOS_Exp, digits=2)

# Then we generate our transition probability under standard of care and under the experimental treatement using survival functions that havent and have had the hazard ratio from above applied to them, respectively.


# The way this works is the below, you take next cycles probability of staying in this state, divide it by this cycles probability of staying in this state, and take it from 1 to get the probability of leaving this state. 

# > head(cbind(t, S_FP_SoC))
#        t  S_FP_SoC
# [1,] 0.0 1.0000000
# [2,] 0.5 0.9948214
# [3,] 1.0 0.9770661
# [4,] 1.5 0.9458256
# [5,] 2.0 0.9015175
# [6,] 2.5 0.8454597
# > 1-0.9948214/1.0000000
# [1] 0.0051786
# > 0.9770661/0.9948214
# [1] 0.9821523
# > 1-0.9821523
# [1] 0.0178477

# p_FP_SoC
p_PFSOS_SoC


#> p_FP_SoC
#  [1] 0.005178566 0.017847796 0.031973721 0.046845943 0.062181645
#p_FP_Exp
p_PFSOS_Exp



# TRANSITION PROBABILITIES: Time-To-Dead TTD

# REALISE HERE THEAT P_PD ISNT THE PROBABILITY OF PROGRESSION TO DEAD, BUT OF PFS TO DEAD, OF FIRST LINE TO DEAD, BECAUSE OUR APD CURVES ONLY EVER DESCRIBE FIRST LINE TREATMENT, BE THAT FIRST LINE SOC TREATMENT OR FIRST LINE EXP TREATMENT.


# To make sure that my PFS probabilities only reflect going from PFS to progression, I create the probability of going from PFS to DEAD under standard of care and the experimental, and decrease my PFS to progression probability by the probability of going into the dead state, such that I am only capturing people going into progression, and not people going into death as well. 


# So, first I create the transition probabilities of progression free into dead for SoC and Exp, then I convert all the probabilities (i.e. those for PFS and those for OS) into rates, minus them from eachother, turn them back into probabilities, and make sure none are negative (and where they are replace these with 0).

# Actually, I don't do the rates thing, I just I take all the probabilities (i.e. those for PFS and those for OS), minus them from eachother.


# Time-dependent transition probabilities are obtained in four steps
# 1) Defining the cycle times [we already did this above]
# 2) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for SoC
# 3) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for Exp based on a hazard ratio if we think we will be applying a hazard ratio in the OS -> Death setting. Probably not, probably what we'll be doing is saying that once you get into the OS state under the experimental strategy, you recieve the same second-line treatment as standard of care again and thus your event-free (i.e. overall survival) probabilities for the cycle times are the same as for SoC. - Actually, I create transition probabilities from treatment to death under SoC and the Exp treatment here such that I can take them from the transition probabilities from treatment to progression for SoC and Exp treatment, because the OS here from Angiopredict is transitioning from the first line treatment to dead, not from second line treatment to death, and once we get rid of the people who were leaving first line treatment to die in PFS, all we have left is people leaving first line treatment to progress. And then we can keep the first line treatment to death probabilities we've created from the OS curves to capture people who have left first line treatment to transition into death rather than second line treatment.
# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. overall survival) probabilities

# 1) Defining the cycle times
(t <- seq(from = 0, by = t_cycle, length.out = n_cycle + 1))

# 2) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for SoC
# S_PD_SoC - survival of progression to dead, i.e. not going to dead, i.e. staying in progression.
# Note that the coefficients [that we took from flexsurvreg earlier] need to be transformed to obtain the parameters that the base R function uses


S_PD_SoC <- pweibull(
  q     = t, 
  shape = exp(coef_TTD_weibull_shape_SoC), 
  scale = exp(coef_TTD_weibull_scale_SoC), 
  lower.tail = FALSE
)

head(cbind(t, S_PD_SoC))



# Having the above header shows that this is probability for surviving in the P->D state, i.e., staying in this state, because you should see in time 0 0% of people are in this state, meaning 100% of people hadnt gone into the progressed state and were in PFS, which make sense in this model, the model starts with everyone in PFS, no-one starts the model in OS, and it takes a while for people to reach the OS state.


# 3) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for Experimental treatment (aka the novel therapy) based on a hazard ratio.
# So here we basically have a hazard ratio for the novel therapy that says you do X much better under the novel therapy than under standard of care, and we want to apply it to standard of care from our individual patient data to see how much improved things would be under the novel therapy.

# Here our hazard ratio is 0.6, I can change that for our hazard ratio.
# - note that S(t) = exp(-H(t)) and, hence, H(t) = -ln(S(t))
# that is, the survival function is the expoential of the negative hazard function, per:
# https://faculty.washington.edu/yenchic/18W_425/Lec5_survival.pdf
# and: 
# https://web.stanford.edu/~lutian/coursepdf/unit1.pdf
# Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv
# And to multiply by the hazard ratio it's necessary to convert the survivor function into the hazard function, multiply by the hazard ratio, and then convery back to the survivor function, and then these survivor functions are used for the probabilities.
HR_PD_Exp <- 0.65
H_PD_SoC  <- -log(S_PD_SoC)
H_PD_Exp  <- H_PD_SoC * HR_PD_Exp
S_PD_Exp  <- exp(-H_PD_Exp)

head(cbind(t, S_PD_SoC, H_PD_SoC, H_PD_Exp, S_PD_Exp))


# If I decide that, as I said,  once you get into the OS state under the experimental strategy, you recieve the same second-line treatment as standard of care again and thus your event-free (i.e. overall survival) probabilities for the cycle times are the same as for SoC, then I can use the following coding - which is just repeating what I did for standard of care but this time giving it to the experimental stratgey:
# 
# S_PD_Exp <- pweibull(
#   q     = t, 
#   shape = exp(coef_TTD_weibull_shape_SoC), 
#   scale = exp(coef_TTD_weibull_scale_SoC), 
#   lower.tail = FALSE
# )
# 
# head(cbind(t, S_PD_Exp))

# I've coded in both options here and I can make a decision when applying this.




# I want to vary my probabilities for the one-way sensitivity analysis, particularly for the tornado       plot of the deterministic sensitivity analysis. 

# The problem here is that df_params_OWSA doesnt like the fact that a different probability for each       cycle (from the time-dependent transition probabilities) gives 122 rows (because there are 60 cycles,      two treatment strategies and a probability for each cycle). It wants the same number of       rows as      there are probabilities, i.e., it would prefer a probability of say 0.50 and then a max and a      min     around that.

# To address this, I think I can apply this mean, max and min to the hazard ratios instead, knowing        that when run_owsa_det is run in the sensitivity analysis it calls this function to run and in this        function the hazard ratios generate the survivor function, and then these survivor functions are used      to generate the probabilities (which will be cycle dependent).

# This is fine for the hazard ratio for the experimental strategy, I can just take:

# HR_PD_Exp as my mean, and:

# Minimum_HR_PD_Exp <- HR_PD_Exp - 0.20*HR_PD_Exp
# Maximum_HR_PD_Exp <- HR_PD_Exp + 0.20*HR_PD_Exp

# For min and max.

# For standard of care there was no hazard ratio, because we took these values from the survival curves     directly, and didnt vary them by a hazard ratio, like we do above.

# To address this, I create a hazard ratio that is exactly one.

# hazard ratio

# A measure of how often a particular event happens in one group compared to how often it happens in       another group, over time. In cancer research, hazard ratios are often used in clinical trials to           measure survival at any point in time in a group of patients who have been given a specific treatment      compared to a control group given another treatment or a placebo. A hazard ratio of one means that         there is no difference in survival between the two groups. A hazard ratio of greater than one or less      than one means that survival was better in one of the groups. https://www.cancer.gov/publications/dictionaries/cancer-terms/def/hazard-ratio

# Thus, I can have a hazard ratio where the baseline value of it gives you the survival curves, and        thus the probabilities, from the actual survival curves we are drawing from, and where the min and max     will be 1 +/- 0.20, which will give us probabilities that are 20% higher or lower than the probabilities from the actual survival curves that we are drawing from in the parametric survival analysis to get transitions under standard of care.

# To do this, I just have to add a hazard ratio to the code that creates the transition probabilities      under standard of care as below, then I can add that hazard ratio, and it's max and min, to the            deterministic sensitivity analysis and vary all the probabilities by 20%.


# So here we basically have a hazard ratio that is equal to 1, so it leaves things unchanged for           patients, and we want to apply it to standard of care from our individual patient data to leave things     unchanged in this function, but allow things to change in the sensitivity analysis.

# Here our hazard ratio is 1, things are unchanged.

# So, first we create our hazard ratio == 1
HR_PD_SoC <- 1

# (I'm creating the below as new parameters, i.e. putting "nu" infront of them, in case keeping the name the same causes a problem for when I want to use them in the deterministic sensivity analysis, i.e., if I generate a parameter from itself - say var_name = var_name exactly, then there may be some way in which R handles code that won't let this work, or will take one parameter before the other, or something and stop the model from executing correctly).

# Then, we create our hazard function for SoC:
NU_S_PD_SoC <- S_PD_SoC
NU_H_PD_SoC  <- -log(NU_S_PD_SoC)
# Then, we multiply this hazard function by our hazard ratio, which is just 1, but which gives us the      opportunity to apply a hazard ratio to standard of care in our code and thus to have a hazard ratio for     standard of care for our one way deterministic sensitivity analysis and tornado diagram.
NUnu_H_PD_SoC  <- NU_H_PD_SoC * HR_PD_SoC
# Again, I was worried that with overlap when creating parameters I would have a problem with the deterministic sensivity analysis so I call it NU again to make it a "new" parameter again.
NU_S_PD_SoC  <- exp(-NUnu_H_PD_SoC)

head(cbind(t, NU_S_PD_SoC, NUnu_H_PD_SoC))



# NU_H_PD_SoC  <- -log(NU_S_PD_SoC)
# # Then, we multiply this hazard function by our hazard ratio, which is just 1, but which gives us the      opportunity to apply a hazard ratio to standard of care in our code and thus to have a hazard ratio for     standard of care for our one way deterministic sensitivity analysis and tornado diagram.
# NU_H_PD_SoC  <- NU_H_PD_SoC * HR_PD_SoC
# # 
# NU_S_PD_SoC  <- exp(-NU_H_PD_SoC)
# 
# head(cbind(t, NU_S_PD_SoC, NU_H_PD_SoC))


# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. survival) probabilities

# Now we can take the probability of being in the PFS state at each of our cycles, as created above, from 100% (i.e. from 1) in order to get the probability of NOT being in the PFS state, i.e. in order to get the probability of moving into the progressed state, or the OS state.


p_PFSD_SoC <- p_PFSD_Exp <- rep(NA, n_cycle)

# First we make the probability of going from progression-free (F) to progression (P) blank (i.e. NA) for all the cycles in standard of care and all the cycles under the experimental strategy.

for(i in 1:n_cycle) {
  p_PFSD_SoC[i] <- 1 - NU_S_PD_SoC[i+1] / NU_S_PD_SoC[i]
  p_PFSD_Exp[i] <- 1 - S_PD_Exp[i+1] / S_PD_Exp[i]
}

# round(p_PFSD_SoC, digits=2)
# round(p_PFSD_Exp, digits=2)


# Then we generate our transition probability under standard of care and under the experimental treatement using survival functions that havent and have had the hazard ratio from above applied to them, respectively. [If we decide not to apply a hazard ratio for the experimental strategy going from progression to dead then neither may have a hazard ratio applied to them].


# The way this works is, you take next cycles probability of staying in this state, divide it by this cycles probability of staying in this state, and take it from 1 to get the probability of leaving this state. 

p_PFSD_SoC

p_PFSD_Exp

# Finally, now that I create transition probabilities from treatment to death under SoC and the Exp treatment I can take them from the transition probabilities from treatment to progression for SoC and Exp treatment, because the OS here from Angiopredict is transitioning from the first line treatment to dead, not from second line treatment to death, and once we get rid of the people who were leaving first line treatment to die in PFS, all we have left is people leaving first line treatment to progress. And then we can keep the first line treatment to death probabilities we've created from the OS curves to capture people who have left first line treatment to transition into death rather than second line treatment.

# p_PFSOS_SoC
# p_PFSD_SoC
# p_PFSOS_SoC <- p_PFSOS_SoC - p_PFSD_SoC
# p_PFSOS_SoC
# 
# p_PFSOS_Exp
# p_PFSD_Exp
# p_PFSOS_Exp <- p_PFSOS_Exp - p_PFSD_Exp
# p_PFSOS_Exp




# Time-constant transition probabilities [ADVERSE EVENTS]:


# To create transition probabilities from longer time periods I can use the information in this email to Daniel:

# Inquiry re: Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer
# - https://outlook.office.com/mail/id/AAQkAGI5OWU0NTJkLTEzMjgtNGVhOS04ZGZiLWZkOGU1MDg3ZmE5MAAQAHQCBS2m%2B%2FVAjAc%2FWSCjQEQ%3D


# There may also be some relevant information in the below:


## Transition probabilities and hazard ratios


# "Note: To calculate the probability of dying from S1 and S2, use the hazard ratios provided. To do so, first convert the probability of dying from healthy, p_HD , to a rate; then multiply this rate by the appropriate hazard ratio; finally, convert this rate back to a probability. Recall that you can convert between rates and probabilities using the following formulas: r = − log(1 − p) and p = 1 − e ( − rt ) . The package darthtools also has the functions prob_to_rate and rate_to_prob that might be of use to you." per: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_25\3_cSTM - history dependence_material\Download exercise handout 

# ?rate_to_prob will tell you more about this function.
# ?prob_to_rate will tell you more about this function.

# You can see conversions from probabilities to rates here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Model Calibration in R\UNZIP to Working Directory_Calibration Participant Materials\ISPOR Calibration Participant Materials\SickSicker_MarkovModel_Function.R

# As will the 50 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv


# The above also describes how to convert probabilities for different time scales, i.e., convert a probability for 5 years to 1 year, etc., and how to convert data that exists as a rate to a probability for use in a Markov model.


# The intial probabilities before I decided to do parametric survival analysis:

# p_HS_SoC  <- 0.05  # probability of becoming OS when PFS, conditional on surviving, under standard of care
# p_HS_trtA <- 0.04  # probability of becoming OS when PFS, conditional on surviving, under EPI Assay
# p_HS_trtB <- 0.02  # probability of becoming OS when PFS, conditional on surviving, under HDX Assay
# p_SD      <- 0.1   # probability of dying          
# p_HD      <- 0.01  # probability of dying when PFS


# H = HEALTHY (PFS) -> HS MEANS HEALTHY TO SICK, HD -> MEANS HEALTHY TO DEAD.
# S = SICK (OS) -> SD MEANS SICK TO DEAD.
# D = DEAD (DEAD) 

# trtA -> Means the first intervention I am studying, i.e. treatment A or the first assay.
# trtB -> Means the second intervention I am studying, i.e. treatment B or the second assay.

# To add age specific mortality to our model, we would use this #03 input model parameters of:

# "C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modeling for Public Health_DARTH\5_Nov_29\4_Cohort state-transition models (cSTM) - time-dependent models_material\Markov_3state_time"

# with the 55 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\Live Session Recording\Live Session Recording August 24th.mp4

# and this would allow us to create a vector of transition probabilities for p_HD above, i.e., from PFS to dead, that is a little bit larger at each cycle, starting at our chosen minimum value at the first cycle and increasing each cycle until it reaches our chosen maximum value at the last cycle.

# Alternatively, C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\4_cSTM - time-dependent models_material shows you how to use a life-table, as does the material from the York course, but I really think there's no need to get that detailed in our own analysis.




# So, I now need to change p_FD_SoC and p_FD_Exp to reflect OS -> DEAD, becuase I now REALISE HERE TEAT P_PD ISNT THE PROBABILITY OF PROGRESSION TO DEAD, BUT OF PFS TO DEAD, OF FIRST LINE TO DEAD, BECAUSE OUR APD CURVES ONLY EVER DESCRIBE FIRST LINE TREATMENT, BE THAT FIRST LINE SOC TREATMENT OR FIRST LINE EXP TREATMENT.
 # and the FD below instead reflects the probability of going to death in the second line treatment, and I make the assumption that everyone gets the same second line treatment and give them the same probability under exp and under SoC to go from the second line therapy into dead.

P_OSD_SoC   <- 0.17 # Probability of dying when in OS.
P_OSD_Exp   <- 0.17 # Probability of dying when in OS.

# When digitising a progression-free survival curve everyone on that curve is on PFS, and if they leave that curve it is because their disease has progressed (so they've gone from progression free to progression, or from PFS to OS) or they've died:

# "Progression-free survival (PFS) is defined as the time from random assignment in a clinical trial to disease progression or death from any cause." https://www.ncbi.nlm.nih.gov/books/NBK137763/

# "Progression-free survival (PFS) is "the length of time during and after the treatment of a disease, such as cancer, that a patient lives with the disease but it does not get worse". https://en.wikipedia.org/wiki/Progression-free_survival#:~:text=Progression-free%20survival%20(PFS),it%20does%20not%20get%20worse".

# Time to Progression: 

# "The length of time from the date of diagnosis or the start of treatment for a disease until the disease starts to get worse or spread to other parts of the body. In a clinical trial, measuring the time to progression is one way to see how well a new treatment works. Also called TTP." https://www.cancer.gov/publications/dictionaries/cancer-terms/def/time-to-progression

# Koen's data is time to progression data, so we can't figure out time to death from it. It may be possible to do this with the data I end up using, or as advised by the C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv folder, but at the moment I'll just set PFS to Dead as a time-constant transition probability as he does. [I UPDATED THIS ABOVE]

# p_PFSD_SoC <- PFS TO DEAD
# p_FP_SoC <-  PFS TO OS
# 
# 
# 
# p_PFSD_Exp <- PFS TO DEAD
# p_FP_Exp <- PFS TO OS
# 
# 
# p_FD_SoC   <- Probability of dying when in OS.
# p_FD_Exp   <- Probability of dying when in OS.


# I could probably apply parametric survival models to OS data, as all individuals from the OS curve were under standard of care, and get free to dead that way from the data.



# PFS -> PFS:

#p_PFS_SoC  <-   (1 - p_FA1_STD)* (1 - p_FA2_STD)* (1 - p_FA3_STD)* (1 - p_FP_SoC) * (1 - p_PFSD_SoC) 
#p_PFS_Exp  <-   (1 - p_PFSD_Exp) * (1 - p_FP_Exp)* (1 - p_FA1_EXPR)* (1 - p_FA2_EXPR)* (1 - p_FA3_EXPR)

# First, I need to create progression free survival probabilities under standard of care.

# It's 1- p_PFSD_SoC, because if you're not going from healthy to dead then you're staying in healthy (i.e. you're not going from PFS to Dead, so you're staying in PFS) so this captures all the people leftover in PFS after those going to dead, and again for 1- p_FP_SoC, if you're not going from healthy to sick, the only other way out of healthy per the transition probability matrix, then you're staying in PFS, so 1- p_FP_SoC takes away all the people who went from healthy to sick, and leaves behind all the people who stayed in healthy (or takes away all the people who went to OS and leaves behind all the people that went to PFS per my model). I also add in the three adverse events as well.


# PFS -> OS:
# 
# p_PFS_OS_SoC    <- (1 - p_PFSD_SoC) * (1 - p_FA1_STD)* (1 - p_FA2_STD)* (1 - p_FA3_STD)* p_FP_SoC
# 
# 
# p_PFS_OS_Exp    <- (1 - p_PFSD_Exp) * (1 - p_FA1_EXPR)* (1 - p_FA2_EXPR)* (1 - p_FA3_EXPR) *  p_FP_Exp
# 
# 




# We have to get the probability for going from PFS to OS, by getting all the people left in PFS after the people who went to death were gone, and then we multiply this probability by the probability of going from healthy to sick, or PFS to OS, because it's the probability conditional on being alive, the probability conditional on being left in the PFS state after the other people who went to the death state are gone, so the probability of going from PFS to OS, conditional on surviving.


# p_PFS_OS_SoC is the probability of transitioning from healthy to sick, conditional on surviving, so it's defined as a conditional probability. So, what the Markov model wants is : m_P_SoC["PFS", "OS"] what's the overall probability of transitioning from PFS to OS, and so that is actually not a conditional probability, which is why we multiply the probability of surviving (1 - p_HD) *      p_HS_SoC by the probability of going to OS conditional on surviving, because the transition in the model should be the marginal not the conditional, i.e. you want to have the end probability.


#So, because you took a probability that was conditional from the literature, or digitised a PFS curve to get a transition probability - and those probabilities come from people who were necessarily still in the PFS state when you digitise a PFS curve, so those are transition probabilities conditional on being in the PFS (or healthy) state - you need to do the multiplication to give you a probability you can put in your transition matrix that reflects that the probability you are working with is conditional.


# # PFS -> DEAD:
# 
# p_PFSD_SoC  <-   (p_PFS_SoC) * (p_PFSD_SoC)
# p_PFSD_Exp  <-   (p_PFS_Exp) * (p_PFSD_Exp)



# Adverse event transition probabilities have all been conditional, this is to reflect that probabilities on transitioning from adverse event to death will necessarily be conditional probabilities - as they come from individuals who were studied in the adverse event group - so they are necessarily probabilities conditional on experiencing the adverse event. Likewise, the probability of experiencing the adverse event when under treatment is conditional on being in PFS for the treatment in the first place, because it will come from reports of adverse events in studies of the treatments given that we look at. 


# Which means conditional probabilities kind of don't matter too much, because the probability won't be applied to anyone who isnt in the state it doesnt necessarily need to be conditional after all. And I guess it would have never been applied to anyone who wasnt in the state, because probabilities are only applied to people who are in the state when the transition probability matrix is multiplied by the cohort trace of people in the state.



# p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD


# Probability of AE1 when PFS, conditional on surviving, under standard of care



#p_A1D_SoC  <- 0.001 

# Probability of going from AE1 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.



#p_A1F_SoC  <- 1-p_A1D_SoC

# Probability of returning from AE1 to PFS is 100% minus the people who have gone into the dead state.




# I repeat this for the other adverse events and for both standard of care and the experimental (novel) treatment:


# PFS -> PFS:
# 
# p_PFS_SoC  <-   (1 - p_FA1_STD)* (1 - p_FA2_STD)* (1 - p_FA3_STD)* (1 - p_PFSOS_SoC) * (1 - p_PFSD_SoC) 
# p_PFS_Exp  <-   (1 - p_PFSD_Exp) * (1 - p_PFSOS_Exp)* (1 - p_FA1_EXPR)* (1 - p_FA2_EXPR)* (1 - p_FA3_EXPR)
# 
# 
# # Probability of A2 when PFS, conditional on surviving, under standard of care
# p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD
# # Probability of going from A2 to death
# p_A1D_SoC  <- 0.001 
# 
# 
# # Probability of A2 when PFS, conditional on surviving, under standard of care
# p_FA2_SoC  <- (p_PFS_SoC) * p_FA2_STD
# # Probability of going from A2 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
# p_A2D_SoC  <- 0.001 
# # Probability of returning from A2 to PFS is 100% minus the people who have gone into the dead state.
# p_A2F_SoC  <- 1-p_A2D_SoC
# 
# # Probability of A3 when PFS, conditional on surviving, under standard of care
# p_FA3_SoC  <- (p_PFS_SoC) * p_FA3_STD
# # Probability of going from A3 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
# p_A3D_SoC  <- 0.001 
# # Probability of returning from A3 to PFS is 100% minus the people who have gone into the dead state.
# p_A3F_SoC  <- 1-p_A3D_SoC
# 
# 
# 
# 
# 
# 
# # Probability of A1 when PFS, conditional on surviving, under standard of care
# p_FA1_EXPR  <- (p_PFS_Exp) * p_FA1_EXPR
# # Probability of going from A1 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
# p_A1D_Exp  <- 0.001 
# # Probability of returning from A1 to PFS is 100% minus the people who have gone into the dead state.
# p_A1F_Exp  <- 1-p_A1D_Exp
# 
# # Probability of A2 when PFS, conditional on surviving, under standard of care
# p_FA2_Exp  <- (p_PFS_Exp) * p_FA2_EXPR
# # Probability of going from A2 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
# p_A2D_Exp  <- 0.001 
# # Probability of returning from A2 to PFS is 100% minus the people who have gone into the dead state.
# p_A2F_Exp  <- 1-p_A2D_Exp
# 
# # Probability of A3 when PFS, conditional on surviving, under standard of care
# p_FA3_Exp  <- (p_PFS_Exp) * p_FA3_EXPR
# # Probability of going from A3 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
# p_A3D_Exp  <- 0.001 
# # Probability of returning from A3 to PFS is 100% minus the people who have gone into the dead state.
# p_A3F_Exp  <- 1-p_A3D_Exp
# 
# 
# # These all have to be conditional on survival, i.e. conditional on being in PFS, this is to ensure that I am only applying adverse events to the people still in the PFS, after transitions etc., have happened. --> This is not how conditional probabilities work, I have the correct interpretation of them elsewhere in this code file.






















## Health State Values (AKA State rewards)
# Costs and utilities  
# Basically the outcomes we are interested in coming out of this model, so we'll look at the cohorts costs over the time horizon and the quality adjusted life years in our cohort over this time horizon.

# Costs

c_PFS_Folfox <-307.81
c_PFS_Bevacizumab <-2580.38
c_OS_Folfiri <-326.02
administration_cost <- 365.00


c_F_SoC       <- administration_cost + c_PFS_Folfox  # cost of one cycle in PFS state under standard of care
c_F_Exp       <- administration_cost + c_PFS_Folfox + c_PFS_Bevacizumab # cost of one cycle in PFS state under the experimental treatment 
#c_F_Ex_trtA       <- 1000  # cost of one cycle in PFS state under the experimental treatment A 
#c_F_Ex_trtB       <- 1000  # cost of one cycle in PFS state under the experimental treatment
c_P       <- c_OS_Folfiri  + administration_cost# cost of one cycle in progression state (I assume in OS everyone gets the same treatment so it costs everyone the same to be treated).
c_D       <- 0     # cost of one cycle in dead state



# Above is the cost for each state, PFS, OS and dead,

# c_trtA    <- 800   # cost of EPI Assay in PFS state (ONCE OFF COST)
# c_trtB    <- 1500  # cost of HDX Assay in PFS state (ONCE OFF COST)
# 
# # We make the cost of the assays above so that when we have treatment strategies we can add this cost of treatment to anyone whose being treated when they receive the treatment.


# Then we define the utilities per health states.


u_F       <- 0.850     # utility when PFS 
u_P       <- 0.650   # utility when OS
u_D       <- 0     # utility when dead




# Discounting factors
d_c             <- 0.04                          
# discount rate for costs (per year)
d_e             <- 0.04                          
# discount rate for QALYs (per year)

# discount rate per cycle equal discount of costs and QALYs by 4%

# Discount weight (equal discounting is assumed for costs and effects)

# Actually I've updated this and do this later on now

# v_dwc <- 1 / (1 + d_c) ^ (0:n_cycle) 
# v_dwe <- 1 / (1 + d_e) ^ (0:n_cycle) 

# So, we create a discount weight vector above, to understand the way this works I'll have to return to my York notes on discounting

p_PFSD_SoC
p_PFSD_Exp

p_PFSOS_SoC
p_PFSOS_Exp
```

Discount rate for costs and utilities I need to return to it more generally with the notes I wrote from York on discounting.
Also I set up 3 strategies, i.e. standard of care, EPI Assay and HDX Assay, to reflect the two assays under study, but it would be very easy to change this to just 2 strategies above.

I also want to add in the costing per the York model, as this broke costs down before adding them, although I can compare this to the York approach to costing in their published article, i.e. are costs in that article combined before they are added to the model or afterwards?

There are some things that I would like to appear in the code chunks, but not in the knitted document.
To do this I can go to:

<https://stackoverflow.com/questions/47710427/how-to-show-code-but-hide-output-in-rmarkdown>

and

<https://stackoverflow.com/questions/48286722/rmarkdown-how-to-show-partial-output-from-chunk?rq=1>

Although I can probably just use:

knitr::opts_chunk\$set(echo = TRUE, warning = FALSE, message = FALSE, eval = T)

But set echo = false

Or even better, click the gear on each code chunk and decide if I would like that code chunk to show things or not.

If I was interested in how to add adverse events to the model, Eva describes how to create an additional state that is Sick+AdverseEvent here:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv

## Draw the state-transition cohort model

```{r}
# 
# diag_names_states  <- c("PFS", "OS", "Dead")  
# 
# m_P_diag <- matrix(0, nrow = n_states, ncol = n_states, dimnames = list(diag_names_states, diag_names_states))
# 
# m_P_diag["PFS", "PFS" ]  = ""
# m_P_diag["PFS", "OS" ]     = "" 
# m_P_diag["PFS", "Dead" ]     = ""
# m_P_diag["OS", "OS" ]     = ""
# m_P_diag["OS", "Dead" ]     = ""
# m_P_diag["Dead", "Dead" ]     = ""
# layout.fig <- c(1, 3, 2) # <- changing the numbers here changes the diagram layout, so mess with these until I'm happy. It basically decides how many bubbles will be on each level, so here 1 bubble, followed by 3 bubbles, followed by 2 bubbles, per the diagram for 1, 3, 2.
# plotmat(t(m_P_diag), t(layout.fig), self.cex = 0.5, curve = 0, arr.pos = 0.64,  
#         latex = T, arr.type = "curved", relsize = 0.85, box.prop = 0.9, 
#         cex = 0.8, box.cex = 0.7, lwd = 0.6, main = "Figure 1")
```

# 04 Define and initialize matrices and vectors

After setting up our parameters above, we initialise our structure below.

This is where we will store all of the model output, and all the things that we need to track over time as we are simulating the progression of this cohort through this disease process.

## 04.1 Cohort trace

```{r}


# WHEN COMING BACK TO COMPARE: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\ISPOR WEBINAR Health Economic Modelling in R\ISPOR_webinar_R-master\ISPOR_webinar_R-master\oncologySemiMarkov_illustration to this Rmarkdown document, a big difference is that this document creates a cycle 0 - whereas the comparison from the ISPOR_webinar_R uses -1 to get into cycle 0 where necessary. I have decided to follow the ISPOR way, because I am interested in duplicating their parametric analysis, so I need to bear this difference in mind as I go through this document.

# Also, continue from Verbose in this and in Koens file

# Markov cohort trace matrix ----

# Initialize matrices to store the Markov cohort traces for each strategy

# - note that the number of rows is n_cycle + 1, because R doesn't use index 0 (i.e. cycle 0)  --> What we mean here, is that when we do our calculations later they need to be for cycle-1 to reflect cycle 0.
m_M_SoC <- m_M_Exp  <-  matrix(
  data = NA, 
  nrow = n_cycle,  
  ncol = n_states, 
  dimnames = list(paste('Cycle', 1:n_cycle), v_names_states)
)

## Initial state vector
# We create an inital vector where people start, with everyone (1 = 100% of people) starting in PFS below:
# v_s_init <- c("PFS" = 1, "OS" = 0, "Dead" = 0)
# v_s_init

# There are cases where you can have an initial illness prevalence, so you would start some people in the sick state and some people in the healthy state, but above we're looking at people with mCRC, so we'll start everyone in PFS.


## Initialize cohort trace for cSTM (cohort state transition model) for all strategies (the strategies are the treatment strategies SOC, treatment A and Treatment B).
# So, basically we are creating a matrix to trace how the cohort is distributed across the health states, over time. 

# A matrix is necessary because there are basically two dimensions to this, the number of time cycles, which will be our rows, and then the number of states - to know which proportion of our cohort is in each state at each time:

# m_M_SoC <- matrix(0, 
#                   nrow = (n_cycles + 1), ncol = n_states, 
#                   dimnames = list(v_names_cycles, v_names_states))
# Above instead of having to bother with -1 throughout the analysis they create a cycle 0.

# Store the initial state vector in the first row of the cohort trace
# m_M_SoC[1, ] <- v_s_init
## Initialize cohort traces
## So, above I made the cohort trace for standard of care, because in my analysis all my patients start in the PFS state, I can duplicate that below to create the cohort trace for treatment A and treatment B.
# m_M_trtA <- m_M_trtB <- m_M_SoC # structure and initial states remain the same

# This gives us three matrices, m_M_trtA, m_M_trtB and m_M_SoC, that we can fill in with our simulations of how patients transitions between health states under each treatment strategy.

# In the first row of the markov matrix [1, ] put the value at the far end, i.e. "<-1" and "<-0" under the colum "PFS" [ , "PFS"], repeating this for "OS", "AE1", "AE2" "AE3" and "Dead".


# Specifying the initial state for the cohorts (all patients start in PFS)
m_M_SoC[1, "PFS"] <- m_M_Exp[1, "PFS"] <- 1
m_M_SoC[1, "OS"]  <- m_M_Exp[1, "OS"]  <- 0
m_M_SoC[1, "Dead"]<- m_M_Exp[1, "Dead"]  <- 0

# Inspect whether properly defined
head(m_M_SoC)
head(m_M_Exp)
#head(m_M_Exp_trtB)
```

## 04.2 Transition probability matrix

```{r}

## If there were time varying transition probabilities, i.e. the longer you are in the model there are changes in your transition probability into death as you get older, etc., you would build a transition probability array, rather than a transition probability matrix, per: 

# 04.2 of:

# "C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modeling for Public Health_DARTH\5_Nov_29\4_Cohort state-transition models (cSTM) - time-dependent models_material\Markov_3state_time"

# with the 1hour: 02minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\Live Session Recording\Live Session Recording August 24th.mp4


## Initialize transition probability matrix, [i.e. build the framework or empty scaffolding of the transition probability matrix]
# all transitions to a non-death state are assumed to be conditional on survival
# - starting with standard of care
# - note that these are now 3-dimensional matrices because we are including time.
 

# m_P_SoC  <- matrix(0,
#                    nrow = n_states, ncol = n_states,
#                    dimnames = list(v_names_states, v_names_states)) # define row and column names
# m_P_SoC


# Initialize matrices for the transition probabilities
# - note that these are now 3-dimensional matrices (so, above we originally included dim = nrow and ncol, but now we also include n_cycle - i.e. the number of cycles).
# - starting with standard of care
m_P_SoC <- array(
  data = 0,
  dim = c(n_states, n_states, n_cycle),
  dimnames = list(v_names_states, v_names_states, paste0("Cycle", 1:n_cycle))
  # define row and column names - then name each array after which cycle it's for, i.e. cycle 1 all the way through to cycle 143. So Cycle 1 will have all of our patients in PFS, while cycle 143 will have most people in the dead state.
)

head(m_P_SoC)


m_P_Exp <- array(
  data = 0,
  dim = c(n_states, n_states, n_cycle),
  dimnames = list(v_names_states, v_names_states, paste0("Cycle", 1:n_cycle))
  # define row and column names - then name each array after which cycle it's for, i.e. cycle 1 all the way through to cycle 143. So Cycle 1 will have all of our patients in PFS, while cycle 143 will have most people in the dead state.
)

head(m_P_Exp)





```

## 04.3 Fill in the transition probability matrix:

```{r}

# It looks like I will need to decide if I have to go back to the start of putting everything in the model and put stuff in for not just the experimental strategy, but for experimental strategy 1 AND experimental strategy 2 individually.

# 
# AE1MARK     <- p_FA1_STD* (1 - p_FA2_STD)* (1 - p_FA3_STD)* (1 - p_FP_SoC) * (1 - p_PFSD_SoC)
# AE2MARK     <- (1 - p_FA1_STD)* p_FA2_STD* (1 - p_FA3_STD)* (1 - p_FP_SoC) * (1 - p_PFSD_SoC)
# AE3MARK   <- (1 - p_FA1_STD)* (1 - p_FA2_STD)* p_FA3_STD* (1 - p_FP_SoC) * (1 - p_PFSD_SoC)

# If I really struggle with the transition probabilities below, I could build them from here alternatively: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Applied Cost Effectiveness Modeling with R\rcea-rscripts\01-markov-cohort

# A better way to fill this may be here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Model Calibration in R\UNZIP to Working Directory_Calibration Participant Materials\ISPOR Calibration Participant Materials\SickSicker_MarkovModel_Function.R

# This also gives advice on filling this in: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\September-Workshop-main\September-Workshop-main\manuscript\Appendix_code_walkthrough_Markov_Tutorial_Part2

# Andy gave me advice on how to add adverse event disutility and costs, but I could also think about the strategy taken here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\September-Workshop-main\September-Workshop-main\R\Functions_cSTM_time_dep_simulation

# Setting the transition probabilities from PFS based on the model parameters
  # So, when individuals are in PFS what are their probabilities of going into the other states that they can enter from PFS?

m_P_SoC["PFS", "PFS",]<- (1 -p_PFSOS_SoC) * (1 - p_PFSD_SoC)
m_P_SoC["PFS", "OS",]<- p_PFSOS_SoC*(1 - p_PFSD_SoC)
m_P_SoC["PFS", "Dead",]<-p_PFSD_SoC

# m_P_SoC["PFS", "PFS", ] <-   1 - (p_PFSD_SoC + p_FP_SoC)
# m_P_SoC["PFS", "OS", ]     <-  1 - (1-p_FP_SoC + p_PFSD_SoC)
# m_P_SoC["PFS", "Dead", ]            <-  1 - (p_FP_SoC + 1 - p_PFSD_SoC)


#m_P_SoC["PFS", "PFS", ] <-   1 - (p_PFSD_SoC + p_FP_SoC)
#m_P_SoC["PFS", "OS", ]     <- p_FP_SoC* (1 - p_PFSD_SoC)
#m_P_SoC["PFS", "Dead", ]            <-  (1 - p_FP_SoC) * (p_PFSD_SoC)

# Setting the transition probabilities from OS
m_P_SoC["OS", "OS", ] <- 1 - P_OSD_SoC
m_P_SoC["OS", "Dead", ]        <- P_OSD_SoC


# Setting the transition probabilities from Dead
m_P_SoC["Dead", "Dead", ] <- 1

# 
# # Setting the transition probabilities from AE1
# m_P_SoC["AE1", "PFS", ] <- p_A1F_SoC
# m_P_SoC["AE1", "Dead", ] <- p_A1D_SoC
# 
# # Setting the transition probabilities from AE2
# m_P_SoC["AE2", "PFS", ] <- p_A2F_SoC
# m_P_SoC["AE2", "Dead", ] <- p_A2D_SoC
# 
# # Setting the transition probabilities from AE3
# m_P_SoC["AE3", "PFS", ] <- p_A3F_SoC
# m_P_SoC["AE3", "Dead", ] <- p_A3D_SoC


m_P_SoC

# Using the transition probabilities for standard of care as basis, update the transition probabilities that are different for the experimental strategy

# 
# m_P_Exp["PFS", "PFS", ] <- p_PFS_Exp
# m_P_Exp["PFS", "AE1", ]     <- p_FA1_Exp
# m_P_Exp["PFS", "AE2", ]     <- p_FA2_Exp
# m_P_Exp["PFS", "AE3", ]     <- p_FA3_Exp
# m_P_Exp["PFS", "OS", ]     <- p_PFS_OS_Exp
# m_P_Exp["PFS", "Dead", ]            <- p_PFSD_Exp
# 
# 
# # Setting the transition probabilities from OS
# m_P_Exp["OS", "OS", ] <- 1 - P_OSD_Exp
# m_P_Exp["OS", "Dead", ]        <- P_OSD_Exp
# 
# # Setting the transition probabilities from Dead
# m_P_Exp["Dead", "Dead", ] <- 1
# 

# Setting the transition probabilities the experimental strategy
m_P_Exp["PFS", "PFS",]<- (1 -p_PFSOS_Exp) * (1 - p_PFSD_Exp)
m_P_Exp["PFS", "OS",]<- p_PFSOS_Exp*(1 - p_PFSD_Exp)
m_P_Exp["PFS", "Dead",]<-p_PFSD_Exp

# Setting the transition probabilities from OS
m_P_Exp["OS", "OS", ] <- 1 - P_OSD_Exp
m_P_Exp["OS", "Dead", ]        <- P_OSD_Exp

# Setting the transition probabilities from Dead
m_P_Exp["Dead", "Dead", ] <- 1






# # Setting the transition probabilities from AE1
# m_P_Exp["AE1", "PFS", ] <- p_A1F_Exp
# m_P_Exp["AE1", "Dead", ] <- p_A1D_Exp
# 
# # Setting the transition probabilities from AE2
# m_P_Exp["AE2", "PFS", ] <- p_A2F_Exp
# m_P_Exp["AE2", "Dead", ] <- p_A2D_Exp
# 
# # Setting the transition probabilities from AE3
# m_P_Exp["AE3", "PFS", ] <- p_A3F_Exp
# m_P_Exp["AE3", "Dead", ] <- p_A3D_Exp
# 
m_P_Exp


# If I wanted to round my transition matrix so things sum exactly to 1, instead of 0.99999999 (in cases where this was happening):
#round(m_P_SoC, digits=2) 
#m_P_SoC


```

```{r}

# An explanation of conditional probabilities:

## Standard of Care
# from PFS
# m_P_SoC["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_SoC)

# It's 1- p_HD, because if you're not going from healthy to dead then you're staying in healthy (i.e. you're not going from PFS to Dead, so you're staying in PFS) so this captures all the people leftover in PFS after those going to dead, and again for 1- p_HS_SoC, if you're not going from healthy to sick, the only other way out of healthy per the transition probability matrix, then you're staying in PFS, so 1- p_HS_SoC takes away all the people who went from healthy to sick, and leaves behind all the people who stayed in healthy (or takes away all the people who went to OS and leaves behind all the people that went to PFS per my model).

# m_P_SoC["PFS", "OS"]    <- (1 - p_HD) *      p_HS_SoC

# This is because, when setting up the input parameters we didn't have a value for staying in progression free survival, so we have to calculate it, i.e. above we have to get the probability for going from PFS to OS, by getting all the people left in PFS after the people who went to death were gone, and then we multiply this probability by the probability of going from healthy to sick, or PFS to OS, because it's the probability conditional on being alive, the probability conditional on being left in the PFS state after the other people who went to the death state are gone, so the probability of going from PFS to OS, conditional on surviving.



# m_P_SoC["PFS", "OS"]    <- (1 - p_HD) *      p_HS_SoC

# p_HS_SoC is the probability of transitioning from healthy to sick, conditional on surviving, so it's defined as a conditional probability. So, what the Markov model wants is : m_P_SoC["PFS", "OS"] what's the overall probability of transitioning from PFS to OS, and so that is actually not a conditional probability, which is why we multiply the probability of surviving (1 - p_HD) *      p_HS_SoC by the probability of going to OS conditional on surviving, because the transition in the model should be the marginal not the conditional, i.e. you want to have the end probability.


#So, because you took a probability that was conditional from the literature, or digitised a PFS curve to get a transition probability - and those probabilities come from people who were necessarily still in the PFS state when you digitise a PFS curve, so those are transition probabilities conditional on being in the PFS (or healthy) state - you need to do the multiplication to give you a probability you can put in your transition matrix that reflects that the probability you are working with is conditional.


# Adverse event transition probabilities have all been conditional, this is to reflect that probabilities on transitioning from adverse event to death will necessarily be conditional probabilities - as they come from individuals who were studied in the adverse event group - so they are necessarily probabilities conditional on experiencing the adverse event. Likewise, the probability of experiencing the adverse event when under treatment is conditional on being in PFS for the treatment in the first place, because it will come from reports of adverse events in studies of the treatments given that we look at. 


# Which means conditional probabilities kind of don't matter too much, because the probability won't be applied to anyone who isnt in the state it doesnt necessarily need to be conditional after all. And I guess it would have never been applied to anyone who wasnt in the state, because probabilities are only applied to people who are in the state when the transition probability matrix is multiplied by the cohort trace of people in the state.




# m_P_SoC["PFS", "Dead"]    <-      p_HD

# Your probability of going from healthy to dead is not conditional on surviving, per the input parameters section.


# from OS
# m_P_SoC["OS", "OS"] <- 1 - p_SD
# m_P_SoC["OS", "Dead"] <-     p_SD

# Per the input parameters, your probability of going from sick to dead is also not conditional on surviving, so we dont need to multiply our probability conditional on surviving by the number of survivors as above.

# That's why though, our PFS to OS probability calculated through multiplication above is so close to the 0.05 in the input parameters, i.e. it's 0.0495, because we multiply it by 1 - p_HD or 1-0.01, which is basically multiplying it by a number very close to 1, so the probability remains very close to where it started off. The probability is conditional on the numbers of people in the alive state, we calculate that to be nearly 100% of people, so the probability remains the same. 


# m_P_SoC["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_SoC)
# And the first one (commented out above) is the same, it's the probability of going from healthy to healthy which, because it's a conditional probability, is the number of individuals left in healthy multiplied by the probability of going from healthy to health conditional on surviving to be in healthy (so, to get a conditional probability we multiply the numbers who have survived by the probability -> here we don't have a probability for healthy to healthy, but we do have a probability for health to sick, so taking 1 - this gives us the probability of not going into sick, i.e. of staying in healthy instead). 

# from Dead
# m_P_SoC["Dead", "Dead"] <- 1

# Once you're in dead you stay in dead, you can't come back to life.


# For handiness, we just take the treatment matrix that already exists for standard of care above, copy it as treatment A and treatment B, and then copy over it with new values specific to treatment A and treatment B, assuming that the probability of going from PFS to dead and staying in OS and going from OS to dead are the same as for standard of care (dead to dead is definitely the same as no one leaves dead) that's likely a reasonable assumption for our model, as once people progress we can assume they all go on to the same next treatment, standard of care progression treatment with the same likelihood of staying in OS and of going to the dead state, regardless of the novel therapy they were on:

# ## EPI Assay
# m_P_trtA <- m_P_SoC
# m_P_trtA["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_trtA)
# m_P_trtA["PFS", "OS"]    <- (1 - p_HD) *      p_HS_trtA
# 
# ## HDX Assay
# m_P_trtB <- m_P_SoC
# m_P_trtB["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_trtB)
# m_P_trtB["PFS", "OS"]    <- (1 - p_HD) *      p_HS_trtB



# You can also think of it, per the 1hour: 05minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\Live Session Recording\Live Session Recording August 24th.mp4


# m_P_SoC["PFS", "OS"]    <- (1 - p_HD) [<- if you don't die] *      p_HS_SoC [then you have a risk of getting sick]

# It wont always be necessary to do this, i.e., to include conditional probabilities, it will depend on the data used and how things were estimated.

# This approach is taken for conditional probabilities, it's probably more relevant for modelling people to really, really old ages, where their mortality gets really high (so this is particularly relevant when you have age specific mortality), where you still have this high mortality of getting sick, you can end up in problems with the probabilities where the probability of leaving the state gets too high, because you haven't properly adjusted for these really high mortality rates, as people get really old.

# So, what this is doing is saying, if you don't die, then it's the remaining people who experience the probability of getting sick, so you're not adding probabilities together, which can be problematic, because sometimes you can have probabilities come to greater than one (i.e., larger than 100%).

# It's always worth considering, if you're extrapolating this risk of illness that's pretty high, on top of a cohort that also has a high risk of death, maybe that's not the right extrapolation, but this is another kind of fail safe to avoid that.

# Depending on how the data is estimated, this a correct interpretation of the probability estimate of going from one state to another (p_HS_SoC, or healthy to sick, or PFS to OS). 


# Using conditional probabilities more generally is also discussed around the 1:04 hour mark of:

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv



```

I may want to get rid of the conditional probabilities above, replacing them with standard transition probabilities per my York training.

## 04.4 Check if transition probability matrices are valid.

```{r}
# This is a check in the DARTH tools package that all the transition probabilities are in [0, 1], i.e., no probabilities are greater than 100%.
# This works as follows, according to line 205 of: https://github.com/DARTH-git/cohort-modeling-tutorial-intro/blob/main/analysis/cSTM_time_indep.R
# 
# ## Check if transition probability matrices are valid ----
# #* Functions included in "R/Functions.R". The latest version can be found in `darthtools` package
# ### Check that transition probabilities are [0, 1] ----
# check_transition_probability(m_P,      verbose = TRUE)  # m_P >= 0 && m_P <= 1
# check_transition_probability(m_P_strA, verbose = TRUE)  # m_P_strA >= 0 && m_P_strA <= 1
# check_transition_probability(m_P_strB, verbose = TRUE)  # m_P_strB >= 0 && m_P_strB <= 1
# check_transition_probability(m_P_strAB, verbose = TRUE) # m_P_strAB >= 0 && m_P_strAB <= 1
# ### Check that all rows sum to 1 ----
# check_sum_of_transition_array(m_P,      n_states = n_states, n_cycles = n_cycles, verbose = TRUE)  # rowSums(m_P) == 1
# check_sum_of_transition_array(m_P_strA, n_states = n_states, n_cycles = n_cycles, verbose = TRUE)  # rowSums(m_P_strA) == 1
# check_sum_of_transition_array(m_P_strB, n_states = n_states, n_cycles = n_cycles, verbose = TRUE)  # rowSums(m_P_strB) == 1
# check_sum_of_transition_array(m_P_strAB, n_states = n_states, n_cycles = n_cycles, verbose = TRUE) # rowSums(m_P_strAB) == 1
# 




# UNCOMMENT THIS! 
check_transition_probability(m_P_SoC,  verbose = TRUE)
# UNCOMMENT THIS! 
check_transition_probability(m_P_Exp,  verbose = TRUE)
#check_transition_probability(m_P_trtA, verbose = TRUE)
#check_transition_probability(m_P_trtB, verbose = TRUE)
# Check that all rows sum in each matrix sum to 1 -> which we know is a necessary condition for transition probability matrices.
# UNCOMMENT THIS! 
check_sum_of_transition_array(m_P_SoC,  n_states = n_states, n_cycles = n_cycle, verbose = TRUE)
# UNCOMMENT THIS! 
check_sum_of_transition_array(m_P_Exp,  n_states = n_states, n_cycles = n_cycle, verbose = TRUE)
#check_sum_of_transition_array(m_P_trtA, n_states = n_states, n_cycles = n_cycle, verbose = TRUE)
#check_sum_of_transition_array(m_P_trtB, n_states = n_states, n_cycles = n_cycle, verbose = TRUE)

# This error message:
#   
#   Error in check_sum_of_transition_array(m_P_SoC, n_states = n_states, n_cycles = n_cycle,  : 
#   This is not a valid transition array 
# 
# Probably reflects that: 
#   
#   > m_P_SoC
# , , Cycle1
# 
#           PFS          OS        Dead       AE1       AE2       AE3
# PFS  0.974925 0.005074995 0.020000000 0.0194985 0.0194985 0.0194985
# [1] 1.058495
# 
# That is, that I've put just any old probability value in at the moment and they are summing to be larger than one, when I've changed this to the actual probabilities I'm sure it'll be OK.
# 
# Same with:
# 
#   , , Cycle28
# 
#            PFS        OS     Dead        AE1        AE2        AE3
# PFS  0.5804491 0.3995509 0.020000 0.01160898 0.01160898 0.01160898
# OS   0.0000000 0.5922950 0.407705 0.00000000 0.00000000 0.00000000
# Dead 0.0000000 0.0000000 1.000000 0.00000000 0.00000000 0.00000000
# AE1  0.9990000 0.0000000 0.001000 0.00000000 0.00000000 0.00000000  
#   
#   > 0.5804491 + 0.3995509 + 0.020000 + 0.01160898 + 0.01160898 + 0.01160898
# [1] 1.034827

# Inspect whether properly defined
# - note that we inspect for the first two cycles
m_P_SoC[ , , 1:2]
m_P_Exp[ , , 1:2]

# Does it visually sum to 1?

```

# 05 Run Markov model

Looking in the below where more than 100% can be in a health state (1.02 in PFS in cycle 4), it seems likely that it's the transitions from adverse events into PFS that could be causing the problems with my probabilities not summing to 1 as per verbose, as the only way to get into PFS is through one of the AE's.
I'll focus on this possibility when repairing the transition probabilities.

PFS AE1 AE2 AE3 OS Dead

Cycle 4 1.0233086 0.01933076 0.01933076 0.01933076 0.031888685 0.06024724

```{r}
# for (t in 1:n_cycles){  # Use a for loop to loop through the number of cycles, basically we'll calculate the cohort distribution at the next cycle [t+1] based on the matrix of where they were at time t, matrix multiplied by the transition probability matrix for the current cycle (constant for us as we use a constant transition probability matrix, rather than a transition probability array).
# We do this for each treatment, as they all have different transition probability matrices. 
#  m_M_SoC [t + 1, ] <- m_M_SoC [t, ] %*% m_P_SoC   # estimate the state vector for the next cycle (t + 1)
#  m_M_trtA[t + 1, ] <- m_M_trtA[t, ] %*% m_P_trtA  # estimate the state vector for the next cycle (t + 1)
#  m_M_trtB[t + 1, ] <- m_M_trtB[t, ] %*% m_P_trtB  # estimate the state vector for the next cycle (t + 1)
# }
# head(m_M_SoC)  # print the first few lines of the matrix for standard of care (m_M_SoC)


# Above I was originally using a transition probability matrix, because I was using a time constant transition probability. But now, because my transition probabilities come from the survival curves, and thus change over time, I am using a transition probability array. 

# Thus, I adapt the above to reflect that my transition probability selected has to come from a certain cycle, i.e. from a certain time, and then be multiplied by the number of people in the matrix, the amount of the cohort in the matrix, at that cycle, i.e. at that time. Thats why below I pick the third dimension of the array, not row, not column, but time: R,C,T i.e.   ->  , ,i_cycle



# So here I once again create the Markov cohort trace by looping over all cycles
# - note that the trace can easily be obtained using matrix multiplications
# - note that now the right probabilities for the cycle need to be selected, like I explained above.
for(i_cycle in 1:(n_cycle-1)) {
  m_M_SoC[i_cycle + 1, ] <- m_M_SoC[i_cycle, ] %*% m_P_SoC[ , , i_cycle]
  m_M_Exp[i_cycle + 1, ] <- m_M_Exp[i_cycle, ] %*% m_P_Exp[ , , i_cycle]
}


head(m_M_SoC)  # print the first few lines of the matrix for standard of care (m_M_SoC)
head(m_M_Exp)  # print the first few lines of the matrix for experimental treatment(m_M_Exp)

#m_M_SoC
m_M_SoC
#m_M_Exp
m_M_Exp

```

# 06 Compute and Plot Epidemiological Outcomes

## 06.1 Cohort trace

```{r}

# So, we'll plot the above Markov model for standard of care (m_M_SoC) to show our cohort distribution over time, i.e. the proportion of our cohort in the different health states over time.

# If I wanted to do the same for Treatment A and Treatment B, I would just copy this code chunk and replace m_M_SoC with m_M_trtA and m_M_trtB

# matplot(m_M_SoC, type = 'l', 
        # ylab = "Probability of state occupancy",
        # xlab = "Cycle",
        # main = "Cohort Trace", lwd = 3)  # create a plot of the data
# legend("right", v_names_states, col = c("black", "red", "green"), 
       # lty = 1:3, bty = "n")  # add a legend to the graph

# plot a vertical line that helps identifying at which cycle the prevalence of OS is highest
# abline(v = which.max(m_M_SoC[, "OS"]), col = "gray")
# The vertical line shows you when your sick (OS) population is the greatest that it will ever be, but it can be changed from which.max to other things (so it is finding which cycle the proportion sick is the highest and putting a vertical line there).

# So, you can see in the graph everyone starts in the PFS state, but that this falls over time as people progress and leave this state, then you see OS start to peak up but then fall again as people leave this state to go into the dead state, which is an absorbing state and by the end will include everyone.

# Plotting the Markov cohort traces
matplot(m_M_SoC, 
        type = "l", 
        ylab = "Probability of state occupancy",
        xlab = "Cycle",
        main = "Makrov Cohort Traces",
        lwd  = 3,
        lty  = 1) # create a plot of the data
matplot(m_M_Exp, 
        type = "l", 
        lwd  = 3,
        lty  = 3,
        add  = TRUE) # add a plot of the experimental data ontop of the above plot
legend("right", 
       legend = c(paste(v_names_states, "(SOC)"), paste(v_names_states, "(Exp)")), 
       col    = rep(c("black", "red", "green"), 2), 
       lty    = c(1, 1, 1, 3, 3, 3), # Line type, full (1) or dashed (3), I have entered this 12 times here because we have 6 lines under standard of care (6 full lines) and 6 lines under experimental treatment (6 dashed lines)
       lwd    = 3,
       bty    = "n")

# plot a vertical line that helps identifying at which cycle the prevalence of OS is highest
abline(v = which.max(m_M_SoC[, "OS"]), col = "gray")
abline(v = which.max(m_M_Exp[, "OS"]), col = "black")
# The vertical line shows you when your sick (OS) population is the greatest that it will ever be, but it can be changed from which.max to other things (so it is finding which cycle the proportion sick is the highest and putting a vertical line there).

# (It's probably not necessary for my own analysis and I can comment these two lines out if I'm not going to use it).

# So, you can see in the graph everyone starts in the PFS state, but that this falls over time as people progress and leave this state, then you see OS start to peak up but then fall again as people leave this state to go into the dead state, which is an absorbing state and by the end will include everyone.


# There is a lot going on with this plot, and that gives me two options for when I do it with the actual data:

# 1. Do two separate plots for soc and exp, like in the plot I commented out at the start of this chunk.

# 2. copy your soc and exp matrixes but get rid of the adverse event states in the copy in order to simplify things, i.e. reasssign all the people in the "adverse event" states to be in the PFS state, because we know that only people from PFS were allowed to be in an adverse event.




# 
# 
# 
#      matplot(m_M_SoC, type = 'l', 
#                ylab = "Probability of state occupancy",
#                xlab = "Cycle",
#                main = "Cohort Trace", lwd = 3)  # create a plot of the data
#        legend("right", v_names_states, col = c("black", "red", "green"), 
#               lty = 1:3, bty = "n")  # add a legend to the graph








```

## 06.2 Overall Survival (OS)

Although in the context of my analysis this would be PFS + OS because it is drawn from the DARTH model where healthy and sick make up OS, while dead means not OS (obviously).

```{r}
# v_os <- 1 - m_M_SoC[, "Dead"]    # calculate the overall survival (OS) probability
# v_os <- rowSums(m_M_SoC[, 1:2])  # alternative way of calculating the OS probability

# I could do my own version of this and chose just to look at pfs, rather than column 1 and 2 to look at anyone not dead.

# i.e. v_os <- (m_M_SoC[, 1])

# best practice would be to rename v_os if I am looking at something that isnt os, i.e. v_pfs and to of course update the table legend, bearing in mind that yet again this is all for standard of care, and that if I wanted to know this for treatment a and/or treatment b I would need to replace the Markov model matrix above.


# plot(v_os, type = 'l', 
#     ylim = c(0, 1),
#     ylab = "Survival probability",
#     xlab = "Cycle",
#     main = "Overall Survival")  # create a simple plot showing the OS

# add grid 
# grid(nx = n_cycles, ny = 10, col = "lightgray", lty = "dotted", lwd = par("lwd"), 
#     equilogs = TRUE) 

# Calculating and plotting overal survival (OS)
v_OS_SoC <- 1 - m_M_SoC[, "Dead"]
v_OS_Exp <- 1 - m_M_Exp[, "Dead"]

plot(v_OS_SoC, 
     type = "l",
     ylim = c(0, 1),
     ylab = "Survival probability",
     xlab = "Cycle",
     main = "Overall Survival",
     lwd  = 3) # create a simple plot showing the OS
lines(v_OS_Exp,
      lty = 3,
      lwd = 3)
legend("right",
       legend = c("SoC", "Exp"),
       lty    = c(1, 3),
       lwd    = 3,
       bty    = "n")

# add grid - completely optional, see if it looks nicer to leave this code in or out:
grid(nx = n_cycle, ny = 10, col = "lightgray", lty = "dotted", lwd = par("lwd"), 
     equilogs = TRUE) 













# Per 1:12 hour mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv

# Often you have a survival curve as input to your model [I guess from a published study], and having that survival curve you need to parameterise your model so that you match that survival curve, and that would be a process of, potentially of calibration, if you can't use the parameters directly in your model.

# So, you could produce your survival curve and compare it to curves from trials, etc., to calibrate your model. So. we'll probably do this and have it in an appendix section.

# For calibration purposes, you want to make sure that your model is outputting something that's comparable to the publications out there on actual data on the same type of patients.

# Part of being comparable to the real world is that if there is a censoring process in actual patient data, then you could incorporate this process into the model to reflect that in your model and to ensure that your own model is comparable to the existing models which may be losing people due to censoring, etc., and which you'll then need to incorporate into your model to be comparable. 


# Another interesting thing you can do is, plot this to ask is that reasonable, does that make sense that this many people are alive after this amount of time? Is the OS what I would expect it to be?


```

## 06.2.1 Life Expectancy (LE)

```{r}
v_le <- sum(v_OS_SoC)  # summing probability of OS over time  (i.e. life expectancy)

# Basically we are summing all the alive states over time through over all the cycles, so 

# v_os <- rowSums(m_M_SoC[, 1:2])

# Is basically the PFS and OS added together.

# Also bear in mind that this is life expectancy under standard of care, and not under either of the new treatments, per: # v_os <- rowSums(m_M_SoC[, 1:2]) above.

v_le


v_le_exp <- sum(v_OS_Exp)  # summing probability of OS over time  (i.e. life expectancy)

v_le_exp



# So, if this gives a value of [1] 20.70332, that is [1] 20.70332 cycles, where in the context of our model, cycles are fortnights, so the life expectancy for our population of patients is 20.70332 fortnights or, in days:


daily_v_le <- v_le * 14
daily_v_le
daily_v_le_exp <- v_le_exp * 14
daily_v_le_exp



# When I calculate LE I calculate it in cycles. Note that my code gives a LE of 20.7 cycles for the SoC group which is approx. 290 days or 0.8 years.  Caculating the LE for the Exp group I calculated the corresponding figures as: 25.9 cycles = 363 days = 1 year (approx.). So a LE gain of about 0.2 life-years.










# Discounted life expectancy:

# If you wanted discounted life expectancy, if you were using life years and you wanted them discounted for your health economic outcomes, you could apply the discount rates - the discount factors - to the vector for overall survival [v_os] and then take it's sum [add it up] as above to get life expectancy that is discounted.

# As the 1:12 hour mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv



```

## 06.3 Disease prevalence

```{r}

# Disease prevalence is the proportion who are sick divided by the proportion who are alive, so it's necessary to account for the fact that some of the cohort have died, so you only calculate prevalence among people who are alive,in the diagram you can see it plateauing over time, even though the number of people in the OS (or "progressed") state have gone up and come down over time and this is because this is prevalence as a proportion of those who are alive, and there are few people who are still alive by cycle 60.

# Probably looks a bit funny dividing OS by v_os below, but it's necessary to remember that v_os is PFS + OS, because it's anyone who is not dead.

# So, I guess in our context you can think of this as progression prevalence over time:

v_prev <- m_M_SoC[, "OS"]/v_OS_SoC
plot(v_prev,
     ylim = c(0, 1),
     ylab = "Prevalence",
     xlab = "Cycle",
     main = "Disease prevalence")
```

# 07 Compute Cost-Effectiveness Outcomes

Good mathematical notation here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Applied Cost Effectiveness Modeling with R\\rcea-master\\rcea-master\\vignettes

## 07.1 Including ADVERSE EVENTS in Mean Costs and QALYs for each strategy

```{r}


# Calculate the costs and QALYs per cycle by multiplying m_M (the Markov trace) with the cost/utility vectors for the different states

# per cycle
# calculate expected costs by multiplying cohort trace with the cost vector for the different health states

# Basically, you take the cohort trace over time for each strategy [m_M_SoC] and multiply this by a vector of our costs for each state: [c(c_H, c_S, c_D)] -> So, basically the number of people in each state at each cycle multiplied by the cost of being in that state [cost of healthy, cost of sick and cost of dead] for each strategy that we look at (standard of care, treatment A and treatment B).

# Bear in mind we are doing matrix multiplication [%*%], because what this does is for each cycle [row in the matrix], take the vector of costs, and multiply it by the distribution in that cycle [breakdown of proportions in the states in that row] and add it all together, to get the total costs for that cycle [row]. This gives us a vector of the costs accrued in this cohort of individuals ending up in these different states for each cycle on a per person basis, because it's a cohort distribution (it always sums to 1).

# So, in cycle 1 everyone is in the OS state, so they only incur the OS cost, as more and more time passes and more and more people get sick, the costs increases due to more people being in the PFS state, but over time this falls again as more and more people go into the dead state, which has no costs as we don't treat corpses.

# v_tc_SoC  <- m_M_SoC  %*% c(c_H, c_S, c_D)  
# v_tc_trtA <- m_M_trtA %*% c(c_H + c_trtA, c_S, c_D)  
# v_tc_trtB <- m_M_trtB %*% c(c_H + c_trtB, c_S, c_D)  

# 1. Probability of adverse events 1,2 3 in the PFS state under standard of care and exp care:

# AE1:Leukopenia = 0.040 AE2:Diarrhea = 0.310 AE3:Vomiting = 0.31

p_FA1_STD     <- 0.040   # probability of adverse event 1 when progression-free under SOC
p_FA2_STD     <- 0.310   # probability of adverse event 2 when progression-free under SOC
p_FA3_STD     <- 0.310   # probability of adverse event 3 when progression-free under SOC

p_FA1_EXPR     <- 0.070   # probability of adverse event 1 when progression-free under EXPR
p_FA2_EXPR     <- 0.110   # probability of adverse event 2 when progression-free under EXPR
p_FA3_EXPR     <- 0.070   # probability of adverse event 3 when progression-free under EXPR

# 2. Cost of treating the AE conditional on it occurring
c_AE1 <- 2835.89
c_AE2 <- 1458.80
c_AE3 <- 409.03
#3. Disutiltiy of AE (negative qol impact x duration of event)


AE1_DisUtil <-0.45
AE2_DisUtil <-0.19
AE3_DisUtil <-0.36

daily_utility <- u_F/14
AE1_discounted_daily_utility <- daily_utility * (1-AE1_DisUtil)
AE2_discounted_daily_utility <- daily_utility * (1-AE2_DisUtil)
AE3_discounted_daily_utility <- daily_utility * (1-AE3_DisUtil)


u_AE1 <- (AE1_discounted_daily_utility*7) + (daily_utility*7)
u_AE2 <- (AE2_discounted_daily_utility*7) + (daily_utility*7)
u_AE3 <- (AE3_discounted_daily_utility*7) + (daily_utility*7)


# I then adjust my state costs and utilities:

# uState<-uState-pAE1*duAE1


c_F_SoC<-c_F_SoC +p_FA1_STD*c_AE1 +p_FA2_STD*c_AE2 +p_FA3_STD*c_AE3
c_F_Exp<-c_F_Exp +p_FA1_EXPR*c_AE1 +p_FA2_EXPR*c_AE2 +p_FA3_EXPR*c_AE3


u_F_SoC<-u_F
u_F_Exp<-u_F


u_F_SoC<-u_F-p_FA1_STD*u_AE1 -p_FA2_STD*u_AE2 -p_FA3_STD*u_AE3
u_F_Exp<-u_F-p_FA1_EXPR*u_AE1 -p_FA2_EXPR*u_AE2 -p_FA3_EXPR*u_AE3


v_tc_SoC <- m_M_SoC %*% c(c_F_SoC, c_P, c_D)
v_tc_Exp <- m_M_Exp %*% c(c_F_Exp, c_P, c_D)

v_tc_SoC
v_tc_Exp








# calculate expected QALYs by multiplying cohort trace with the utilities for the different health states 

# The three vectors of utilities are basically built in the exact same way as the three vectors of costs above:

# v_tu_SoC  <- m_M_SoC  %*% c(u_H, u_S, u_D)  
# v_tu_trtA <- m_M_trtA %*% c(u_H, u_S, u_D) 
# v_tu_trtB <- m_M_trtB %*% c(u_H, u_S, u_D) 

# The file I am mirroring has the following qoute:

# "

# - note that to obtain QALYs, the utility needs to be mutiplied by the cycle length as well


# v_tu_SoC <- m_M_SoC %*% c(u_F, u_P, u_D) * t_cycle
# v_tu_Exp <- m_M_Exp %*% c(u_F, u_P, u_D) * t_cycle

# To get the QALY's we not only need to multiply the state occupancy with the utilities, but also with the duration of the time cycle, because QALY's are 2-dimensional in that they combine the duration of time and the health utility.

# "

# Maybe that's because the utility originally came from a year in the disease state, and we want it to reflect the proportion of a year that is our cycle length?

# So in the example t_cycle <- 1/4 # cycle length of 3 months (in years) - so maybe their utility originally came from a years utility and they wanted to decrease it to 3 months, aka the cycle lengths, utility.

# If that was the case it would be:

# A year of utility in this state is 0.75, so 3 months should be a quarter of this, should be 0.25 of this. So, t_cycle (0.25) * u_F (0.75) = 0.1875 i.e. your utility for 3 months if u_F is your utility for 12 months.

# v_tu_SoC <- m_M_SoC %*% c(u_F, u_P, u_D) * t_cycle

# I did the maths on his approach, and without *t_cycle the first cycle of each utility value is 0.8, but after *t_cycle it is 0.2, i.e. a quarter of what it was before. Which is why I think again that he is just making the yearly utility lower to match the 3 monthly cycles, i.e. quartering a yearly utility. A quarter of utility for a quarter of a year.

# So, I think he's just trying to generate the QALYs per cycle in both states.

# And slide 25 of C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models kind of proves that.

# In it they say: cycles are 6 months.

# Their R code in: 

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models\markov_smoking_deterministic.R

# Says:

# Now define the QALYS associated with the states per cycle

# QALY associated with 1 - year in the smoking state is Normal(mean = 0.95,  SD = 0.01)
# Divide by 2 as cycle length is 6 months
# state_qalys["Smoking"] <-  0.95 / 2 [I know they divide by 2 but we could have also multiplied by a half, i.e. *0.05].

# QALY associated with 1 - year in the not smoking state is 1 (no uncertainty)
# So all PSA samples have the same value
# Again divide by 2 as cycle length is 6 months
# state_qalys["Not smoking"] <-  1.0 / 2

# I like their approach, they define utility at the start, then they can do the following:

		# Now use the cohort vectors to calculate the 
		# total QALYs for each cycle
		# cycle_qalys[i_treatment, ] <-  cohort_vectors[i_treatment, , ] %*% state_qalys[]

# i.e., take the cohort (or Markov trace) for each of the treatment options and multiply it by the state qalys.

# So, I could take where he says: "QALY's are 2-dimensional in that they combine the duration of time and the health utility." as saying "we need to give the patient a utility for this health state that matches how long they were in this health state, i.e., if the utility of one year in this state is x and we know the patient was in this health state for 2 weeks, then we need to give them 2 weeks of x as their utility."


# Andrew calculated QALYs as: 
#
# QALYs.SP0 <- trace.SP0%*%state.utilities
# QALYs.SP0
# 
# undisc.QALYs.SP0 <- colSums(QALYs.SP0)
# undisc.QALYs.SP0

# However, he described the values that he started off with as:

# The quality of life utilities of a year spent in each of the model health states has been
# estimated to be 0.85, 0.3 and 0.75 for the Successful primary, Revision, and Successful revision health states respectively.

# and said the cycle length is one year, so maybe that's why he doesnt need to multiply by the time passed, because his utilities are for a year spent in this state, whereas when we start our utilies may not necessarily match.

# So, what I think this means is that provided utilities are for the period of the cycle we can do the below, and if they are not for the period of the cycle then we can just convert them like in the smoking file above and then apply them as though they are for the period of the cycle:




v_tu_SoC <- m_M_SoC %*% c(u_F_SoC, u_P, u_D)
v_tu_Exp <- m_M_Exp %*% c(u_F_Exp, u_P, u_D)

v_tu_SoC
v_tu_Exp

# BUT, we need to remember that the above displays the amount of utilitie's gathered in each cycle.

sum(v_tu_SoC)
sum(v_tu_Exp)

# Particularly, these are quality adjusted cycles, these are quality adjusted life years where cycles are annual, so I need to consider what this means for utility where cycles are monthly, or fortnightly.

# When I calculate the QALYs above, I don’t convert these quality adjusted cycles to years. If I sum each of v_tu_SoC and v_tu_Exp I get 16.7 quality adjusted cycles in the SoC arm and 21.1 quality adjusted cycles in the Exp arm. I can convert these quality adjusted cycles to years for fortnights by working out how many fortnights there are in a year (26.0714) and then divide by this number. These correspond to 0.64 and 0.81 QALYs respectively so 0.17 QALYs gained.

v_tu_SoC <- v_tu_SoC/26.0714
v_tu_Exp <- v_tu_Exp/26.0714

# So, these are per-cycle utility values, but, our cycles aren't years, they are fortnights, so these are per fortnight values, if we want this value to reflect the per year value, so that we have a quality adjusted life year, or QALY, then we need to adjust this utility value by how many of these fortnights there are in a year (26.0714), that is divide by how many fortnights there are in a year to bring the per fortnight value to a per year value


# James, I'd like your thoughts on what I've done here and whether this seems reasonable.


sum(v_tu_SoC)
sum(v_tu_Exp)

# QALYS Gained:

Qalys_gained <- v_tu_Exp - v_tu_SoC
sum(Qalys_gained)

# You can see above that there are no utility differences between the different treatments considered: c(u_H, u_S, u_D), it's just different utilities per the health states people are in.

# If we did want to do different utilities for the health state you are in per the treatment you are on, we could define this in the input parameters and then add this in above when creating the vector of utilities for that treatment.

sum(v_tc_SoC)
sum(v_tc_Exp)

# The question is, should I be making that similar conversion of cycles to years for costs? I could do this as below: 

# v_tc_SoC <- v_tc_SoC/26.0714
# v_tc_Exp <- v_tc_Exp/26.0714
# 
# sum(v_tc_SoC)
# sum(v_tc_Exp)


# But, I assume my costs could be correct, if I correctly defined my costs per cycle.

# This is probably where I would like some feedback, is the manner in which I generate my costs (particularly) and QALYs above, correct?


# Reviewing the literature, the fact that: Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., El-Rayes, B. F., & Flowers, C. R. (2015). First-and second-line bevacizumab in addition to chemotherapy for metastatic colorectal cancer: a United States–based cost-effectiveness analysis. Journal of Clinical Oncology, 33(10), 1112. has similar costs, QALYS and Life Years gained and reports similar ICERs (the $352,734/QALY in the UK, which is similar to my own several hundred thousand per QALY without dividing costs by 26.0714 and dissimilar to my ~18,000 per QALY when I divide costs by 26.0714 makes me think I shouldnt be dividing costs by 26.0714).

```

```{r}

# There's probably a more elegant way to do this, but if I wanted to add in the once off cost of the COSLOSSUS test, I could do something like the below:

# v_tc_trtA <-  v_tc_trtA+10
# v_tc_trtB <-  v_tc_trtB+100

# I just need to think about this and does it make sense, because it's adding to the cost each cycle, so probably the best indicator would be whether the cost outputted in 07.2 is only higher by the cost of the test when we do this, or if it is higher by a number of other costs, so I can check that below when adding costs in.

# Page 17 of An Introductory Tutorial on Cohort State-Transition Models in R Using a Cost-Effectiveness Analysis Example https://arxiv.org/pdf/2001.07824.pdf suggests that these are total costs expected per individual, and with testing being a one off cost, maybe you could just add it onto the costs you have at the end? Table 4: Total expected discounted QALYs and costs per average individual in the cohort of the Sick-Sicker model by strategy accounting for within-cycle correction. Costs QALYs Standard of care $151,580 20.711 Strategy A $284,805 21.499 Strategy B $259,100 22.184 Strategy AB $378,875 23.137 The total expected discounted QALYs and costs for the Sick-Sicker model under the four strategies accounting for within-cycle correction are shown in Table 4.


```

## 07.2 Discounted Mean Costs and QALYs

```{r}

# Finally, we'll aggregate these costs and utilities into overall discounted mean (average) costs and utilities.

# Obtain the discounted costs and QALYs by multiplying the vectors of total cost and total utility we created above by the discount rate for each cycle:

# Its important to remember what scale I'm on when I applied my discounting formula.
# If I set d_e<-0 then my code estimates 16.7 and 21.1 QALYs in each group which must be the quality adjusted cycles.

# Setting the discount rate back to 4% gives me 1.97 and 1.98 QALYs (which are really QA-cycles).

# Looking at the discounting vector I have defined below, I have converted cycles to days but I need to convert the discount rate to a daily discount. If I don't, the result is that discounting reduces the cycles dramatically which reduces the difference which increases with time.

# I can adress this by defining the discount rate as divided by 365 (i.e. the number of days in a year) then the results become 16.4 and 20.6 QA-cycles which of course become 0.63 and 0.79 QALYs respectively, or 0.16 QALYs gained.

d_c <- d_c/365
d_e <- d_e/365

# - Then, the discount rate for each cycle needs to be defined accounting for the cycle length, as below:


v_dwc <- 1 / ((1 + d_c) ^ ((0:(n_cycle-1)) * t_cycle)) 
v_dwe <- 1 / ((1 + d_e) ^ ((0:(n_cycle-1)) * t_cycle))


# So, below we take the vector of costs, transposing it [the t() bit] to make it a 1 row matrix and using matrix multiplication [%*%] to multiply it by that discount factor vector, which is what you multiply by the outcome in each cycle to get the discounted value of the outcome for that cycle, and then it will all be summed all together across all cycles [across all cells of the 1 row matrix]. Giving you tc_d_SoC which is a scalar, or a single value, which is the lifetime expected cost for an average person under standard of care in this cohort. 

# Discount costs by multiplying the cost vector with discount weights (v_dwc) 
# tc_d_SoC  <-  t(v_tc_SoC)  %*% v_dwc
# tc_d_trtA <-  t(v_tc_trtA) %*% v_dwc
# tc_d_trtB <-  t(v_tc_trtB) %*% v_dwc

tc_d_SoC <-  t(v_tc_SoC) %*% v_dwc 
tc_d_Exp <-  t(v_tc_Exp) %*% v_dwc


# So, now we have the average cost per person for treatment with standard of care, and the experimental treatment. 


# Discount QALYS by multiplying the QALYs vector with discount weights (v_dwe) [probably utilities would have been a better term here, if I hadnt of updated it from fortnightly health state quality of life, to yearly health state quality of life]
# tu_d_SoC  <-  t(v_tu_SoC)  %*% v_dwe
# tu_d_trtA <-  t(v_tu_trtA) %*% v_dwe
# tu_d_trtB <-  t(v_tu_trtB) %*% v_dwe


tu_d_SoC <-  t(v_tu_SoC) %*% v_dwe
tu_d_Exp <-  t(v_tu_Exp) %*% v_dwe


# Store them into a vector -> So, we'll take the single values for cost for an average person under standard of care and the experimental treatment and store them in a vector v_tc_d:
v_tc_d <- c(tc_d_SoC, tc_d_Exp)
v_tu_d <- c(tu_d_SoC, tu_d_Exp)

v_tc_d
v_tu_d

# To make things a little easier to read we might name these values what they are costs for, so we can use the vector of strategy names [v_names_str] to name the values:

names (v_tc_d) <- v_names_strats
v_tc_d

names (v_tu_d) <- v_names_strats
v_tu_d


Discounted_Qalys_gained <- tu_d_Exp - tu_d_SoC
sum(Discounted_Qalys_gained)

# For utility, the utility values aren't different for the different states depending on the treatment strategy, i.e. SOC, Experimental Treatment, but the time spent in the states with the associated utility is different due to the treatment you're on, so your utility value will be higher if the treatment keeps you well for longer so that you stay in a higher utility state for longer than a lower utility state, i.e., progression.


# Dataframe with discounted costs and effectiveness

# So then we aggregate them into a dataframe with our discounted costs and utilities, and then we use this to calculate ICERs in: ## 07.3 Compute ICERs of the Markov model

# df_ce <- data.frame(Strategy = v_names_strats,
#                     Cost     = v_tc_d, 
#                     Effect   = v_tu_d)
# df_ce
```

## 07.3 Compute ICERs of the Markov model

This study also found that Bevacizumab wasnt a cost-effective addition to care in a number of studies, so I should follow their manner of discussing this: Goldstein, Daniel A., et al. "Bevacizumab for Metastatic Colorectal Cancer: A Global Cost‐Effectiveness Analysis." The Oncologist 22.6 (2017): 694-699.
<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5469597/pdf/onco12164.pdf>

I think somewhere in all my example files there must be some example code on generating life years, etc., and reporting them as part of your results table.
I basically want to be able to report the following per page 5 of Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colorectal Cancer <file:///C:/Users/jonathanbriody/Downloads/goldstein2014.pdf>:

Results In the base case analysis, effectiveness and costs were compared between the PK and BSA group.
PK FOLFOX provided 2.03 QALYs at a cost of \$50,177 compared with BSA FOLFOX with 1.46 QALYs at a cost of \$37,173.
The ICER for PK FOLFOX was \$22,695 per QALY.
In terms of LYs, PK FOLFOX provided 2.60 LYs compared with BSA FOLFOX with 1.99 LYs.
The ICER of PK 5-FU was \$21,423 per LY.
All numerical results are summarized in Table 5.

The code to do this is here to add costs, QALYS, NMBs and Life Years to your leauge table:

cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html

(If I wanted to take a Net Monetary Benefit (NMB) approach instead, I could go to the 35 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_24\Live Session Recording\Live Session Recording August 24th.mp4)

DOMINATED STRATEGIES EXPLAINED: <https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html>

```{r}

# The discounted costs and QALYs can be summarized and visualized using functions from the 'dampack' package
(df_cea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
                           effect     = c(tu_d_SoC, tu_d_Exp),
                           strategies = v_names_strats))
df_cea

# df_cea <- calculate_icers(cost       = df_ce$Cost,
#                           effect     = df_ce$Effect,
#                           strategies = df_ce$Strategy
#                           )
# df_cea

# The above uses the DARTHtools package to calculate our ICERS, incremental cost and incremental effectiveness, and also describes dominance status:

# This uses the "calculate_icers function", which does all the sorting, all the prioritization, and then computes the dominance, and not dominance, etc., and there's a publication on the methods behind this, based on a method from colleagues in Stanford.

# The default view is ordered by dominance status (ND = non-dominated, ED = extended/weak dominance, or D= strong dominance), and then ascending by cost per: https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html


# The icer object can be easily formatted into a publication quality table using the kableExtra package, as below, but there's probably a better way to do this per Dampack just under:

# library(kableExtra)
# library(dplyr)
# df_cea %>%
#  kable() %>%
#  kable_styling()


## CEA table in proper format ---- per: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\September-Workshop-main\September-Workshop-main\analysis\cSTM_time_dep_simulation.r
table_cea <- format_table_cea(df_cea) # Function included in "R/Functions.R"; depends on the `scales` package
table_cea

```

## Results

I like the way they describe their results: *A Cost-Effectiveness Analysis of Currently Approved Treatments for HBeAg-Positive Chronic Hepatitis B <https://sci-hub.ru/10.2165/00019053-200826110-00006>*

> A good way to report your results can be seen in Table 4 of the following:
>
> Rivera, F., Valladares, M., Gea, S., & López-Martínez, N.
> (2017).
> Cost-effectiveness analysis in the Spanish setting of the PEAK trial of panitumumab plus mFOLFOX6 compared with bevacizumab plus mFOLFOX6 for first-line treatment of patients with wild-type RAS metastatic colorectal cancer.
> Journal of Medical Economics, 20(6), 574-584.
> <https://sci-hub.st/10.1080/13696998.2017.1285780> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Evidence Synthesis\\Economic Models\\Rivera et al_2017_Cost-effectiveness analysis in the Spanish setting of the PEAK trial of.pdf

## 07.4 Plot frontier of the Markov model

Understand this plot here:

<https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html>

Also saved here:

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\sensitivity analysis help files\\Basic Cost Effectiveness Analysis.pdf

```{r}
plot(df_cea, effect_units = "QALYs", label = "all")

# plot(df_cea, effect_units = "QALYs")


# When we plot it we have 2 strategies it is possible that something would be off the frontier, would be weakly dominated or strongly dominated, with just a few strategies it's not necessarily that impressive, but with lots of strategies then dampack can be helpful.

```

The bottom axis of the above diagram shows you how effective the intervention is, the y axis shows you how costly the intervention is, here standard of care is cheaper but less effective than treatment B, which is more effective but more expensive, treatment A is not on the frontier, it has been dominated.

The frontier is also discussed here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R_HTA_LMIC_Intro_to_RHTA_modeling

(If I wanted to see an example where the frontier doesnt exist, I could go to the 38 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_24\Live Session Recording\Live Session Recording August 24th.mp4)

<https://www.google.ie/search?q=icer+fronter&hl=en&dcr=0&ei=nerzYYS_AciHhbIPlqyMuAk&ved=0ahUKEwjEgar1wdT1AhXIQ0EAHRYWA5cQ4dUDCA4&uact=5&oq=icer+fronter&gs_lcp=Cgdnd3Mtd2l6EAMyBwghEAoQoAE6BwgAEEcQsAM6BQgAEJECOgsIABCABBCxAxCDAToOCC4QgAQQsQMQxwEQowI6CwguELEDEMcBEK8BOg4ILhCABBCxAxDHARDRAzoICAAQsQMQgwE6BQguEJECOgQIABBDOgcIABCxAxBDOggIABCABBCxAzoOCC4QgAQQsQMQxwEQrwE6CAguEIAEELEDOgcIABDJAxBDOgUIABCSAzoFCAAQgAQ6CwguEIAEEMcBENEDOgUILhCABDoLCC4QgAQQxwEQrwE6BAgAEAo6CgguEMcBENEDEAo6BAguEAo6BwgAEMkDEAo6BggAEBYQHjoICAAQFhAKEB46BAgAEA06BQghEKABSgQIQRgASgQIRhgAUKIJWJYZYNEbaANwAngAgAGMAYgB6AiSAQM4LjSYAQCgAQHIAQjAAQE&sclient=gws-wiz>

<https://yhec.co.uk/glossary/cost-effectiveness-frontier/>

[\<https://www.hiqa.ie/sites/default/files/2017-01/Revised_Economic_Guidelines_posted_100714.pdf\>](https://www.hiqa.ie/sites/default/files/2017-01/Revised_Economic_Guidelines_posted_100714.pdf){.uri} [also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\GitHub\\COLOSSUS_Model\\Revised_Economic_Guidelines_posted_100714.pdf]

<https://researchonline.lshtm.ac.uk/id/eprint/4648686/1/The%20efficiency-frontier%20approach%20for%20health_GREEN%20AAM.pdf> also saved here:C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS\_Model\The efficiency-frontier approach for health_GREEN AAM.pdf

The efficiency frontier is described on page 277 [in the textbook] of: [file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine\_%20integrating%20evidence%20and%20values.pdf](file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine_%20integrating%20evidence%20and%20values.pdf)

# **Sensitivity Analysis Below:**

"It is well known7,8 that conventional univariate sensitivity analysis, whereby individual parameters are varied while maintaining all remaining parameters at their baseline value, is likely to underestimate uncertainty because, in reality, parameters will not vary in isolation. Probabilistic sensitivity analysis involves specifying distributions for model parameters to represent uncertainty in their estimation and employing Monte Carlo simulation to select values at random from those distributions.5,6 In this way, probabilistic models allow the effects of joint uncertainty across all the parameters of the model to be considered. , in probabilistic modeling, parameters are considered random variables, which can take a range of values described by the specified distribution. Parameters in decision models represent summary values related to the average experience across a population of (potential) patients. Therefore, the relevant uncertainty to capture in the formation of a distribution for the parameter is 2nd-order uncertainty related to the sampling distribution of the parameter, not the variability in the values observed in a particular population (1st-order uncertainty; see Stinnett and Paltiel3 for further discussion)." This also has mathematical notation for calculating Beta and Gamma distributions at the end.
Probabilistic Analysis of Cost-Effectiveness Models: Choosing between Treatment Strategies for Gastroesophageal Reflux Disease Andrew H. Briggs, DPhil, Ron Goeree, MA, Gord Blackhouse, MBA, Bernie J. O'Brien, PhD www.med.mcgill.ca/epidemiology/courses/EPIB654/Summer2010/EF/example%20PPI.pdfinsu

All plots produced by dampack are ggplot objects and can be readily customized.
<https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

I've put detailed notes in:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_27\\3_SA_material\markov\_sick-sicker_SA_solutions

Which should be read in tangent with: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2\_Making Models Probabilistic\A2.1.2 Distributions for parameters\notes.txt

when doing the sensitivity analysis below:

There is similarly relevant information at the 1:38 hour mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv

And in this document:

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Briggs et al 2012 model parameter estimation and uncertainty.pdf

**Negative ICER's:**

In presenting 1-way uncertainty analysis, reporting negative incremental cost-effectiveness ratios (ICERs) should be avoided as they are meaningless.26,27 Instead, the ICER range should be limited to results corresponding to positive incremental health consequences and costs---quadrant I in the cost-effectiveness plane.
Results for which incremental costs are positive and health consequences negative should be indicated qualitatively as ''dominated'' and those with negative incremental costs and positive health consequences as ''dominant.'' ICERs corresponding to negative incremental costs and health consequences---quadrant III---should be distinguished from ICERs in quadrant I.
C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf

# 08 Deterministic Sensitivity Analysis

A good description of one way sensitivity is here: Cost-effectiveness of capecitabine and bevacizumab maintenance treatment after first-line induction treatment in metastatic colorectal cancer <https://sci-hub.ru/10.1016/j.ejca.2017.01.019>

When i'm thinking about the range around the point estimate, i.e., the max and min, I'll have to return to my uncertainty notepad file and see how daniel did it to make his table in the goldstein paper because I know he used the same range there for the DSA and PSA based on how he described the results of the tornado diagram, i.e.:

Also I can look at the following study and their Table:

We then assessed the impact of varying single parameters on incremental costs, QALYs, and ICUR.
We used the 95% CI of the hazard ratios for death and tumor progression reported in the trial to vary the survival in each health state.
We varied utilities in stable disease states from the worst case scenario (cough, dyspnea, and pain) to the best case scenario (treatment response with no symptoms) [15,16].
We varied the utility of PD and FN states from the worst health state utility in our study (severe bleeding) to the utility of stable disease on treatment state [15,17].
We varied the utility of severe bleeding by 10%.
We varied the number of bevacizumab cycles from a minimum of 4 to a maximum of 15 cycles, to reflect a range of cycles going from the minimally recommended number of platinum-doublet cycles with no maintenance [4]to continuation of bevacizumab for 3 months after tumor progression [5,15].
We varied all other unit costs by 20% of the base case.
Discount rates varied from 0% to 6% (Table 2A).
We repeated the one-way sensitivity analysis assuming a dose of 7.5 mg/kg of bevacizumab while varying the other individual parameters within the same ranges used for the dose of 15 mg/kg.
This allowed us to estimate the cost-effectiveness and model robustness of bevacizumab at the dose used in the AVAiL trial [22].
Goulart, B., & Ramsey, S.
(2011).
A trial-based assessment of the cost-utility of bevacizumab and chemotherapy versus chemotherapy alone for advanced non-small cell lung cancer.
Value in Health, 14(6), 836-845.
sciencedirect.com/science/article/pii/S1098301511014124

We assumed the mean value of each distribution to be the base case value of the corresponding variable, and the standard deviation to be half of the 95% confidence intervals for utility and 20% of the mean values for other variables.
So, I can just cite this when I have point estimates and want to build a range around these: Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Ramalingam, S. S., ... & Flowers, C. R.
(2015).
Necitumumab in metastatic squamous cell lung cancer: establishing a value-based cost.
JAMA oncology, 1(9), 1293-1300.

[There are more studies that take the 20% approach describe in this file under "the range around things"].

A number of sensitivity analyses were conducted to address uncertainty in parameter estimation and assess robustness of the model results.

A one-way sensitivity analysis evaluates the relative weight of each parameter in our model on overall uncertainty of cost-effectiveness [@hamdyelsisi2019].

We varied each model parameter one at a time, while holding the other parameters constant, subsequently we assessed the impact on incremental costs, QALYs, and ICER.

The mean, range and distributions applied in these analyses are described in Table X [big table that has the base-case, max, min, reference and distribution].

Goldstein says "Utilities were varied over their 95% confidence intervals." so I can think about that.

The below supports my thoughts on the above, that I use the same range for DSA and PSA in the descriptive Table like Goldstein did.

"The general principle remains that assumptions for specifying the distribution and/or defining the interval for uncertainty analysis should follow standard statistical methods (e.g., beta distributions are a natural match for binomial data; gamma or log normal for right skew parameters; log normal for relative risks or hazard ratios; logistic for odds ratios15). These distributions can be used directly in PSA or to define the interval (plausible range) for a DSA." <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

"This is true whatever the uncertainty analysis's technical specification. For a 1-way DSA, it is necessary to specify the parameter's point estimate and a defensible range; these may be taken directly from the estimation process, with the latter based, for example, on a 95% confidence interval. Representation of uncertainty depends on the uncertainty analysis planned. For DSA, an interval estimate representing beliefs about the parameter's plausible range is required. For PSA, a distribution is specified via its parameters." <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

For cost parameters, a template was completed....

Costs, utilities and probabilities were varied within +/- 20% of baseline values, per [@goldstein2014a; @goulart2011]....
[these are just guys who have done the 20% varying, if I find better online I can use that].

To study the influence of a range of transition risks, the PFS and OS curves \$S(t)\$ in the model were altered for both treatment strategies via a hazard ratio adjustment \$\\gamma\$, as \$[S(t)\^{\\gamma}]\$.
Subsequently, transition risks for each cycle were calculated from these adjusted survival curves [@goldstein2014a].
The adjustment hazard ratio was 1 +/- 0.2, i.e., the base case varied within plus or minus 20%.

We individually consider the influence of each parameter on the ICER in a one way deterministic sensitivity analysis.

**Describing Tornado diagram results:**

We create a tornado plot of the one-way sensitivity analysis .
In Figure X, the parameters with the largest effect on the ICER differed between treatment arms.
Results remained robust to adjustments to X, with an ICER range between \$X,000 to X,000 per QALY.
There was a limited influence of variation in other parameters on the ICER of THE NOVEL THERAPY (\< \$X,000 per QALY) .
A parameter of interest was X, which - when varied between \$X00 and \$X00 - placed the ICER range between \$X,000 and X,000 per QALY .

[- read this in the context of this article and mirror their display, particularly putting the max and the min on either end of the tornado diagram bars: Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
(2014).
Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
Clinical colorectal cancer, 13(4), 219-225.]

Across a range of parameter values, the ICER was above X per QALY, supporting the robustness of results.

[]

"One-way DSA may be reported using a tornado diagram (Figure 1). The horizontal axis is the outcome; along the vertical axis, parameters are arrayed and horizontal bars represent the outcome range associated with the specified parameter's range. The outcome point estimate corresponding to base-case values is indicated by a vertical line cutting through all horizontal bars. Commonly, the longest bar (reflecting the parameter generating the widest uncertainty) is placed at the top, and the other bars are arrayed in descending order of length. A tornado diagram should be accompanied by a legend or table indicating the upper and lower bounds of values for each parameter, with their justification in terms of the evidence base. A table may be used instead of a tornado diagram or the results ranges provide in the text of the report (e.g., the text might state that ''the outcome ranged from X to Y when parameter Z was varied from A to B''). It is important that the range A to B represents a defensible range for parameter Z, not an arbitrary one."

Results of 1-way threshold analyses are easily reported in text (e.g., ''The ICER remains less than Y as long as the value of X is greater than A,'' or ''Alternative 1 dominates 2 if the value of Z is less than B'').

<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

The tornado plot in Figure X describes which parameters are driving the majority of the variation in the outcome.
In the figure, "Parameter Level High/Low" corresponds to parameter values that are above or below the median point estimate.
Subsequently, it is possible to highlight where high or low expected outcome values result from variation in parameter values.

*For example, the tornado plot below tells us that the parameter `"muDieCancer"` has the most leverage in affecting the effectiveness model outcome, and that values below the median value of `"muDieCancer"` in the one-way sensitivity analysis are associated with higher expected effectiveness outcomes.* [Helpful in interpreting my own tornado diagram with NMB].

*It is important to note that some important information is obscured by tornado plots and caution should be exercised when interpreting it. As the parameter of interest varies across its range in the one-way sensitivity, the strategy that maximizes the outcome of interest can also change across this range. The plot is not showing how the expected outcome changes for a single strategy, but how the expected outcome of the optimal strategy changes. The designation of which strategy is optimal is liable to alternate over the range of the parameter of interest, and this is hidden in a tornado plot.* [Important just generally to note].

*For owsa objects that contain many parameters that have minimal effect on the parameter of interest, you may want to consider producing a plot that highlights only the most influential parameters. Using the `min_rel_diff` argument, you can instruct `owsa_tornado` to exclude all parameters that fail to produce a relative change in the outcome below a specific fraction.*

*owsa_tornado(o,*

*min_rel_diff = 0.05)*

*In order to attain the data.frame used to produce the tornado plot, use the `return` argument to change the type of object returned by the `owsa_tornado` function.*

*owsa_tornado(o,*

*return = "data")*

*- <https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>*

Page 26 of <https://cran.r-project.org/web/packages/dampack/dampack.pdf> has information on making changing to the visual parts of Tornado plots, putting things in black and white, etc.,.

Very simple tornado diagram explanation: <https://mbounthavong.com/blog/2018/5/26/communicating-data-effectively-with-data-visualizations-tornado-diagram>

At the moment, my tornado plot describes a NMB.
If I wanted to change this to describe an ICER, it looks like according to the below I could do this just by adding the code from above to generate an ICER to the oncologySemiMarkov_function.R, storing it in df_cea, make sure it equals the ICER generated above and change:

outcomes = c("NMB"), \# output to do the OWSA on

to

outcomes = c("ICER"), \# output to do the OWSA on

But, at the moment I have code above where doing a tornado diagram on effectiveness as an outcome is described, so I can take their approach and apply my code to NMB if I like.

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\GitHub\\COLOSSUS_Model\\Deterministic Sensitivity Analysis\_ Generation.pdf

If a reviewer wants me to do a more unusual deterministic sensitivity analysis, which is probabilistic or includes regression, I can find information on doing that here: <https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

**Describing plotted outcome values over a range of values of the parameters of interest and treatment strategies:** OR, MORE SIMPLY PUT -\> **Describing Plotting One Way Sensitivity Analysis (OWSA) Plot:**

In Figure X we conduct a one-way deterministic sensitivity analysis separately on each of the different parameters, per treatment strategy.
This figure illustrates how the expected outcome value from the decision model varies (on the x axis) as the parameters vary across the range of minimum to maximum values described in Table X (on the y axis).

**Describing Optimal Strategy:**

While a tornado plot provides a useful visual aid to identify which parameters are driving the majority of the variation in the outcome of interest, such plots also obscure the fact that as a parameter changes from it's minimum to it's maximum value, the treatment strategy that maximises the specified model outcome may also vary across this range.
Tornado plots depict changes in the expected outcome of the optimal strategy, rather than for a single strategy.
However, this "optimal strategy" can vary when the parameter of interest alternates from low to high values, something which is generally obsfucated in tornado plots.
Consequently, we also conduct a visualization of the optimal strategy, i.e., the strategy that maximises the expected outcome, over each parameter range in Figure X. We derive this optimal strategy plot from the results of the one-way sensitvity analysis.
We only plot parameters that make changes in the optimal strategy as they vary.
Thus, we can illustrate when the treatment strategy (SoC or experimental treatment) maximising the expectation of the outcome of interest (here NMB, but we can change this to ICERS) alternates as a function of each parameter of interest.

[Per: [https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html]](https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html%5D)

[If I do this and my optimal strategy says that the designation of which strategy is optimal doesnt alternate, I can report this as a result and show all my non-altered optimal strategy boxes in the diagram by removing plot_const = FALSE, rather than thinking it invalidates the reason for doing an optimal strategy plot].

In plotting the optimal strategy as the parameter values change, in Figure X we demonstrate that the optimal strategy varies from SoC to Exp as X PARAMETER changes from Y to Z.

**Describing Two-Way Sensitivity Analysis:**

Finally, we *conduct a two-way uncertainty analysis to* investigate changes *in the outcome* when varying two parameters simultaneously.
As informed by [@briggs2012] we conduct a two-way sensitivity *uncertainty* analysis for parameters expected to have a logical covariance.
We examine how the expected outcome (here NMB) changes as both the hazard ratio adjustment applied to the PFS curve under standard of care - and accordingly the event rate in the control intervention - and the hazard ratio with the novel therapy change.
The results of this uncertainty analysis are reported in Figure X. The axes represent the range of possible values of the parameters of interest.
The quadrant is partitioned into regions according to which treatment strategy maximizes the expected outcome as a function of the two parameters being varied.
With ICERS, all the possible ICERS following the TWSA are plotted in the quadrant and the boundary line indicates the specified ICER threshold [i.e., the WTP], where on on each side one treatment is preferred over the other, i.e., above the line treatment strategy A may be preferred to treatment strategy B, and vice-versa below, so, it dominates the other because it's ICER value is sufficient to make this treatment cost-effective instead of the other.

ICERs explained on page 8 of:

[\<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf\>](file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf){.uri}

Per the following links, you can plot these on any model outcome - such as cost and effect, not just NMB.
This is supported by when I went into the ispor webinar and applied these plots with ICERS as the outcomes rather than NMB, and it worked.

<https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

syzoekao.github.io/CEAutil/

Model Parameter Estimation and Uncertainty Analysis: A Report of the ISPOR-SMDM Modeling Good Research Practices Task Force Working Group--6 Andrew H. Briggs, DPhil, Milton C. Weinstein, PhD, Elisabeth A. L. Fenwick, PhD, Jonathan Karnon, PhD, Mark J. Sculpher, PhD, A. David Paltiel, PhD, on Behalf of the ISPOR-SMDM Modeling Good Research Practices Task Force

Figure 1 here: <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colorectal Cancer Daniel A. Goldstein,1 Qiushi Chen,2 Turgay Ayer,2 David H. Howard,1,3 Joseph Lipscomb,1,3 R. Donald Harvey,1 Bassel F. El-Rayes,1 Christopher R. Flowers

<file:///C:/Users/jonathanbriody/Downloads/goldstein2014.pdf>

Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China Xin Guan, Hongchao Li, Xiaomo Xiong, Cike Peng, Ning Wang, Xiao Ma & Aixia Ma

<https://www.tandfonline.com/doi/pdf/10.1080/13696998.2021.1888743>

Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt Gihan Hamdy Elsisi, Yousery Nada, Noha Rashad & João Carapinha <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Evidence%20Synthesis/Paper%20Materials%20and%20Methods/Hamdy%20Elsisi%20et%20al_2019_Cost-effectiveness%20of%20sorafenib%20versus%20best%20supportive%20care%20in%20advanced.pdf>

A Trial-Based Assessment of the Cost-Utility of Bevacizumab and Chemotherapy versus Chemotherapy Alone for Advanced Non-Small Cell Lung Cancer Bernardo Goulart, MD, MS1,2,\*, Scott Ramsey, MD, PhD1, <https://sci-hub.ru/10.1016/j.jval.2011.04.004>

## 08.1 Load PFS-PFSer Markov model function

To make the function work for my health states I rename "health" and "sick" in it to "PFS" and "OS" respectively.

One thing that concerns me is that the first line of the function sets willingness to pay at 10,000 and this is then used to build the net monetary benefit, I wonder if this needs to be updated for different willingess to pay values directly in the function or if I can do it in this Markov_3state.Rmd file?

oncologySemiMarkov \<- function(l_params_all, n_wtp = 10000) {

It may just be creating an wtp for the NMB calculation in the function, because there is none for use in this function and one is needed to calculate:

v_nmb_d \<- v_tu_d \* n_wtp - v_tc_d

but probably a good idea to ensure this is the case (i.e., change the n_wtp to see how it changes the nmb) and also ensure that I am happy with the NMB value currently under use (10,000 above), i.e. do a little reading and see if another n_wtp may be more appropriate and maybe reach out to darth and see if there is a reason their wtp's don't match.

## 08.3 One-way sensitivity analysis (OWSA)

```{r}

# A brief note on how parameters are varied one at a time:

# A simple one-way DSA starts with choosing a model parameter to be investigated. Next, the modeler specifies a range for this parameter and the number of evenly spaced points along this range at which to evaluate model outcomes. The model is then run for each element of the vector of parameter values by setting the parameter of interest to the value, holding all other model parameters at their default base case values.

# To conduct the one-way DSA, we then call the function run_owsa_det with my_owsa_params_range, specifying the parameters to be varied and over which ranges, and my_params_basecase as the fixed parameter values to be used in the model when a parameter is not being explicitly varied in the one-way sensitivity analysis.

# [In case I struggle to describe varying the hazard ratio, I describe this below]:

# In the example below, we will conduct four one-way DSAs separately on four different parameters: the utility benefit of Treatment_A; the cost of Treatment_A; the reduction, as a hazard ratio, on the rate of progressing from Sick to Sicker with Treatment_B; and the rate of dying from the Healthy state.

# my_owsa_params_range <- data.frame(pars = c("u_trtA", "c_trtA", "hr_S1S2_trtB", "r_HD"),
#                              min = c(0.9, 9000, 0.3, 0.001),
#                              max = c(1,   24000, 0.9, 0.003))]

# https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html



# I've written up an explanation of tornado diagram probabilities and the rules behind the creating of boxes in Tornado diagrams, etc., here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\Tornado Diagram Probabilities Explained.R


# Using ICERs in the tornado diagram:

# Typically, it's very easy to get the tornado plot to work with ICERS rather than with NMB, you just change   outcomes         = c("NMB"), to   outcomes         = c("DSAICER"), and then change the function to include the following code, basically creating ICERs the same way you do in this Rmd file and then an insertion of DSAICER  = DSA_ICER into the df_ce so that you have an ICER to include under outcomes         = c("") as above: 


    # v_nmb_d   <- v_tu_d * n_wtp - v_tc_d
    # 
    # (df_DSAcea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
    #                               effect     = c(tu_d_SoC, tu_d_Exp),
    #                               strategies = v_names_strats))
    # df_DSAcea
    # 
    # DSA_ICER    <- c(df_DSAcea[2,6])
    # 
    # 
    # df_ce <- data.frame(Strategy = v_names_strats,
    #                     Cost     = v_tc_d,
    #                     Effect   = v_tu_d,
    #                     NMB      = v_nmb_d,
    #                     DSAICER  = DSA_ICER)
    # 
    # return(df_ce)


# However, I was having some trouble getting my tornado plot to generate. I think I've tracked the issue down to ggplot, which the dampack tornado plot code uses. I suspect that this was because some of my values were very small, because the exact issue I was getting was:

#Error in if (zero_range(as.numeric(limits))) { :
#missing value where TRUE/FALSE needed 

# and per the following webpage https://stackoverflow.com/questions/12462479/r-ggplot2-simple-plot-cannot-specify-log-axis-limits other people have had the same issue and talked about ggplot and how this error might mean it won't accept certain issues. Also here: https://stackoverflow.com/questions/28474630/missing-value-where-true-false-needed-error-in-if-statement-in-r and here:https://stackoverflow.com/questions/55867994/ggplot-has-error-missing-value-where-true-false-needed and here: https://stackoverflow.com/questions/67810309/pot-histograms-and-had-error-missing-value-where-true-false-needed and here: https://stackoverflow.com/questions/53052874/define-new-scales-axis-tranform-for-ggplot and https://github.com/tidyverse/ggplot2/issues/2907 and  https://github.com/tidyverse/ggplot2/issues/930 and https://community.rstudio.com/t/can-i-transform-scales-axes-from-log-to-linear-scale-in-ggplot2/69628/7 and https://groups.google.com/g/ggplot2/c/Z5dt6ZBnGII?pli=1

# I should check if they build tornado diagrams without ggplot in the York code, and if they do at least I can see the ggplot in action and remedy the issue using one of the solutions suggested at the above links.


# If I can't figure out how to get a tornado diagram to work with ICER's rather than NMB, I could follow the approach here of doing a probabilistic analysis and then creating another tornado diagram with ICERS on the probabilistic analysis. These examples explain the technicalities of how this is done in R but also from a theoretical viewpoint how this works:

# https://github.com/DARTH-git/dampack/blob/master/R/owsa.R

# Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\doingtornadoprobabilsticallyexample.R

# https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\cran-r-project-org-web-packages-dampack-vignettes-psa_analysis-html.pdf

# Alternatively, I could take the following approach described here: 

# https://rpubs.com/mbounthavong/decision_tree_model_tutorial also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\rpubs-com-mbounthavong-decision_tree_model_tutorial.pdf with the code that it calls saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\tornado_diagram_code.R

# I've actually coded up the above solution in quite a bit of detail. To get it working though I'll have to change oncologySemiMarkov_function.R to be:

#    
#     v_nmb_d   <- v_tu_d * n_wtp - v_tc_d
#     
#     (df_DSAcea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
#                                   effect     = c(tu_d_SoC, tu_d_Exp),
#                                   strategies = v_names_strats))
#     df_DSAcea
#     
#     DSA_ICER    <- c(df_DSAcea[2,6])
#     
#     # I'm picking the row and column where the ICER value appears in the df_DSAcea dataframe created by calculate_icers.
#     
#     # Generate the ouput
#     return(c(v_names_strats, v_tc_d, v_tu_d, DSA_ICER))
#   })
#   
# }
# 
    
    
# Or I could try the below:

# https://cran.r-project.org/web/packages/tornado/vignettes/tornadoVignette.html also saved here: https://cran.r-project.org/web/packages/tornado/vignettes/tornadoVignette.html

# But if these don't work, because of using ggplot also, I could see if I can find a package or manual creation code that doesnt use ggplot, or seeing as the ggplot part of the code in the mbounthavong code happens somewhere I can see it and change it, I might be able to see if one of the fixes mentioned in the stack overflow links about can be applied and let the tornado plot be built.

# Worst case scenario, I create the tornado diagram and two-way sensitivity analysis as NMB and report NMB's in the league table of results so it doesnt look too out of place.



#Normal chrome Session buddy has all the tabs on describing tornado diagrams that I need to read saved. Canary session buddy has all the tabs saved on doing a tornado diagram outside of the darth framework.

# A CODE CHUNK WHERE IT IS EASY TO RUN ALL THE CODE IN ONE FILE, WHICH WILL BE TAKEN AND ADDED TO THE ACTUAL TORNADO CODE CHUNKS LOWER DOWN.


#p_PD  <- 0.05
#p_FD  <- 0.02      
#u_F <- 0.5
#p_FD_SoC  <- 0.05
#p_FD_Exp  <- 0.05

#p_FA1_SoC  <- p_FA1_STD
#p_FA2_SoC  <- p_FA2_STD
#p_FA3_SoC  <- p_FA3_STD

# Because of how p_FA1_SoC is made, [i.e., p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD means that I will get a different value of p_FA1_SoC for each cycle based on multiplying a static value of p_FA1_STD by the varying value of p_PFS_SoC] I will get the following error if I include it as it was created. I can address this just like the hazard ratio problem, but at the moment I havent thought about probabilities and the whole (p_PFS_SoC) * p_FA1_STD part too deeply, so instead, I decide to make it non-varying by including just the static value that is multiplied by the changing p_PFS_SoC value as above. This all also holds for p_FA2_SoC and p_FA3_SoC.


# Error in data.frame(pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC",  : 
#  arguments imply differing number of rows: 29, 386


# p_A1F_SoC = 0.01


# p_A1D_SoC <- 0.00100000000000000000


# I was having some trouble with the A1D probabilities - with the basecase not actually being between the min and the max. My assumptions was that because the numbers were so small they were being rounded and thus moved to a value that wasnt bigger or smaller than the base case value. Although I don't think this should fix things, as this is just the format in which things are printed to the screen, rather than the value that is saved in the parameter, I still set R to display as many digits as possible (22 is the max) per: https://stackoverflow.com/questions/6669681/preventing-r-from-rounding, as suspected things are unchanged whether options(digits=2) of options(digits=22)

# I realised that the issue was that when the parameter was set equal to a value in the list below, this only changed the parameter value in the list, not outside the list, so when I create my min and max using that parameter, it is using it's initial value, not the value assigned in the list, the problem with this is that if the parameter value in the list and the value in the min and max calculation don't align, then the basecase value of the parameter from the list is unlikely to lie between the min and max created from this parameter value outside the list. I have a github commit where I get into this in greater detail here: https://github.com/JonathanBriody/COLOSSUS_Model/commit/3b0cac700598e8d111c96ab39e13ce656ae80210 if the link doesnt work it's a commit called "the problem was this" around 5.15pm on the 20/07/22

options(digits=4)

## 4.1 Initialization ----

# Load the model as a function that is defined in the supporting script
# source("Functions_markov_3state.R")
# Test function
# calculate_ce_out(l_params_all)

source(file = "oncologySemiMarkov_function.R")
# If I change any code in this main model code file, I will also need to update the function that I call.


# Create list l_params_all with all input probabilities, costs, utilities, etc.,

# Test whether the function works (and generates the same results)
# - to do so, first a list of parameter values needs to be generated

# In Koen's code they manually set the value of things here, i.e.   HR_FP_Exp = 0.6, however, to support replicability I just set the variables equal to themselves for this list, and that way if I change something like HR_FP_Exp = 0.6, to HR_FP_Exp = 0.9 earlier in this R code it will automatically update here as well.

# I know that the values they used in this section were the same as their earlier values as they gave identical results when changed from manual values to setting the variable names equal to themselves, apart from .01 of a difference for NMB, but that's probably a rounding thing:



# 
# > l_params_all <- list(
# +   coef_weibull_shape_SoC = 0.7700246,
# +   coef_weibull_scale_SoC = 1.742534,
# +   HR_FP_Exp = 0.6,
# +   p_FD      = 0.02,      
# +   p_PD      = 0.1,      
# +   c_F_SoC   = 400,  
# +   c_F_Exp   = 800,
# +   c_P       = 1000,  
# +   c_D       = 0,     
# +   u_F       = 0.8,   
# +   u_P       = 0.5,   
# +   u_D       = 0,  
# +   d_e       = 0.03,  
# +   d_c       = 0.03,
# +   n_cycle   = 60,
# +   t_cycle   = 0.25
# + )
# > 
# > oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)
#           Strategy     Cost   Effect      NMB
# 1 Standard of Care 11493.14 3.725848 63023.81
# 2     Experimental 18913.35 4.148299 64052.63
# > l_params_all <- list(
# +   coef_weibull_shape_SoC = coef_weibull_shape_SoC,
# +   coef_weibull_scale_SoC = coef_weibull_scale_SoC,
# +   HR_FP_Exp = HR_FP_Exp,
# +   p_FD      = p_FD,      
# +   p_PD      = p_PD,      
# +   c_F_SoC   = c_F_SoC,  
# +   c_F_Exp   = c_F_Exp,
# +   c_P       = c_P,  
# +   c_D       = c_D,     
# +   u_F       = u_F,   
# +   u_P       = u_P,   
# +   u_D       = u_D,  
# +   d_e       = d_e,  
# +   d_c       = d_c,
# +   n_cycle   = n_cycle,
# +   t_cycle   = t_cycle
# + )
# > oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)
#           Strategy     Cost   Effect      NMB
# 1 Standard of Care 11493.14 3.725848 63023.82
# 2     Experimental 18913.35 4.148299 64052.64

# Now I update this list with the variables I have:

# If I updated utility for AE above, then I'll have to take that into account for u_F below:

l_params_all <- list(
  HR_FP_Exp = HR_FP_Exp,
  HR_FP_SoC = HR_FP_SoC,
  HR_PD_Exp = HR_PD_Exp,
  HR_PD_SoC = HR_PD_SoC,
  P_OSD_SoC = P_OSD_SoC,      
  P_OSD_Exp = P_OSD_Exp,
  p_FA1_STD = p_FA1_STD,
  p_FA2_STD = p_FA2_STD,
  p_FA3_STD = p_FA3_STD,
  p_FA1_EXPR = p_FA1_EXPR,
  p_FA2_EXPR = p_FA2_EXPR,
  p_FA3_EXPR = p_FA3_EXPR,
  administration_cost = administration_cost,
    #ITS FINE TO INCLUDE THE INGRIDENIENTS THAT MAKE UP c_F_SoC, c_F_Exp, c_P, but don't include c_F_SoC, c_F_Exp, c_P themsleves, BECAUSE WE ARE CHANGING WHAT BUILDS THESE COSTS WITH THE INGRIEDIENTS, SO WE DON'T WANT TO CHANGE IT AGAIN HERE ONCE WE'VE CHANGED WHAT BUILDS IT
  c_PFS_Folfox = c_PFS_Folfox,
  c_PFS_Bevacizumab = c_PFS_Bevacizumab,
  c_OS_Folfiri = c_OS_Folfiri,
  c_D       = c_D,    
  c_AE1 = c_AE1,
  c_AE2 = c_AE2,
  c_AE3 = c_AE3,
  u_F = u_F,   
  #ITS FINE TO INCLUDE U_F BUT DON'T INCLUDE U_F_SoC, BECAUSE WE ARE CHANGING WHAT BUILDS U_F_SoC WTH U_F AND AE1_DisUtil SO WE DON'T WANT TO CHANGE IT AGAIN HERE ONCE WE'VE CHANGED WHAT BUILDS IT
  u_P = u_P,   
  u_D = u_D,  
  AE1_DisUtil = AE1_DisUtil,
  AE2_DisUtil = AE2_DisUtil,
  AE3_DisUtil = AE3_DisUtil,
  d_e       = d_e,  
  d_c       = d_c,
  n_cycle   = n_cycle,
  t_cycle   = t_cycle
)

 
      #######################################################################
  
#    p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD
#    p_FA2_SoC  <- (p_PFS_SoC) * p_FA2_STD
#    p_FA3_SoC  <- (p_PFS_SoC) * p_FA3_STD
    
    # I think that, to ensure the probability is a conditional probability for p_FA1_SoC, I need to define it as above, because I know that p_PFS_SoC changes due to the hazard ratio changes, BUT, if I want to alter the probability of the adverse event in my model, I should let: p_FA1_STD reflect my adverse event and include this in the OWSA code, rather than p_FA1_SoC - because p_FA1_STD can be changed by 20% and then this code above will create p_FA1_SoC conditional on the new probability for p_PFS_SoC as this changes with the changing hazard ratios in the senstivity analysis. So, p_FA1_SoC will be conditional on the updated p_PFS_SoC rather than the original p_PFS_SoC pre-hazard ratio changes to p_PFS_SoC.

# The above was applicable when adverse events represented health states initially.



# Test function

# Test whether the function works (and generates the same results)

oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 45000)



# I can't use these probabilities because they are from Free to Progressed, but now those probabilities are time-sensitive, so if I try to change these here and include them in the tornado diagram I'll be including too many things, as I explain in my description on hazard ratios.

# Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
# Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC
# 
# 
# Minimum_p_FP_Exp <- p_FP_Exp - 0.20*p_FP_Exp
# Maximum_p_FP_Exp <- p_FP_Exp + 0.20*p_FP_Exp


# Hazard Ratios:

# To vary transition probabilities is slightly complex in a time dependent model.


# FOR THE TRANSITION PROBABILITY FROM FREE TO PROGRESSION WE CAN MULTIPLY THAT PARAMETER BY 20% ONCE AND IT WILL CHANGE THE TRANSITION PROBABILITIES AT EACH CYCLE 
 
# > Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
# > Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC

# > p_FP_SoC
#  ...0.249399924
# > Minimum_p_FP_SoC
#  ...0.1995199390
# > Maximum_p_FP_SoC
#  ...0.299279908

# 0.0499 is 20% of 0.249399924
# 0.249399924 - 0.0499 = 0.199499924
# 0.249399924 + 0.0499 = 0.299299924 

# So, this allows us to have the 20% variability on our time dependent transition probabilities


# WE DONT NEED TO HAVE A MAX (MEAN+20%) AND MIN (MEAN-20%) FOR THE HAZARD RATIO TOO IF WE HAVE THESE FOR TRANSITION PROBABILITIES FOR THE EXPERIMENTAL TREATMENT.

# There's a problem below, df_params_OWSA doesnt like the fact that a different probability for each cycle (from the time-dependent transition probabilities) gives 122 rows. 
 
# > df_params_OWSA <- data.frame(
# +   pars = c("c_F_Exp", "u_F", "p_FP_SoC", "p_FP_Exp"),   # names of the parameters to be changed
# +   min  = c(400,  0.70, Minimum_p_FP_SoC, Minimum_p_FP_Exp),         # min parameter values
# +   max  = c(1200, 0.90, Maximum_p_FP_SoC, Maximum_p_FP_Exp)          # max parameter values
# + )
# Error in data.frame(pars = c("c_F_Exp", "u_F", "p_FP_SoC", "p_FP_Exp"),  : 
#   arguments imply differing number of rows: 4, 122
# 

# To address this, I can go back to the Goldstein approach and change the hazard ratio instead. For the experimental strategy this is simple to do as there already is a hazard ratio in generating experimental strategy transition probabilities. 

# For the standard of care strategy I'll have to add a hazard ratio into oncologySemiMarkov_function that is equal to 1 and get's multiplied by the standard of care stuff first, then I can vary this by a min and a max as above.





HR_FP_Exp
    
Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp


HR_FP_SoC
    
Minimum_HR_FP_SoC <- HR_FP_SoC - 0.20*HR_FP_SoC
Maximum_HR_FP_SoC <- HR_FP_SoC + 0.20*HR_FP_SoC


# Now that we're using the OS curves, I add hazard ratios for PFS to dead that reflect the hazard ratio of the experimental strategy changing the probability of going from PFS to Death, and the hazard ratio of 1 that apply in standard of care so that I can vary transition probabilities under standard of care in this one-way sensitivity analysis:

HR_PD_SoC
    
Minimum_HR_PD_SoC <- HR_PD_SoC - 0.20*HR_PD_SoC
Maximum_HR_PD_SoC <- HR_PD_SoC + 0.20*HR_PD_SoC


HR_PD_Exp
    
Minimum_HR_PD_Exp <- HR_PD_Exp - 0.20*HR_PD_Exp
Maximum_HR_PD_Exp <- HR_PD_Exp + 0.20*HR_PD_Exp








# Probability of progressive disease to death:

# Under the assumption that everyone will get the same second line therapy, I give them all the same probability of going from progessed (i.e., OS) to dead, and thus only need to include p_PD here once - because it is applied in oncologySemiMarkov_function.R for both SoC and Exp. ACTUALLY I THINK IT SHOULD BE P_OSD_SoC P_OSD_Exp BOTH INCLUDED.

P_OSD_SoC

Minimum_P_OSD_SoC <- P_OSD_SoC - 0.20*P_OSD_SoC
Maximum_P_OSD_SoC <- P_OSD_SoC + 0.20*P_OSD_SoC

P_OSD_Exp

Minimum_P_OSD_Exp <- P_OSD_Exp - 0.20*P_OSD_Exp
Maximum_P_OSD_Exp <- P_OSD_Exp + 0.20*P_OSD_Exp







# Probability of going from PFS to Death states under the standard of care treatment and the experimental treatment:

# # HR_PD_SoC and HR_PD_Exp address this as above:

# p_FD_SoC
# 
# Minimum_p_FD_SoC <- p_FD_SoC - 0.20*p_FD_SoC
# Maximum_p_FD_SoC <- p_FD_SoC + 0.20*p_FD_SoC
# 
# p_FD_Exp
# 
# Minimum_p_FD_Exp<- p_FD_Exp - 0.20*p_FD_Exp
# Maximum_p_FD_Exp <- p_FD_Exp + 0.20*p_FD_Exp



# Probability of Adverse Events:

p_FA1_STD
Minimum_p_FA1_STD <- p_FA1_STD - 0.20*p_FA1_STD
Maximum_p_FA1_STD <- p_FA1_STD + 0.20*p_FA1_STD

p_FA2_STD
Minimum_p_FA2_STD <- p_FA2_STD - 0.20*p_FA2_STD
Maximum_p_FA2_STD <- p_FA2_STD + 0.20*p_FA2_STD

p_FA3_STD
Minimum_p_FA3_STD <- p_FA3_STD - 0.20*p_FA3_STD
Maximum_p_FA3_STD <- p_FA3_STD + 0.20*p_FA3_STD


p_FA1_EXPR
Minimum_p_FA1_EXPR <- p_FA1_EXPR - 0.20*p_FA1_EXPR
Maximum_p_FA1_EXPR <- p_FA1_EXPR + 0.20*p_FA1_EXPR

p_FA2_EXPR
Minimum_p_FA2_EXPR <- p_FA2_EXPR - 0.20*p_FA2_EXPR
Maximum_p_FA2_EXPR <- p_FA2_EXPR + 0.20*p_FA2_EXPR

p_FA3_EXPR
Minimum_p_FA3_EXPR <- p_FA3_EXPR - 0.20*p_FA3_EXPR
Maximum_p_FA3_EXPR <- p_FA3_EXPR + 0.20*p_FA3_EXPR






#install.packages("Rmpfr")
#library(Rmpfr)

# 
# Minimum_p_A1D_SoC <- mpfr(Minimum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A1D_SoC <- mpfr(Maximum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
# 
# 
# Minimum_p_A2D_SoC <- mpfr(Minimum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A2D_SoC <- mpfr(Maximum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
# 
# Minimum_p_A3D_SoC <- mpfr(Minimum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A3D_SoC <- mpfr(Maximum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default




# Cost:

# If I decide to include the cost of the test for patients I will also need to include this in the sensitivity analysis here:

administration_cost

Minimum_administration_cost <- administration_cost - 0.20*administration_cost
Maximum_administration_cost <- administration_cost + 0.20*administration_cost

c_PFS_Folfox

Minimum_c_PFS_Folfox  <- c_PFS_Folfox - 0.20*c_PFS_Folfox
Maximum_c_PFS_Folfox  <- c_PFS_Folfox + 0.20*c_PFS_Folfox

c_PFS_Bevacizumab 

Minimum_c_PFS_Bevacizumab  <- c_PFS_Bevacizumab - 0.20*c_PFS_Bevacizumab
Maximum_c_PFS_Bevacizumab  <- c_PFS_Bevacizumab + 0.20*c_PFS_Bevacizumab

c_OS_Folfiri 

Minimum_c_OS_Folfiri  <- c_OS_Folfiri - 0.20*c_OS_Folfiri
Maximum_c_OS_Folfiri  <- c_OS_Folfiri + 0.20*c_OS_Folfiri

c_D  

Minimum_c_D  <- c_D - 0.20*c_D
Maximum_c_D  <- c_D + 0.20*c_D

c_AE1

Minimum_c_AE1  <- c_AE1 - 0.20*c_AE1
Maximum_c_AE1  <- c_AE1 + 0.20*c_AE1

c_AE2

Minimum_c_AE2  <- c_AE2 - 0.20*c_AE2
Maximum_c_AE2  <- c_AE2 + 0.20*c_AE2

c_AE3

Minimum_c_AE3  <- c_AE3 - 0.20*c_AE3
Maximum_c_AE3  <- c_AE3 + 0.20*c_AE3


# Utilities:

u_F

Minimum_u_F <- u_F - 0.20*u_F
Maximum_u_F <- u_F + 0.20*u_F 


u_P

Minimum_u_P <- u_P - 0.20*u_P
Maximum_u_P <- u_P + 0.20*u_P 


u_D

Minimum_u_D <- u_D - 0.20*u_D
Maximum_u_D <- u_D + 0.20*u_D 


AE1_DisUtil

Minimum_AE1_DisUtil <- AE1_DisUtil - 0.20*AE1_DisUtil
Maximum_AE1_DisUtil <- AE1_DisUtil + 0.20*AE1_DisUtil 


AE2_DisUtil

Minimum_AE2_DisUtil <- AE2_DisUtil - 0.20*AE2_DisUtil
Maximum_AE2_DisUtil <- AE2_DisUtil + 0.20*AE2_DisUtil 


AE3_DisUtil

Minimum_AE3_DisUtil <- AE3_DisUtil - 0.20*AE3_DisUtil
Maximum_AE3_DisUtil <- AE3_DisUtil + 0.20*AE3_DisUtil 


 
 
# Discount factor
# Cost Discount Factor
# Utility Discount Factor
# I divided these by 365 earlier in the R markdown document, so no need to do that again here:

d_e

Minimum_d_e <- d_e - 0.20*d_e
Maximum_d_e <- d_e + 0.20*d_e



d_c

Minimum_d_c <- d_c - 0.20*d_c
Maximum_d_c <- d_c + 0.20*d_c

# 
# df_params_OWSA <- data.frame(
#   pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A1D_SoC", "p_A2D_SoC", "p_A3D_SoC"),   # names of the parameters to be changed
#   min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_p_PD, Minimum_p_FD_SoC, Minimum_p_FD_Exp, Minimum_p_FA1_SoC, Minimum_p_A1F_SoC, Minimum_p_FA2_SoC, Minimum_p_A2F_SoC, Minimum_p_FA3_SoC, Minimum_p_A3F_SoC, Minimum_p_A1D_SoC, Minimum_p_A2D_SoC, Minimum_p_A3D_SoC),         # min parameter values
#   max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_p_PD, Maximum_p_FD_SoC, Maximum_p_FD_Exp, Maximum_p_FA1_SoC, Maximum_p_A1F_SoC, Maximum_p_FA2_SoC, Maximum_p_A2F_SoC, Maximum_p_FA3_SoC, Maximum_p_A3F_SoC, Maximum_p_A1D_SoC, Maximum_p_A2D_SoC, Maximum_p_A3D_SoC)          # max parameter values
# )

# A one-way sensitivity analysis (OWSA) can be defined by specifying the names of the parameters that are to be incuded and their minimum and maximum values.


# We create a dataframe containing all parameters we want to do the sensitivity analysis on, and the min and max values of the parameters of interest 
# "min" and "max" are the mininum and maximum values of the parameters of interest.


# options(scipen = 999) # disabling scientific notation in R

df_params_OWSA <- data.frame(
  pars = c("HR_FP_Exp", "HR_FP_SoC", "HR_PD_SoC", "HR_PD_Exp", "P_OSD_SoC", "P_OSD_Exp", "p_FA1_STD", "p_FA2_STD", "p_FA3_STD", "p_FA1_EXPR", "p_FA2_EXPR", "p_FA3_EXPR", "administration_cost", "c_PFS_Folfox", "c_PFS_Bevacizumab", "c_OS_Folfiri", "c_AE1", "c_AE2", "c_AE3", "d_e", "d_c", "u_F", "u_P", "AE1_DisUtil", "AE2_DisUtil", "AE3_DisUtil"),   # names of the parameters to be changed
  min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_HR_PD_SoC, Minimum_HR_PD_Exp, Minimum_P_OSD_SoC, Minimum_P_OSD_Exp, Minimum_p_FA1_STD, Minimum_p_FA2_STD, Minimum_p_FA3_STD, Minimum_p_FA1_EXPR, Minimum_p_FA2_EXPR, Minimum_p_FA3_EXPR, Minimum_administration_cost, Minimum_c_PFS_Folfox, Minimum_c_PFS_Bevacizumab, Minimum_c_OS_Folfiri, Minimum_c_AE1, Minimum_c_AE2, Minimum_c_AE3, Minimum_d_e, Minimum_d_c, Minimum_u_F, Minimum_u_P, Minimum_AE1_DisUtil, Minimum_AE2_DisUtil, Minimum_AE3_DisUtil),         # min parameter values
  max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_HR_PD_SoC, Maximum_HR_PD_Exp, Maximum_P_OSD_SoC, Maximum_P_OSD_Exp, Maximum_p_FA1_STD, Maximum_p_FA2_STD, Maximum_p_FA3_STD, Maximum_p_FA1_EXPR, Maximum_p_FA2_EXPR, Maximum_p_FA3_EXPR, Maximum_administration_cost, Maximum_c_PFS_Folfox, Maximum_c_PFS_Bevacizumab, Maximum_c_OS_Folfiri, Maximum_c_AE1,  Maximum_c_AE2, Maximum_c_AE3, Maximum_d_e, Maximum_d_c, Maximum_u_F, Maximum_u_P, Maximum_AE1_DisUtil, Maximum_AE2_DisUtil, Maximum_AE3_DisUtil)          # max parameter values
)



    








# I made sure the names of the parameters to be varied and their mins and maxs are in the same order in all the brackets above in order to make sure that the min and max being applied are the min and the max of the parameter I want to consider a min and a max for.



# The OWSA is performed using the run_owsa_det function


# This function runs a deterministic one-way sensitivity analysis (OWSA) on a given function that produces outcomes. rdrr.io/github/DARTH-git/dampack/src/R/run_dsa.R

DSAICER  <- run_owsa_det(

# run_owsa_det: https://rdrr.io/github/DARTH-git/dampack/man/run_owsa_det.html

  # We need to make sure we consistently use "DSAICER" throughout, or else the function will present with an error saying "DSAICER" not found.  
   
# Arguments:
  
  params_range     = df_params_OWSA,     # dataframe with parameters for OWSA

# params_range	
# data.frame with 3 columns of parameters for OWSA in the following order: "pars", "min", and "max".
# The number of samples from this range is determined by nsamp. 
# "pars" are the parameters of interest and must be a subset of the parameters from params_basecase.


# Details
# params_range
 
# "pars" are the names of the input parameters of interest. These are the parameters that will be varied in the deterministic sensitivity analysis. variables in "pars" column must be a subset of variables in params_basecase
 

  
  params_basecase  = l_params_all,       # list with all parameters

# params_basecase	

# a named list of basecase values for input parameters needed by FUN, the user-defined function. So, I guess it takes the values that the parameters are equal to in l_params_all as the base case, so if cost is generated equal to 1,000 it'll take that as the base case, and then take the min and the max around this from the data.frame we created above.

# To conduct the one-way DSA, we then call the function run_owsa_det with my_owsa_params_range, specifying the parameters to be varied and over which ranges, and my_params_basecase as the fixed parameter values to be used in the model when a parameter is not being explicitly varied in the one-way sensitivity analysis. https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html

  nsamp            = 100,                # number of parameter values

# nsamp	

# number of sets of parameter values to be generated. If NULL, 100 parameter values are used -> I think Eva Enns said these are automatically evenly spaced out values of the parameters.

# Additional inputs are the number of equally-spaced samples (nsamp) to be used between the specified minimum and maximum of each range, the user-defined function (FUN) to be called to generate model outcomes for each strategy, the vector of outcomes to be stored (must be outcomes generated by the data frame output of the function passed in FUN), and the vector of strategy names to be evaluated.

# cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html

  FUN              = oncologySemiMarkov, # function to compute outputs

# FUN	
# function that takes the basecase in params_basecase and runs the analysis in the function... to produce the outcome of interest. The FUN must return a dataframe where the first column are the strategy names and the rest of the columns must be outcomes.
#  

 outcomes         = c("DSAICER"),           # output to do the OWSA on

# string vector with the outcomes of interest from FUN produced by nsamp
# This basically tells run_owsa_det what the name of the outcome of interest from the function we fed it is. Here our function previously had NMB, i.e., the net monetary benefit.

#  outcomes         = c("NMB"),           # output to do the OWSA on

# outcomes	

  strategies       = v_names_strats,       # names of the strategies

# strategies
# Set it equal to a vector of strategy names. The default NULL will use strategy names in FUN (  strategies = NULL,)
# Here that's "Standard of Care" and "Experimental Treatment".

  progress = TRUE,

# progress	
# TRUE or FALSE for whether or not function progress should be displayed in console, i.e., like 75% complete, 100%, etc.,

# The input progress = TRUE allows the user to see a progress bar as the DSA is conducted. When many parameters are being varied, nsamp is large, and/or the user-defined function is computationally burdensome, the DSA may take a noticeable amount of time to compute and the progress display is recommended.
# cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html


  n_wtp            = 45000               # extra argument to pass to FUN to specify the willingness to pay
)

# Value
# A list containing dataframes with the results of the sensitivity analyses. The list will contain a dataframe for each outcome specified. List elements can be visualized with plot.owsa, owsa_opt_strat and owsa_tornado from dampack

# Basically, run_owsa_det creates the above.

# Also, each owsa object returned by run_owsa_det is a data frame with four columns: parameter, strategy, param_val, and outcome_val. For each row, param_val is the value used for the parameter listed in parameter and outcome_val is the value of the specified outcome for the strategy listed in strategy. 

# So, OWSA_NMB shows a row is created for each sampling of the parameter value from min to max, so 100 rows per parameter (because nsamp = 100,), and the outcome value associated with a parameter value of this size is displayed under outcome_val.

# You can see how this works by putting the below into the console.

# OWSA_NMB

# Or just DSAICER for DSAICER.

# Resources on this available here:


# https://rdrr.io/github/DARTH-git/dampack/man/run_owsa_det.html (also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\sensitivity analysis help files\rdrr-io-github-DARTH-git-dampack-man-run_owsa_det-html.pdf)


# This link is pretty good for showing that the OWSA can be run across a variety of outcomes outcomes = c("Cost", "QALY", "LY", "NMB"), and then you can pick which of the OWSA outcomes you'd like to focus on and build tornado diagrams, etc., from that:

# "Because we have defined multiple parameters in my_owsa_params_range, we have instructed run_owsa_det to execute a series of separate one-way sensitivity analyses and compile the results into a single owsa object for each requested outcome. When only one outcome is specified run_owsa_det returns a owsa data frame. When more than one outcome is specified, run_owsa_det returns a list containing one owsa data frame for each outcome. To access the owsa object corresponding to a given outcome, one can select the list item with the name “owsa_”. For example, the owsa object associated with the NMB outcome can be accessed as l_owsa_det$owsa_NMB."

# https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html 

owsa_tornado(owsa = DSAICER, txtsize = 11)

# Plotting the outcomes of the OWSA in a tornado plot
# - note that other plots can also be generated using the plot() and owsa_opt_strat() functions



# owsa_tornado(DSAICER,
# 
# min_rel_diff = 0.05)

# For owsa objects that contain many parameters that have minimal effect on the parameter of interest, you may want to consider producing a plot that highlights only the most influential parameters. Using the min_rel_diff argument, you can instruct owsa_tornado to exclude all parameters that fail to produce a relative change in the outcome below a specific fraction.


# Maybe use the below to change the formatting of the above:
# TornadoPlot(main_title = "Tornado Plot", Parms = paramNames, Outcomes = m.tor, 
#             outcomeName = "Incremental Cost-Effectiveness Ratio (ICER)", 
#             xlab = "ICER", 
#             ylab = "Parameters", 
#             col1="#3182bd", col2="#6baed6")

```

## 08.3.1 Plot OWSA

```{r}
plot(DSAICER, txtsize = 10, n_x_ticks = 4, 
     facet_scales = "free") +
  theme(legend.position = "bottom") 

# I can add the following to the above: 

# + ggtitle("Expected Value of Perfect Information")

# To include a title on this plot, per: https://mran.microsoft.com/snapshot/2021-03-21/web/packages/dampack/dampack.pdf

#txtsize - base text size

# n_y_ticks - number of axis ticks
# n_x_ticks	- number of axis ticks

# Function for determining number of ticks on axis of ggplot2 plots. https://www.quantargo.com/help/r/latest/packages/dampack/1.0.1/number_ticks


# [SO, IN THE DIAGRAM THE PARAMETER WE ARE CONSIDERING IS AT THE TOP OF THE DIAGRAM, THE NET MONETARY BENEFIT IS ON THE LEFT OF THE DIAGRAM AND YOU'LL SEE THAT THE LEFT EDGE OF THE LINE IS THE MINIMUM VALUE FOR THE PARAMETER WE ARE INTERESTED IN, THE RIGHT EDGE OF THE LINE IS THE MAXIMUM VALUE FOR THE PARAMETER WE ARE INTERESTED IN, AND SOMEWHERE IN THE MIDDLE IS THE BASECASE WE ASSIGNED THIS PARAMETER. A GOOD EXAMPLE OF THIS IS c_P, COST OF PROGRESSION, WHERE THE BASECASE WAS 1,000 EURO, WHICH WE SEE IN THE MIDDLE, THE MINIMUM IS 800 EURO WHICH WE SEE ON THE FAR LEFT OF THE LINE, AND THE MAXIMUM IS 1,200 WHICH WE SEE ON THE FAR RIGHT OF THE LINE, AND WE SEE THAT AS WE MOVE FROM THE MINUMUM, THE 800 EURO ON THE FAR LEFT OF THE LINE, TO THE MAXIMUM, THE 1200 EURO ON THE FAR RIGHT OF THE LINE (I.E., AS THE COST OF BEING IN THE PROGRESSED STATE INCREASES) THE NET MONETARY BENEFIT OF BOTH TREATMENT OPTIONS FALL, WITH THE NMB OF THE EXPERIMENTAL TREATMENT OPTION FALLING FROM 235,000 TO 230,000].



# Explained here: https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html


# and here: https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html


# Because I'm using ICERs, there's only one value, whereas for NMB, there's a value under each treatment strategy, but ICERs are ratios comparing the costs of one treatment strategy to another, and the outcomes of one treatment strategy to another. So, where when I was using NMB as my outcome I would have a curve for "SoC" and "Experimental Treatment", because each would get an NMB, now I only have the one curve, to show how the ICER value changes. So, before I could have how the net monetary benefit changed under each treatment strategy as the cost of treatment A went up, or the rate of changing from healthy to dead went up, per the "# Plot outcome of each strategy over each parameter range" on: https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html, now I only have the ICER, so I see how the ICER changes as the cost of parameters, etc., change.

# Because only one ICER value is created, but it's created for both SoC and Exp, I imagine that the exp line and SoC line are created, but sit on top of eachother in the diagram below:

```

## 08.3.2 Optimal strategy with OWSA

```{r}

# There are too many parameters to make things legible, so instead you should create a tornado plot that highlights only the most influential parameters. Using the min_rel_diff argument, you can instruct owsa_tornado to exclude all parameters that fail to produce a relative change in the outcome below a specific fraction.

# owsa_tornado(OWSA_NMB,
# 
# min_rel_diff = 0.05)

# Following this, create a vector of the parameters that appear in this tornado plot below and then create the optimal strategy with OWSA per these parameters.

# v_owsa_opt_strat <- c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_A1D_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_A2D_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A3D_SoC", "c_F_SoC", "c_F_Exp", "c_P","c_AE1", "c_AE2", "c_AE3", "d_e", "d_c", "u_F", "u_P", "u_AE1", "u_AE2", "u_AE3")

# Then include params = v_owsa_opt_strat, in the below and get rid of plot_const = FALSE

owsa_opt_strat(owsa = DSAICER, txtsize = 10, plot_const = FALSE)


# plot_const = FALSE
# plot_const	
# whether to plot parameters that don't lead to changes in optimal strategy as they vary.

# params	
# params = v_owsa_opt_strat,
# vector of parameters to plot

# return	
# either return a ggplot object plot or a data frame with ranges of parameters for which each strategy is optimal. return = c("plot", "data"),

# Basically, the data frame will show you the ranges for each parameter that each strategy is optimal.You can see below that for p_FD_Exp the experimental treatment is the optimal strategy from 0.04 (the minimum) through to 0.055 (0.05575757575757575), while standard of care is the optimal strategy from 0.059 (0.05595959595959596) through to 0.06 (the maximum). 

# p_FD_Exp	Experimental.Treatment	4.0000000000000001e-02	5.5757575757575756e-02	
# p_FD_Exp	Standard.of.Care	5.5959595959595959e-02   6.0000000000000005e-02	

# Whereas for p_FA3_SoC the experimental treatment strategy is optimal from it's min 0.016 all the way through every value in the range to it's max: 0.024 

# p_FA3_SoC	Experimental.Treatment	1.6000000000000000e-02	2.4000000000000000e-02	

# The data frame is a bit of a pain though because it returns things in scientific notation, but according to the below:

 
# Scientific Notation
# E+01 means moving the decimal point one digit to the right, E+00 means leaving the decimal point where it is, and E–01 means moving the decimal point one digit to the left. Example: 1.00E+01 is 10, 1.33E+00 stays at 1.33, and 1.33E–01 becomes 0.133.
# https://neo.ne.gov/programs/stats/inf/79.htm#:~:text=Scientific%20Notation,-The%20scientific%20notation&text=E%2B01%20means%20moving%20the,1.33E%E2%80%9301%20becomes%200.133.

# There's also a good calculator to do this for you here: https://www.free-online-calculator-use.com/scientific-notation-converter.html#

# https://rdrr.io/github/DARTH-git/dampack/man/owsa_opt_strat.html


# Like owsa_tornado(), the return argument in owsa_opt_strat() allows the user to access a tidy data.frame that contains the exact values used to produce the plot.

# owsa_opt_strat(o, 
#                return = "data")

# https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html



# If return == "plot", a ggplot2 optimal strategy plot derived from the owsa object, or if return == "data", a data.frame containing all data contained in the plot. The plot allows us to see how the strategy that maximizes the expectation of the outcome of interest changes as a function of each parameter of interest.

# Visualize optimal strategy (max NMB) over each parameter range
# owsa_opt_strat(my_owsa_NMB)

# facet_ncol	
# Number of columns in plot facet.

# facet_nrow	
# number of rows in plot facet.

# txtsize
# base text size

# facet_lab_txtsize	
# text size for plot facet labels (I think this allows me change the size of the text on the plots and the size of the numbers, ledgend, etc., independently).

# Explained here: 

# https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html

# https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html

# https://rdrr.io/github/DARTH-git/dampack/man/owsa_opt_strat.html

# https://www.quantargo.com/help/r/latest/packages/dampack/1.0.1/owsa_opt_strat

```

## 

## 08.4 Two-way sensitivity analysis (TWSA)

```{r}


# To conduct the two-way DSA, we call the function run_twsa_det with my_twsa_params_range. The general format of the function arguments for run_twsa_det are the same as those for run_owsa_det. In run_twsa_det, equally spaced sequences of length nsamp are created for the two parameters based on the inputs provided in the params_range argument. These two sequences of parameter values define an nsamp by nsamp grid over which FUN is applied to produce outcomes for every combination of the two parameters. https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html


## 4.2 Defining and performing a two-way sensitivity analysis ----

# I can also preform a TWSA on a a probabilistic sensitivity analysis (make_psa_obj) or a deterministic sensitivity analysis object (run_owsa_det) per: https://rdrr.io/github/DARTH-git/dampack/man/twsa.html

# A lot of this code should be reflective of the earlier code of the one-way sensitivity analysis, so if I'm confused as to what one thing is doing or another I can look at the piece of code that is confusing me here, and scroll back up to my explanation of that code in OWSA.

# Run deterministic two-way sensitivity analysis (TWSA) https://rdrr.io/github/DARTH-git/dampack/src/R/run_dsa.R

# To perform a two-way sensitivity analysis (TWSA), a similar data.frame with model parameters is required

# dataframe containing all parameters, their basecase values, and the min and 
# max values of the parameters of interest

df_params_TWSA <- data.frame(pars = c("HR_FP_SoC", "HR_FP_Exp"),
                             min  = c(Minimum_HR_FP_SoC, Minimum_HR_FP_Exp),  # min parameter values
                             max  = c(Maximum_HR_FP_SoC, Maximum_HR_FP_Exp) # max parameter values
)

# We could have chosen any of the below from our OWSA data.frame to use as our parameters:
# df_params_OWSA <- data.frame(
#   pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_A1D_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_A2D_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A3D_SoC", "c_F_SoC", "c_F_Exp", "c_P","c_AE1", "c_AE2", "c_AE3", "d_e", "d_c", "u_F", "u_P", "u_AE1", "u_AE2", "u_AE3"),   # names of the parameters to be changed
#   min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_p_PD, Minimum_p_FD_SoC, Minimum_p_FD_Exp, Minimum_p_FA1_SoC, Minimum_p_A1F_SoC, Minimum_p_A1D_SoC, Minimum_p_FA2_SoC, Minimum_p_A2F_SoC, Minimum_p_A2D_SoC, Minimum_p_FA3_SoC, Minimum_p_A3F_SoC, Minimum_p_A3D_SoC, Minimum_c_F_SoC, Minimum_c_F_Exp, Minimum_c_P, Minimum_c_AE1, Minimum_c_AE2, Minimum_c_AE3, Minimum_d_e, Minimum_d_c, Minimum_u_F, Minimum_u_P, Minimum_u_AE1, Minimum_u_AE2, Minimum_u_AE3),         # min parameter values
#   max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_p_PD, Maximum_p_FD_SoC, Maximum_p_FD_Exp, Maximum_p_FA1_SoC, Maximum_p_A1F_SoC, Maximum_p_A1D_SoC, Maximum_p_FA2_SoC, Maximum_p_A2F_SoC, Maximum_p_A2D_SoC, Maximum_p_FA3_SoC, Maximum_p_A3F_SoC, Maximum_p_A3D_SoC, Maximum_c_F_SoC, Maximum_c_F_Exp, Maximum_c_P, Maximum_c_AE1,  Maximum_c_AE2, Maximum_c_AE3, Maximum_d_e, Maximum_d_c, Maximum_u_F, Maximum_u_P, Maximum_u_AE1, Maximum_u_AE2, Maximum_u_AE3)          # max parameter values
# )
# 

# It's a pain, but we have to only enter 2 parameters of interest at a time into our model per the error built into the source code for run_twsa_det:

# "two-way sensitivity analysis only allows for and requires 2 different paramters of interest at a time" https://rdrr.io/github/DARTH-git/dampack/src/R/run_dsa.R

# And:

# The structure of the function is very similar to run_owsa_det(). The primary difference is the function can only take two parameters at a time in the params_range. https://syzoekao.github.io/CEAutil/


# The TWSA is performed using the run_twsa_det function


TWSA_DSAICER <- run_twsa_det(params_range    = df_params_TWSA,    # dataframe with parameters for TWSA
                         params_basecase = l_params_all,      # list with all parameters, the "pars" chosen in the data.frame to be analysed here must be a subset of these.
                         
                         nsamp           = 40,                # number of parameter values. If NULL, 40 parameter values are used

 # number of parameter values

# nsamp	

# number of sets of parameter values to be generated. If NULL, 40 parameter values are used -> I think Eva Enns said these are automatically evenly spaced out values of the parameters.

# Additional inputs are the number of equally-spaced samples (nsamp) to be used between the specified minimum and maximum of each range, the user-defined function (FUN) to be called to generate model outcomes for each strategy, the vector of outcomes to be stored (must be outcomes generated by the data frame output of the function passed in FUN), and the vector of strategy names to be evaluated.

# cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html
                         
                         
                         FUN             = oncologySemiMarkov,  # function to compute outputs

# Function that takes the basecase in params_all and produces the outcome of interest. The FUN must return a dataframe where the first column is the strategy names and the rest of the columns must be outcomes. Which df_ce in the function does.

# Described here:

# https://rdrr.io/cran/dampack/man/run_twsa_det.html

                         outcomes        = c("DSAICER"),             # output to do the TWSA on
                         strategies      = v_names_strats,       # names of the strategies.The default (NULL) will use strategy names in FUN
                         progress        = TRUE, #Progress bar like before

                         n_wtp           = 45000               # extra argument to pass to FUN to specify the willingness to pay
)
# Plot TWSA

plot(TWSA_DSAICER)


# Sometimes you see "maximise" written in people's TWSA code, why is that:

# maximize	
# If TRUE, plot of strategy with maximum expected outcome (default); if FALSE, plot of strategy with minimum expected outcome

# per: https://www.quantargo.com/help/r/latest/packages/dampack/1.0.1/plot.twsa



# [[A 2-way uncertainty analysis will be more useful if informed by the covariance between the 2 parameters of interest or on the logical relationship between them (e.g., a 2-way uncertainty analysis might be represented by the control intervention event rate and the hazard ratio with the new treatment). <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>]]

# The Briggs 2012 paper said to include 2 parameters in the TWSA that have a logical relationship,(e.g., a 2-way uncertainty analysis might be represented by the control intervention event rate and the hazard ratio with the new treatment), which are expected to have a relationship because the hazard ratio for the experimental strategy is multiplied by the rate of events under soc, that's how hazard ratios for experimental interventions work, they are just a multiplier for the number of events under SoC in order to give number of events under Exp care, so more events under SoC means more events under the experimental strategy when it's hazard ratio is applied to the number of events under SoC. In the model, the function is called in the sensitivity analysis and applies a hazard ratio

# I also describe applying the Exp HR to the post changes SoC in my function where this is done.

# I can't use Free to Progressed probabilities, because now those probabilities are time-sensitive, so if I try to change these here and include them in the TWSA I'll be including too many things, just like in the tornado diagram, as I explain in my description on hazard ratios.

# Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC

# Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC

# Minimum_p_FP_Exp <- p_FP_Exp - 0.20*p_FP_Exp

# Maximum_p_FP_Exp <- p_FP_Exp + 0.20*p_FP_Exp

# To address this, I can apply this mean, max and min to the hazard ratios instead, knowing that when run_owsa_det is run in the sensitivity analysis it calls the oncology_semi_markov_function to run and in this function the hazard ratios generate the survivor function, and then these survivor functions are used to generate the probabilities (which will be cycle dependent), so I am varying the transition probabilities by 20% using a static value.


# Alterantive modelling approach I could try here if so inclined:


# "When the base-case result of an analysis strongly favors one alternative, a threshold analysis may be presented as a worst-case or ''even if'' analysis (e.g., ''Even if the risk reduction is as low as X, the ICER remains below Y,'' or ''Even if the relative risk reduction with alternative A is as low as X and the cost of treatment is as high as Y, alternative A dominates B''). Threshold values can easily be combined with the tornado presentation by marking them on the horizontal bars."

# <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>


# If for some reason I wanted a three-way sensitivity analysis per Andrew Brigg's discussion on page 8 above, there may be some code to describe doing this here: https://rdrr.io/github/syzoekao/CEAutil/src/inst/rmd/Rcode.R (github here: https://github.com/syzoekao/CEAutil/) and here: https://syzoekao.github.io/CEAutil/#44_two-way_sensitivity_analysis also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\introduction to R for cost-effectiveness analysis.pdf

```

# 09 Probabilistic Sensitivity Analysis (PSA)

If you wanted to calculate the proportion of samples for which the incremental net benefit is positive, or try Gianlucas way of plotting results as a cost-effectiveness plane, you could try BCEA here: hazaC:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Health Economic Modeling in R A Hands-on Introduction\\Health-Eco\\Markov models\\markov_smoking_probabilistic (there's also definitely good tutorials on using BCEA online) and he has slides on this here: <https://gianluca.statistica.it/slides/ispor-2022/bcea/#27>

Here's how to generate the standard error (SE) for a log-normal distribution, and then to make random draws for a log-normal distribution: <https://hesim-dev.github.io/hesim/articles/markov-cohort.html> also here: <https://devinincerti.com/2018/02/10/psa.html#beta-distribution>

The best way to check if the way you are calculating distributions is correct is by making comparisons to publications which include distributions, ses, etc., here are some publications that include that information:

This study has an appendix where they report distribution, SE, base case, range and alpha and beta for the beta distribution and normal distribution: Spackman, D. E., & Veenstra, D. L.
(2008).
A cost-effectiveness analysis of currently approved treatments for HBeAg-positive chronic hepatitis B.
Pharmacoeconomics, 26(11), 937-949.
<https://sci-hub.ru/10.2165/00019053-200826110-00006> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\spackman2008.pdf

Appendix here: <http://download.lww.com/wolterskluwer_vitalstream_com/permalink/pcz/a/00019053-920082611-00002.pdf> and also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\00019053-920082611-00002.pdf

[It also may be worth following some of their writing style.]

This also has mean, higher and lower CI and SE and distribution: <https://www.ncbi.nlm.nih.gov/books/NBK97497/pdf/Bookshelf_NBK97497.pdf> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\Bookshelf_NBK97497.pdf

PDF Page 212 of: Picot, J., et al. "The clinical effectiveness and cost-effectiveness of bortezomib and thalidomide in combination regimens with an alkylating agent and a corticosteroid for the first-line treatment of multiple myeloma: a systematic review and economic evaluation." Health technology assessment (Winchester, England) 15.41 (2011): 1.
<https://www.ncbi.nlm.nih.gov/books/NBK97497/pdf/Bookshelf_NBK97497.pdf>

This study has mean and SE so you may be able to find the range around the mean in the studies they cite: Is it worth offering a routine laparoscopic cholecystectomy in developing countries?
A Thailand case study <https://sci-hub.ru/10.1186/1478-7547-3-10>

Sharp, Linda, et al. "Cost-effectiveness of population-based screening for colorectal cancer: a comparison of guaiac-based faecal occult blood testing, faecal immunochemical testing and flexible sigmoidoscopy." British journal of cancer 106.5 (2012): 805-816.
<https://www.nature.com/articles/bjc2011580.pdf> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\bjc2011580.pdf

Koen has an excel and paper here, but it's probably more useful for point estimates: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Matching the model with the evidence Koen both are saved here, but the filepath may be too long to access them without moving them to downloads or desktop.

This has mean and alpha beta but nothing else: Kristin, Erna, et al. "Economic evaluation of adding bevacizumab to chemotherapy for metastatic colorectal cancer (mCRC) patients in Indonesia." Asian Pacific Journal of Cancer Prevention: APJCP 22.6 (2021): 1921.
<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Evidence%20Synthesis/Economic%20Models/Kristin%20et%20al_2021_Economic%20Evaluation%20of%20Adding%20Bevacizumab%20to%20Chemotherapy%20for%20Metastatic.pdf>

The information provided here is less infromative: Jenks, Michelle, et al. "Tegaderm CHG IV securement dressing for central venous and arterial catheter insertion sites: a NICE medical technology guidance." Applied Health Economics and Health Policy 14.2 (2016): 135-149.
<https://link.springer.com/content/pdf/10.1007/s40258-015-0202-5.pdf>

This may also have informations on SE's given point estimates and 95 CI's Baio, G.
(2014).
Bayesian models for cost‐effectiveness analysis in the presence of structural zero costs.
Statistics in medicine, 33(11), 1900-1913.
<https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4285321/pdf/sim0033-1900.pdf>

Obviously the Goldstein study, but this is more making use of point estiamtes: <file:///C:/Users/Jonathan/Dropbox/PC/Downloads/goldstein2014.pdf>

This may have some limited info on mean, range and SE in the Tables at the end: "How sensitive are cost-effectiveness analyses to choice of parametric distributions?" <https://www.mrc-bsu.cam.ac.uk/wp-content/uploads/CE_para_york.pdf>

There's information on mean alpha beta and se here: Cost-effectiveness of insulin pumps compared with multiple daily injections both provided with structured education for adults with type 1 diabetes: a health economic analysis of the Relative Effectiveness of Pumps over Structured Education (REPOSE) randomised controlled trial <https://bmjopen.bmj.com/content/bmjopen/8/4/e016766.full.pdf>

This may also have some information: Cost-effectiveness and programmatic benefits of maternal vaccination against pertussis in England <https://sci-hub.ru/10.1016/j.jinf.2016.04.012>

This has a point estimate, range, distribution and alpha beta: Campbell, J. R., Johnston, J. C., Cook, V. J., Sadatsafavi, M., Elwood, R. K., & Marra, F.
(2019).
Cost-effectiveness of latent tuberculosis infection screening before immigration to low-incidence countries.
Emerging infectious diseases, 25(4), 661.
<https://sci-hub.ru/10.3201/eid2504.171630>

This has a point estimate, range, distribution, alpha and beta: Gao, L., Moodie, M., Mitchell, P. J., Churilov, L., Kleinig, T. J., Yassi, N., ... & EXTEND-IA TNK Investigators.
(2020).
Cost-effectiveness of tenecteplase before thrombectomy for ischemic stroke.
Stroke, 51(12), 3681-3689.
<https://www.ahajournals.org/doi/epdf/10.1161/STROKEAHA.120.029666>

There is information on point estimates, se, alpha and beta on PDF Page 71 of :A cluster randomised trial, cost-effectiveness analysis and psychosocial evaluation of insulin pump therapy compared with multiple injections during flexible intensive insulin therapy for type 1 diabetes: the REPOSE Trial <https://www.journalslibrary.nihr.ac.uk/hta/hta21200#/abstract>

Slide 39 of this has a mean, se, distribution, low and high 95% interval: <https://clas.ucdenver.edu/marcelo-perraillon/sites/default/files/attached-files/lecture_13_uncertainty.pdf> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\lecture_13_uncertainty.pdf

This has mean, range, shape, scale and distribution for Beta and Log-normal: Appendix F: Health economics report (2015 work undertaken by NICE Internal Clinical Guidelines) <https://www.nice.org.uk/guidance/ng33/evidence/appendix-f-full-he-report-pdf-80851860763>

It's possible that this will have some information for normal distributions: Bayesian sample size determination for costeffectiveness studies with censored data <https://sci-hub.ru/10.1371/journal.pone.0190422#savedHeader>

Beta and Gamma with point estimate and shape and scale: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\paper with shapes and scales\\12874_2017_437_MOESM1_ESM.pdf

This book explains some of the concepts you need to describe, like how to read a two-way sensitivity analysis graph: Muennig, P., & Bounthavong, M.
(2016).
Cost-effectiveness analysis in health: a practical approach.
John Wiley & Sons.
- saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\cost effectiveness a.pdf

There's some code for using the method of moments to generate parameters for drawing random samples here: <https://devinincerti.com/2018/02/10/psa.html> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\Probabilistic Sensitivity Analysis with R.pdf

formulas to calculate alpha and beta for gamma and beta distriobutions appear in the literature here: Briggs, Andrew H., et al. "Probabilistic analysis of cost-effectiveness models: choosing between treatment strategies for gastroesophageal reflux disease." Medical decision making 22.4 (2002): 290-308.
www.med.mcgill.ca/epidemiology/courses/EPIB654/Summer2010/EF/example%20PPI.pdf

This may provide some useful guides on the PSA <https://rdrr.io/github/DARTH-git/dampack/api/>

Go back to the resources and papers you reviewed for the deterministic sensitivity analysis.

In probabilistic sensitivity analysis, parameters were varied simultaneously according to their sampling distributions.
We used g distribution for cost parameters, b distribution for parameters bounded on the 0 to 1 interval (eg, AE incidence estimates and utilities), and the uniform distribution for the discount factor.
The range for the discount factor was 0.00 to 0.05.
In multivariate analysis, we ran 10,000 replications in the probabilistic sensitivity analysis.
The baseline values, ranges, and distributions for model parameters are presented in Table 4.
Per: Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colo .
<file:///C:/Users/jonathanbriody/Downloads/goldstein2014.pdf>

This has some information on distributions:

<https://cran.r-project.org/web/packages/dampack/vignettes/psa_generation.html>

Also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\sensitivity analysis help files\\Probabilistic Sensitivity Analysis\_ Generation.pdf

If we want to repeat all our one-way sensivity analysis probabilistically, rather than deterministically, you can see how to do this here, it might make for a good appendix because the description is technically complex, so would read well in the paper, but the application in R is actually very straightforward: <https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

**My writing on what a PSA is (then I'll describe each results section):**

To evaluate the robustness of the model, we conducted a probabilistic sensitivity analysis (PSA) to determine the consequences of parameter uncertainty [*in our model*]{.underline} for cost-effectiveness outcomes.
Simultaneous samples were drawn from model parameters in Monte Carlo simulations according to their sampling distributions, reflecting any uncertainty in parameter estimates in the evidence.
Thus, joint uncertainty was propagated through the model over 10,000 replications, providing a comprehensive assessment of model parameter uncertainty.
Confidence intervals or standard errors extracted from published sources were used for the probabilities, adverse events, costs and utilities.
Where this information was not available, model inputs were varied 20% above and below base case values per Flowers I think?.
The baseline values, ranges, and distributions for model parameters are reported in Table X.

For the hazard ratio we generate random draws from a log-normal distribution.
The value of ratio intervention effects have a lowest possible value of 0, a value of 1 corresponding to no intervention effect, and a highest value possible value of infinity.
Logically, a ratio that halves effects and one that doubles effects should average to no effect.
However, the average of 0.5 and 2 is 1.25, rather than 1.
Thus, a log transformation is applied to make the hazard ratio number scale symmetric: the log of 0 is minus infinity, the log of 1 is zero, and the log of infinity is infinity.
The log of 0.5 is --0.69 and the log of the OR of 2 is 0.69, such that when the halving and doubling ratios are averaged, a value of 0 is obtained, i.e., the log transformed value of 1, or no effect.

```{r}


### PROBABILISTIC ANALYSIS ----

# We conduct a probabilistic analysis (PSA) of the model to estimate the uncertainty in the model outcomes.

# To do this, we generate samples for each model parameter from parametric distributions and evaluate the model for each set of parameter samples.


## Sampling the parameter values ----

# In order for the results reported in the PSA to be reproducible we need to set the random number seed. We also defining the number of runs, i.e., how many times we will re-sample for the parameter distributions, typically the number chosen is 10,000 in published studies, so we do that too:
n_runs <- 10000
set.seed(123)

# In the PSA, first we sample the parameter values from their distributions and store them in a data.frame.
# If a model parameter has no uncertainty and, hence, is fixed, we can set it equal to itself.


# To take random draws for the Weibull parametric survival distribution that we fitted at the start of the document using the 'flexsurv' package, we use the following piece of code.

# This applies mvrnorm (i.e., takes samples from the specified multivariate normal distribution) to the mean (mu) of 

# l_TTP_SoC_weibull$coefficients
#     shape     scale 
# 0.7700246 1.7425343 

# i.e., to the mean of the shape and the scale variables from the Weibull parametric survival distribution that we fitted.

# And gives a sigma (a positive-definite symmetric matrix specifying the covariance matrix of the variables) that is equal to the covariance between the shape and the scale variables:

# > l_TTP_SoC_weibull$cov
#             shape       scale
# shape 0.001983267 0.000291494
# scale 0.000291494 0.000782051

# You'll see that it's covariance between the two variables because you can match shape with scale twice, and each time it's the same value, so the same covariance between the two variables.

# n is just the number of samples.

# Described here:
# rdocumentation.org/packages/MASS/versions/7.3-58.1/topics/mvrnorm

# So, correlated sets (a "set" here is like one random draw from the distribution, because the draw is for shape and scale at the same time) of coefficients for the survival distribution (i.e., correlated sets of shape and scale values for the survival distribution, which are the coefficients as above) are generated using the variance-covariance matrix that was obtained from fitting the weibull distribution (the cov in l_TTP_SoC_weibull is equal to the covariance between the shape and the scale variables and is printed as a 2 by 2 matrix above) and a multivariate normal distribution (this is just saying we are applying mvrnorm to take samples from the multivariate normal distribution for the mean (mu) values of shape and scale).

# Multivariate normal distribution described here too: https://devinincerti.com/2018/02/10/psa.html'


m_coef_weibull_SoC <- mvrnorm(
  n     = n_runs, 
  mu    = l_TTP_SoC_weibull$coefficients, 
  Sigma = l_TTP_SoC_weibull$cov
)

head(m_coef_weibull_SoC)

m_coef_weibull_OS_SoC <- mvrnorm(
  n     = n_runs, 
  mu    = l_TTD_SoC_weibull$coefficients, 
  Sigma = l_TTD_SoC_weibull$cov
)

head(m_coef_weibull_OS_SoC)



# Now that we have applied mvrnorm to get a random shape and a random scale we don't need to include HR_FP_SoC in our data.frame to create random probability draws for SoC, because we can just select the shape and scale from m_coef_weibull_SoC so that in our function the updated coef_weibull_shape_SoC and coef_weibull_scale_SoC are used to generate S_FP_SoC with a random value, which will in turn generate transition probabilities under standard of care with a random value.










 # According to:
#
# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models\markov_smoking_probabilistic.R
# 
# 
# It is OK to do the following:
# 
# # QALY associated with 1-year in the smoking state is Normal(mean = 0_95, SD = 0_01)
# # Divide by 2 as cycle length is 6 months
# state_qalys[, "Smoking"] <- rnorm(n_samples, mean = 0.95, sd = 0.01) / 2
#   
# So, when doing probabilistic sensitivity analysis and I need my mean and sd for the method of moments this may be useful information when drawing these from population norms.
# This will also be useful for utility and PSA in the adverse event setting.


# I create the SD to include in the sensitivity analysis applied to the hazard ratio below, as I need to create it outside the data.frame for the PSA:

UpperCI <- 0.89
LowerCI <- 0.54
SD <- (log(UpperCI) - log(LowerCI)) / 3.92

MEAN_HR_FP_Exp <- HR_FP_Exp
# I copy the hazard ratio above in case the below would start building HR_FP_Exp from the updated HR_FP_Exp made from random draws, i.e., in case it started including the HR_FP_Exp updated from the random draws from HR_FP_Exp in meanlog = (HR_FP_Exp), when we really need to be drawing the variability from the hazard ratio we started out with, and not a hazard ratio that we started out with, but that has since already been varied and will be varied from again if we take a random draw from this pre-varied hazard ratio.

# HR_FP_Exp = rlnorm(n_runs, meanlog = log(HR_FP_Exp), sdlog = SD)

# !!! I MAY NEED TO REPEAT THIS FOR: HR_PD_Exp

# I create the SD to include in the sensitivity analysis applied to the hazard ratio below, as I need to create it outside the data.frame for the PSA:

OS_UpperCI <- 0.86
OS_LowerCI <- 0.49
OS_SD <- (log(OS_UpperCI) - log(OS_LowerCI)) / 3.92

MEAN_HR_PD_Exp <- HR_PD_Exp
# I copy the hazard ratio above in case the below would start building HR_FP_Exp from the updated HR_FP_Exp made from random draws, i.e., in case it started including the HR_FP_Exp updated from the random draws from HR_FP_Exp in meanlog = (HR_FP_Exp), when we really need to be drawing the variability from the hazard ratio we started out with, and not a hazard ratio that we started out with, but that has since already been varied and will be varied from again if we take a random draw from this pre-varied hazard ratio.




# Below I create the data.frame that I will use in the PSA.
# So, this is the PSA input dataset.

df_PA_input <- data.frame(
  coef_weibull_shape_SoC = m_coef_weibull_SoC[ , "shape"],
  coef_weibull_scale_SoC = m_coef_weibull_SoC[ , "scale"],
  coef_TTD_weibull_shape_SoC = m_coef_weibull_OS_SoC[ , "shape"],
  coef_TTD_weibull_scale_SoC = m_coef_weibull_OS_SoC[ , "scale"],
             

  
### Hazard Ratios  
  
# They use exp for rnorm (i.e., to make random draws from a normal distribution) I to use want rlnorm.

#  HR_FP_Exp = exp(rnorm(n_runs, log(0.6), 0.08))

# The rlnorm() function in R is used for generating random draws from a log-normal distribution.
 
# To make draws from the log-normal distribution you need to enter a hazard ratio and standard deviation.

# If you find a hazard ratio and confidence interval in the literature, rather than a hazard ratio and a standard deviation, you can make conversions to a standard deviation.

# To generate random draws for the hazard ratio, I need a mean for the hazard ratio(just the hazard ratio value itself), and a standard deviation built from the 95% confidence interval of the hazard ratio (the SD is built from log(Upper CI) - log(Lower CI)/2*SE, so sd is already built from logs for inclusion in rlnorm and doesnt need to be set as sdlog = log() unlike meanlog = log() where you need to take the log of the hazard ratio value). 


# SD: (natural log(Upper confidence interval) -  natural log(lower confidence interval) / 2*Standard error (i.e. 1.95*2 = 3.92 for a 95% confidence interval)


# So, it looks like I take the natural log of the upper limit minus the natural log of the lower limit (in confidence intervals the lower limit is reported on the left and the upper limit is reported on the right, so it would be 95% CI (30.0 [LOWER LIMIT], 34.2[UPPER LIMIT]), and I would rearrange these to have [ln(UPPER LIMIT) - ln(LOWER LIMIT)], i.e., [ln(34.2)-ln(30.0)]) and divide by 2 times the standard error. Provided the sample size is large, then for the 95% confidence interval this would be 2 x 1.96 = 3.92 For 90% confidence intervals 3.92 should be replaced by 3.29, and for 99% confidence intervals it should be replaced by 5.15. 

# I often come across hazard ratios and their confidence intervals in the published literature on clinical trials, but rarely do I see standard deviations. 
 
# A typical example from the literature is the following: "HR, 0.69; 95% CI, 0.54 to 0.89 in mCRC for cetuximab plus FOLFOX-4 vs FOLFOX-4 alone" -  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7044820/pdf/bmjopen-2019-030738.pdf:
# 
# To compute the standard deviation for hazard ratios from the above typical reports in the literature, so that I can then generate random draws from a log-normal distribution for a hazard ratio I would do the following:
# 
# (natural log(Upper confidence interval) -  natural log(lower confidence interval) / 2*Standard error (i.e. 1.95*2 = 3.92 for a 95% confidence interval)
# 
# or in R: (log(0.89) - log(0.54)) / 3.92 = 0.1274623
# 
# 
# UpperCI <- 0.89
# LowerCI <- 0.54
# SD <- (log(UpperCI) - log(LowerCI)) / 3.92
# SD = 0.1274623
# 

# If I need to provide a citation for this formula I can use the below which, although this does not provide the formula I use, it provides enough information that is similar to the above, and which I could say ultimately informed the formula:
# Higgins, J. P. T., & Deeks, J. J. (2011). Chapter 7.7. 3.3: Obtaining standard deviations from standard errors, confidence intervals, t values and P values for differences in means. Cochrane handbook for systematic reviews of interventions, Version, 5(0).
# 
# 
# This approach is also supported by the following post, which provides good clarity on all of the above:
# 
# https://stats.stackexchange.com/questions/546192/calculate-the-standard-deviation-from-a-hazard-ratios-confidence-interval


# random draws from a log-normal distribution

# hr_draws <- rlnorm(nsims, meanlog = log(mean), sdlog = SD). 

# 
# We take random draws from the log-normal distribution for ratios, because if we were to take random draws from the ratio as it stands, because the hazard ratio can't go any lower than 0 but can go to plus infinity, our random draws would be skewed in the values we take from the distribution, on the other hand, if we were to put things on the natural log scale we would be taking our random draws from a more normalised distribution. So, you'll see that in:
# 
# rlnorm(n_sim, meanlog = log(mean),  sdlog = SD)
# 
# it's the log of the mean (the ratio value, so the hazard ratio) and the log of the standard deviation (which we put on the log scale when we calculate it so we don't need to put in as sdlog = log() here, just sdlog = ) in order to be taking random draws from a log-normal distribution (i.e. a log normally distributed HAZARD RATIO).

HR_FP_Exp = rlnorm(n_runs, meanlog = log(MEAN_HR_FP_Exp), sdlog = SD),
HR_PD_Exp = rlnorm(n_runs, meanlog = log(MEAN_HR_PD_Exp), sdlog = OS_SD),


# Per [SLIDE 16] onwards in C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\notes.txt I don't think we need to worry about the duration of time a hazard ratio refers to. In my model I definitely don't, as the hazard ratio and underlying rate survival curves that make up SoC transition probabilities come from the same paper, and thus the same time period. 























#### To generate random draws for the probabilities:

# IMPORTANT, THIS VALUE IS BOUNDED BY 1 OR 0:
# When calculating se from point estimates, remember that utility and probability values cannot be less than 0 or greater than 1, so don't calculate a min or max that is less than 0 or greater than 1. If you do, then change it to be 0 or 1, as appropriate, i.e., rather than 1.02, make it 1.


# When, as in slide 12 of C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\a212ProbabilisticDistributions-210604-154.pdf you can have many transitions from one state into others [i.e. in A you can stay in State A or you can go into State B, State C or State D rather than just going from state C into State D] it's not clear how you can do this in the beta code.

# i.e., when alpha values are all the times things happened, while  beta values are all the times they didnt, how do you apply 

# transistion_probabiliy <- rbeta(n_runs,alpha,beta)
# when there are so many things that can happen?

# FOR NON-MULTI STATE, I.E. FOR THE SICK TO DEAD STATE IN THE MARKOV MODEL YOU CAN APPLY THE BETA DISTRIBUTION, BECAUSE YOU CAN ONLY LEAVE (ALPHA) OR STAY (BETA).

# Sometimes a Dirichlet is suggested, a Dirichlet distribution is the multidimensional generalization of the beta distribution, so it's the same as applying the beta distribution, - as per: https://www.rdocumentation.org/packages/rBeta2009/versions/1.0/topics/rdirichlet but I would need to have all the counts included I think, which works when you are drawing all your transition data from one larger dataset, like in a clinical trial, but my data necessarily comes from several sources, so I if I just build everything as conditional probabilities, per my notes in this Notepad [search 19/08/22: in C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\notes.txt] in combination with the slides [C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\a212ProbabilisticDistributions-210604-154.pdf] these are notes on, that is 1-Probabilities like I've done in R already, then I should be able to apply a Beta distribution to each probability and not have to worry about applying a Dirichlet.

# Here's a quote explaining how best to do that from my notes above:

# "In the above you would have a beta distribution with parameters 731, 527 [the 512+15] to look at whether you stayed in State B and then, conditional on leaving State B, we could assign another beta distribution with parameters 512 and 15 to whether you, conditional on having left State B then transited to State C. And, of course, death is then the third category. Death is the residual probability. [i.e., conditional on having left state B is 1-the probability of staying in State B, so you could apply a Beta distribution to the probability of going into State C multiplied by 1-the probability of staying in State B. Death would be 1-the probability you create of moving into State C, because if you're not going into State C then you're going into Death as there's nowhere else for you to go, I think if you wanted to apply a Beta distribution to Death you could apply a Beta distribution to the probability of going into Death and then multiply that by 1-the probability of staying in the B state]."

# Conditional probabilities are built from the below p_'s in the function, so you basically create the beta distribution probabilities below as p_'s, and then in the function you multiply them by 1-whatever probability as necessary such that they become conditional probabilities.

##1

# Fitting Beta distributions to (constant) probability parameters [I guess the "constant" here is to clarify that this isnt how you do it for time varying transition probabilities, like the transition probabilities I create from Weibull].

# There are two main ways to do this, per: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.3 Practical exercise R\A231_Making_Models_Proba.pdf say you had something like the following printed in a study:

# ‘The hospital records of a sample of 100 patients receiving a primary THR were examined retrospectively. Of these patients, two patients died either during or immediately following the procedure. The operative mortality for the procedure is therefore estimated to be 2%.’

# You could use this information directly to specify the parameters (alpha and beta) of a beta distribution for the probability of operative mortality during primary surgery. These are noted by “a.” and “b.” respectively. Your alpha values are all the times things happened, while your beta values are all the times they didnt, so 2 and 98.

# alpha <- 2 ## alpha value for operative mortality from primary surgery
# beta <- 100- alpha ## beta value for operative mortality from primary surgery

# tp.PTHR2dead <- rbeta(n_runs,alpha,beta) ## Operative mortality rate  (OMR) following primary THR

# (as coded in C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.3 Practical exercise R\A233_Making_Models_Probabilistic_Solutions-210604 (1))

# The question is, does this give the transition probability for the time period that the quote describes? Well in the same pdf and r code as above you have the following:

# "The following information has been provided to you concerning revision procedures among patients already having had a revision operation (i.e. the re-revision risk). ‘The hospital records of a sample of 100 patients having experienced a revision procedure to replace a failed primary THR were reviewed at one year. During this time, four patients had undergone a further revision procedure.’iv) Use this information to fit a constant transition probability for the annual re-revision rate parameter (tp.rrr), using the same approach as for operative mortality above. Your beta value for re-revision risk should be equivalent to 96."


# a.rrr <- 4   ## alpha value for re-revision risk
# b.rrr <- 100-a.rrr  ## beta value for re-revision risk
# tp.rrr <-rbeta(1,a.rrr,b.rrr) ## Re-revision risk transition probability

# tp.PTHR2dead
# tp.RTHR2dead
# tp.rrr


# Because the cycle length in the model is one year according to page 3 of C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A1_Advanced Markov Modelling\A1.3 Practical exercise R\A131_Advanced_Markov_Modelling_Instructions-210528-18 this indicates that it does.


##2
                          # METHOD OF MOMENTS:


# In cases where you don't have this information, but you do have a mean and a standard error, you can do the following (per: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\notes.txt):

# My initial reading for probabilistic analysis with probabilities suggested that the method of moments can be applied to a mean and standard error for a transition probability sourced from the literature to create an appropriate alpha and beta to use in a probabilistic sensitivity analysis:

# So, I built the following code to translate the formula for the methods of moments into R, to allow us to take our transition probabilities from the literature:
# 
# 
#  ## Methods of moments
# 
# 
# #Assume the below mean and std error come from the reported literature
# 
# mean <- 0.75 ## mean from the literature
# std.error <- 0.04 ## standard error from the literature
# 
# # The method of moments can be coded up as below to get the alpha and beta from the sample using the mean and standard error from the sample:
# 
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# 
# # Following this, you can randomly draw from the beta distribution using the below, I only have one random draw, but you could change that 1 to 10,000 (the standard number of draws) and save these as a matrix of draws to use in your probabilistic sensitivity analysis:
# 
# probabilistic.beta.dist.draw <- rbeta(1,alpha,beta) ## drawing from the Beta distribution based on a and b

# Per the above pdf:

# "Note that the rbeta function takes 3 arguments, first the number of draws or samples, the alpha parameter (named shape1) and the beta parameter (named shape2)." 
# "rbeta(n = 5, shape1 = 50, shape2 = 100)"

# while in their R code they use the below, as included above, so I don't think it matters if you include shape1= or just include the alpha and beta directly, but I'll keep it as below, as that's how DARTH do it. 

# tp.rrr <-rbeta(1,a.rrr,b.rrr) ## Re-revision risk transition probability

# The below gives the same results either way:

# set.seed(100)
# n = 1
# b4 <- rbeta(n, shape1 = 2, shape2 = 8)
# b4
# [1] 0.1329533
# 
# set.seed(100)
# n = 1
# b4 <- rbeta(n, 2, 8)
# b4
# [1] 0.1329533

## 3

# RANGE:

# I don't describe generating random draws when the probabilities don't have an SE above, however, I can do this in the same way that I figured out how to do this for the utility beta distributions below. If they are reported with a range, he section on utility below also advises how to handle this.

  p_FD      = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_PD      = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_A1F_SoC = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_A1D_SoC = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_A2F_SoC = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_A2D_SoC = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_A3F_SoC = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_A3D_SoC = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_FD_SoC  = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_FD_Exp  = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_PFSD_SoC = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_PFSD_Exp = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_FA1_SoC = rbeta(n_runs, shape1 = 16, shape2 = 767),
  p_FA2_SoC = rbeta(n_runs, shape1 = 22.4, shape2 = 201.6),
  p_FA3_SoC = rbeta(n_runs, shape1 = 16, shape2 = 767),


# If I was going with my initial plan of applying the PSA to the original COLOSSUS limited sample size, I could say, the uncertainty in this limited sample size is propogated through the model by applying a dirichlet and beta distribution to a fully probabilistic analysis of the data.


# Calculate the mean, maximum and variance of the Beta and Gamma here: https://www.pluralsight.com/guides/beta-and-gamma-function-implementation-in-r also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\R Guide_ Beta and Gamma Function Implementation _ Pluralsight.pdf




  # Cost vectors with length n_runs


##  Costs


# 1) COSTS WITH A RANGE:



# Maximum <- SOMEVALUEHERE
# Mean <- SOMEVALUEHERE

# se <- ((Maximum) - (Mean)) / 2
# se                                  
 
# mean <-Mean
# mean
 
# mn.cIntervention <- mean ## mean cost of intervention
# se.cIntervention <- se ## standard error of cost of intervention
 
# a.cIntervention <- (mn.cIntervention/se.cIntervention)^2 ## alpha value for cost of intervention (shape)
# b.cIntervention <- (se.cIntervention^2)/mn.cIntervention ## beta value for cost of intervention (scale)
 
# a.cIntervention
# b.cIntervention
 
 
# c_H   = rgamma(n_sim, shape = a.cIntervention, scale = b.cIntervention),  # cost of intervention
 
# If costs are reported with a 95% confidence interval the maximum is the upper confidence interval. If the interval is not centered on the mean, use the maximum or minimum in place of the upper confidence interval, depending on which one is further away from the mean SE = (m-CIlower)/t, here the bit in the brackets is rearranged when dealing with lower limits to have the MEAN minus the lower confidence interval so that things don't become negative (because the lower value will of course always be lower than the mean, as it's supposed to be below the mean, so a smaller number minus a larger number will be negative), but when the interval is centered on the mean the formula of SE = (CIupper-m)/t and SE = (m-CIlower)/t are inherently doing the same thing, that is getting the difference between the mean and the confidence interval and dividing this by t. So both formulas are options to use in getting the SE, and an author may choose to use the one which incorporates the interval that is farthest from the mean in order to incorporate as much variability in doing this SE calculation as possible, i.e. to reflect the variability in their SE calculation.

# 
# As per: https://stats.stackexchange.com/questions/550293/how-to-calculate-standard-error-given-mean-and-confidence-interval-for-a-gamma-d/550892#550892
 
 
# "I would like to note that, while those values in the table happen to correspond with ±2σ, the minimum and maximum values do not generally follow such simple formula with mean plus-minus some standard deviation.
 
# In this case, the minimum and maximum values only correspond to the interval μ±2σ because the distribution seems to have been truncated at those values."
 
 
# i.e., my formula will only work if the minimum and maximum values reported in a study are ±2 SE from the mean, i.e. + 2 SE from the mean for the maximum value, and -2 SE from the mean for the minimum value.
 
# And that will be the case any time there is a 95% Confidence Interval:
 

# [ "Since 95% of values fall within two standard deviations of the mean according to the 68-95-99.7 Rule, simply add and subtract two standard deviations from the mean in order to obtain the 95% confidence interval. Notice that with higher confidence levels the confidence interval gets large so there is less precision." https://www.westga.edu/academics/research/vrc/assets/docs/confidence_intervals_notes.pdf also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\gammadist\confidence_intervals_notes.pdf

# A good image can be found as per: https://www.quora.com/What-is-meant-by-one-standard-deviation-away-from-the-mean also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\gammadist\www-quora-com-What-is-meant-by-.pdf]

# So, if I'm taking my costs and a range about these costs from the literature, I just need to make sure the range is a 95% Confidence Interval, like that reported here: Tilson, L., Sharp, L., Usher, C., Walsh, C., Whyte, S., O’Ceilleachair, A., ... & Barry, M. (2012). Cost of care for colorectal cancer in Ireland: a health care payer perspective. The European journal of health economics, 13(4), 511-524. (https://www.researchgate.net/publication/51188456_Cost_of_care_for_colorectal_cancer_in_Ireland_A_health_care_payer_perspective) reports a cost in Table 3 for Colorectal cancer with a 95% CI: € 48,835 (€40,548–€62,582)..

# For "PK test cost MEAN: 400.00 MIN: 300.00 MAX: 500.00" from Table 4, row 3 page 222 in Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R. (2014). Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer. Clinical colorectal cancer, 13(4), 219-225 plugged into the formula above to find the SE, I get an SE of 50, so plugging the 50 and mean into the CI = m ± t*SE from Jochen above I get:
 
# 400 + 2*50 = 500
 
# 400 - 2*50 = 300
 
# Which is the same confidence interval as I got in the Table 4 row 3, i.e. 300 and 500.
 
# So, this might be a good way to check if my standard error is correct. Although this will only work if my confidence interval is centred on the mean, i.e. if I am the same distance from the mean to the max and to the min. If my confidence interval is NOT centred on the mean, I can calculate 2 standard errors, one between the lower interval and the mean, and one between the upper interval and the mean, and then apply these in CI = m ± t*SE, knowing that:

# CIupper = m + t*SE

# CIlower = m - t*SE

# I've figured out a simple test for whether the max/min reported is ±2σ:
# se <- ((Max) - (Mean)) / 2
# MaxMatch <- Mean + 2*se 
# MinMatch <- Mean - 2*se
 
# I can then check if the max/min reported in a Table match the above, and if so I know that the max/min reported is ±2σ.
 
# If not, it may be that they used the se off the minimum as it's further away, so try the below:
 
# se <- ((Mean) - (Min)) / 2
# MaxMatch <- Mean + 2*se 
# MinMatch <- Mean - 2*se
 
 
# I can then check if the max/min reported in a Table match the above, and if so I know that the max/min reported is ±2σ.
 
 
 
 
# Even if it's not symmetric, this may still be a 95% confidence interval if the mean is at 2 of one of the standard errors from the min or the max, we can see this in action below:
 
 
# > Maximum <- 27655.84
# > Mean <- 11565.60
# > 
# > se <- ((Maximum) - (Mean)) / 2
# > se                                  
# [1] 8045.12
# > 
# > MaxMatch <- Mean + 2*se 
# > MinMatch <- Mean - 2*se
# > MaxMatch
# [1] 27655.84
# > MinMatch
# [1] -4524.64
 
# But still, for these values
 
# FN: Value: 11,565.60 Minimum: 7092.16 Maximum: 27,655.84  Distribution: γ: (2.067, 5596.247)
 
# that are not centered on the mean, this still came out the same as the Table using this se <- ((Maximum) - (Mean)) / 2 method. Because looking at the below, the upper interval (the maximum) is further from the mean at a difference of 16090.24, than the lower interval (the minimum) at a difference between it and the mean of 4473.44 (i.e. the maximum is nearly four times further away from the mean that the minimum).
 
 
# > Maximum <- 27655.84
# > Minimum <- 7092.16
# > Mean <-  11565.60
  
# > Maximum - Mean
# [1] 16090.24
  
# > Mean - Minimum
# [1] 4473.44
 
 
# And when we plug the se in for the further away max we get:
 
 
# > a.cIntervention
# [1] 2.066671
# > b.cIntervention
# [1] 5596.247
 
# Just the same as the values for FN above.
# 

# So, this kind of applies to situations where an interval is reported, when you might want to check if it's a 95% confidence interval, and if not, which interval is furthest from the mean, in cases of a point estimate only, you do the following:




# 2) POINT ESTIMATE ONLY:

# If they arent reported with a 95% confidence interval, the Goldstein paper says: "Drug costs were varied within +/-20% of their baseline values as previously done by Goulart and Ramsey".
 
# I checked this for FOLFOX drug cost Min: 443.91 Value: 355.13 Max: 532.69  gamma(100, 4.439)
 
# Mean<-   443.91
# Maximum <- Mean + 0.20*Mean
# Minimum <- Mean - 0.20*Mean
 
# Taking 20% plus the mean and 20% minus the mean, I get the same maximum and minimum as above, exactly:
 
# > Maximum
# [1] 532.692
# > Minimum
# [1] 355.128
 
# They still applied the SE = (CIupper-m)/t or SE = (m-CIlower)/t to this interval which they generated from 20% plus the mean and 20% minus the mean.

# In the Goldstein 2014 paper, where the ranges are more than 20% away from the mean, i.e., 25%, etc., they use the same formula for ranges that are symmetrically far from the mean, i.e., 25% bigger or smaller than the mean, and they use the same formula but incoporating the range that's furthest from the mean when generating the se when the ranges arent symmetric about the mean.





# OK, so the question is, can we apply any percentage we like to the point estimate we find for costs to generate the max or min and then just apply the formula above to generate the SE? I reviewed Koen's study below:

# Degeling, K., Franken, M. D., May, A. M., van Oijen, M. G., Koopman, M., Punt, C. J., ... & Koffijberg, H. (2018). Matching the model with the evidence: comparing discrete event simulation and state-transition modeling for time-to-event predictions in a cost-effectiveness analysis of treatment in metastatic colorectal cancer patients. Cancer epidemiology, 57, 60-67.

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Matching the model with the evidence Koen

# In the excel file, on the second sheet there is a row called parameter 1. For cost this parameter is the alpha value, and when you click on 16.000 it shows the formula that created this alpha value, which is the same as for my alpha value formula above. This formula includes the mean and the SE. When I take the mean (which is just the baseline value) and plug this into the R code to generate the SE:

# I get the same se as in the excel file when making the max and min 50% larger or smaller than the mean in the code below. 
# Mean<-   2062.35
# Maximum <- Mean + 0.50*Mean
# Minimum <- Mean - 0.50*Mean
# se <- ((Maximum) - (Mean)) / 2
# se
# 515.5875

# Even more important, they apply the same code as I have for creating the alpha value above using this mean and SE generated just from taking 50% either side of the mean. 

# The implication of this is that you can generate the SE of a point estimate by taking any percentage either side of the mean and that you can also apply the code as above to generate the alpha and beta. Unfortunately in the excel file they don't create the beta in parameter 2, but the lambda, which is something else again. 


# This may solve the riddle of lambda in Koen's code, I noticed that here they use 1/Beta: https://www.pluralsight.com/guides/beta-and-gamma-function-implementation-in-r so I took that approach with the Lambda in Koen's code, plugging it in for beta, and got the same value of the mean as I started out with.

# Mean<-   2062.35
# Maximum <- Mean + 0.50*Mean
# Minimum <- Mean - 0.50*Mean
# se <- ((Maximum) - (Mean)) / 2

# se
# Maximum
# Minimum
# mean <-Mean

# mn.cIntervention <- mean ## mean cost of intervention
# se.cIntervention <- se ## standard error of cost of intervention
# a.cIntervention <- (mn.cIntervention/se.cIntervention)^2 ## alpha value for cost of intervention (shape)
# b.cIntervention <- (mn.cIntervention)/((se.cIntervention)^2) ## beta value for cost of intervention (scale) using Koen's backwards version of Andy's (se.cIntervention^2)/mn.cIntervention to generate the Lambda code.

# a.cIntervention
# b.cIntervention
# se

# c_H   = rgamma(10000, shape = a.cIntervention, scale = 1/b.cIntervention) # cost of intervention
# mean(c_H)
 

# Likewise, if I do the following:

# 1/ b.cIntervention
# I get the exact same value for b.cIntervention as when I use the original code to generate b.cIntervention from Andy, as below.

# b.cIntervention <- (se.cIntervention^2)/mn.cIntervention ## beta value for cost of intervention (scale)
# b.cIntervention

# So, it looks like the whole Lambda thing is just a different way of getting b.cIntervention, but Koen's way of doing it requires the addition of 1/ for the scale part of rgamma, while Andy's method doesnt.

# So, in this it looks like Andy is generating the scale for the beta, while Koen is generating the rate: 

# This is supported by this: 

# "Density, distribution function, quantile function and random generation for the Gamma distribution with parameters alpha (or shape) and beta (or scale or 1/rate)." https://search.r-project.org/CRAN/refmans/Rlab/html/Gamma.html 


# "There is a R function for simulating this random variable. Here in addition to the number of values to simulate, we just need two parameters, one for the shape and one for either the rate or the scale. The rate is the inverse of the scale. The general formula is: rgamma(n, shape, rate = 1, scale = 1/rate)." https://pubs.wsb.wisc.edu/academics/analytics-using-r-2019/gamma-variables-optional.html also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\13.7 Gamma Variables (Optional) _ Analytics Using R.pdf

# I think the code above also implies that if you were to include it as a "rate" instead of calling it scale in the rcode for Koen's rate, then you wouldnt need to do the 1/ bit over scale. So, he may have taken this approach as well.This is supported by: "rate:	a numeric vector in the range [0, Inf) that specifies the inverse of the scale parameter." https://docs.tibco.com/pub/enterprise-runtime-for-R/4.1.1/doc/html/Language_Reference/stats/GammaDist.html

# This is supported by the below:

# > set.seed(1)
# > x <- rgamma(n = 1000, shape = 5, scale = 3)
# > mean(x)
# [1] 14.61593
# Knowing that the rate is the inverse of the scale, we can get the rate as 1/3 below:
# > 1/3
# [1] 0.3333333
# > set.seed(1)
# > x <- rgamma(n = 1000, shape = 5, rate = 0.3333333)
# > mean(x)
# [1] 14.61593
# > set.seed(1)
# > x <- rgamma(n = 1000, shape = 5, scale = 1/0.3333333)
# > mean(x)
# [1] 14.61593

# Thus, Koen generated the rate and used that in rgamma, while Andy showed me how to use scale and use that in rgamma. 


# This 20% approach is also taken by Lesley tilson and cited in her study and other studies here: https://trello.com/c/m653JBWV/48-20-cost-sensitivity


  c_F_SoC   = rgamma(n_runs, shape = 16, scale = 25), 
  c_F_Exp   = rgamma(n_runs, shape = 16, scale = 50), 
  c_P       = rgamma(n_runs, shape = 100, scale = 10), 
  c_D       = 0,
# The cost of being dead doesnt vary, it always costs the same to treat a dead person, which is 0 because we don't give any of our chemo, etc., to a dead body.
  c_AE1 = rgamma(n_runs, shape = 16, scale = 25), 
  c_AE2 = rgamma(n_runs, shape = 16, scale = 50),
  c_AE3 = rgamma(n_runs, shape = 100, scale = 10),


  # Utility vectors with length n_runs 

# There may be helpful books here: C:\Users\Jonathan\Dropbox\PhD\HTA\Markov Modelling\books



##### POINT ESTIMATES ONLY:


# IMPORTANT, THIS VALUE IS BOUNDED BY 1 OR 0:
# When calculating se from point estimates, remember that utility and probability values cannot be less than 0 or greater than 1, so don't calculate a min or max that is less than 0 or greater than 1. If you do, just round it to 0 or 1.


# If you have a point estimate, then you can generate the standard error/standard deviation using the method of moments again.

#Per Table 4 in the Goldstein paper:
# > mean<-   0.850
# > Maximum <- mean + 0.20*mean
# > Maximum
# [1] 1.02
# > Minimum <- mean - 0.20*mean
# > Minimum
# [1] 0.68
# > se <- ((Maximum) - (mean)) / 2
# > se  
# [1] 0.085
# > std.error <- se
# > alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# > alpha <- mean*alpha.plus.beta ## alpha (a)
# > beta <- alpha*(1-mean)/mean ## beta(b)
# > alpha
# [1] 14.15
# > beta
# [1] 2.497059
# So, a perfect match to the Goldstein paper.
# Because this approach to point estimates ends up making the same range around this point estimate as reported in Table 4, the implication is that when a point estimate is reported with a range, you can generate the SE the exact same way as above, that is, se <- ((Maximum) - (mean)) / 2, because we are applying the same manner of generating the SE to the range that was reported in the paper, as to the point estimate.


# I double check the manner in which I calculate the SE with the excel file from Koen's paper: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Matching the model with the evidence Koen both are saved here, but the filepath may be too long to access them without moving them to downloads or desktop.

# For his paper, I update the Maximum/Minimum part by taking the max as 50% bigger than the mean. When I do this, I get the exact same SE as he reports in the excel file, I know this because in his cells creating the alpha and beta values for utility or probability on sheet 2 of his excel file, he includes 2 numbers, one is the mean reported on sheet 1 for the parameter whose utility or probability he is taking random draws from, and then the other number is an EXACT match for the SE number I calculate, and must therefore be the SE number he calculates. Especially because he then applies a formula to this mean and SE to generate the alpha and the beta. You can see this in my model code below, where the mean and SE I report in my own analysis turn up in his alpha and beta formulas.

# Although he uses a different method to calculate the alpha and the beta, these typically work out as very close to the values calculated for the alpha and beta using my method, which Briggs told me to use once I had the SE and mean, so I think Koen just has a deeper understanding of mathematics and is using this to apply a different formula for the generation of alpha and beta from the SE and mean.

# The takeaway here is that my method for calculating SE is correct, as proven by Koen's excel file, and once I have the SE and the mean (mean is just the point estimate value I started with for this parameter) Andrew Briggs told me what to do to calculate the alpha and the beta, so I now can be confident in the manner in which I take the PSA for utility or probability when it comes to beta distributions. 
 
# mean<-   0.042178846
# Maximum <- mean + 0.50*mean
# Maximum
# Minimum <- mean - 0.40*mean
# Minimum
# 
# se <- ((Maximum) - (mean)) / 2
# se  
# 
# std.error <- se
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# beta
# 
# 
# 
# 
# mean: 0.042178846
# se: 0.01054471
# 
# alpha =(((0.042178846)^2)*(1-(0.042178846))/((0.01)^2)-(0.042178846))
# beta =((1-(0.042178846))*(((1-(0.042178846))*(0.042178846))/((0.01)^2)-1))
#
# My alpha and beta:
#
# > alpha
# [1] 15.28296
# > beta
# [1] 347.0541
#
# Koen's alpha and beta:
# alpha = 14.961
# beta = 363.614

# mean<-   0.042178846

# u_ME       = rbeta(10000, shape1 =  15.28296, shape2 = 347.0541)
# mean(u_ME)
# [1] 0.04208022
# u_KOEN       = rbeta(10000, shape1 =  14.961, shape2 = 363.614)
# mean(u_KOEN)
# [1] 0.03940224
# Which will both come to 0.04 if rounding to 2 decimal points

# 
# mean= 0.042178846
# se= 0.01
#  
# newalpha =(((mean)^2)*(1-(mean))/((se)^2)-(mean))
# newbeta =((1-(mean))*(((1-(mean))*(mean))/((se)^2)-1))
# > newalpha
# [1] 16.99799
# > newbeta
# [1] 385.9999
# 
# 
# manualalpha =(((0.042178846)^2)*(1-(0.042178846))/((0.01)^2)-(0.042178846))
# manualbeta =((1-(0.042178846))*(((1-(0.042178846))*(0.042178846))/((0.01)^2)-1))
# > manualalpha
# [1] 16.99799
# > manualbeta
# [1] 385.9999

## RANGE:

##### Calculating Beta values when you have a range around your point estimate in the literature (THIS CAN BE APPLIED TO PROBABILITY OR UTILITY): 


# Here are my formulas, and if you read further below you can see how I got to these formulas.

# Here's the bottomline takeaway on the Beta formuala. My SE's are typically very close to the SE's that I recover from the published studies. I've seen first hand, that you need big changes to the SE to affect the mean you draw in the PSA. So, what I'll do is apply the briggse or altbriggse as appropriate, and then when I do the PSA I'll check the mean on the parameter drawn from the PSA draws to make sure the parameter is on average the same as the one we started with, I can do this by doing mean() whatever the parameter is, and that will show me the average value.

# Brigs code to generate the SE for a parameter with a range perfectly centered around the mean (briggsse), and then repeated but for the situations where the range isnt PERFECTLY centered around the mean (altbriggsse), i.e., the min is further away from the mean than the max, or vice versa (even a little):

# briggsse <- ((max)-(mean))/1.96
# altbriggsse <- (max-min)/(2*1.96)

## Generating the alpha and beta:

# mean <- Somevaluehere

# std.error <- briggsse OR altbriggsse - as appropriate
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# beta
# 
# u_ME       = rbeta(10000, shape1 =  a, shape2 = b)
# mean(u_ME)

# I could also create alpha and beta using Koen's code, which is a different way of calculating these vales than what Briggs showed me, but typically gives the same alpha and beta, so can be ignored:

# newalpha =(((mean)^2)*(1-(mean))/((se)^2)-(mean))
# newbeta =((1-(mean))*(((1-(mean))*(mean))/((se)^2)-1))
# newalpha
# newbeta


# Calculating a Beta distribution when you have a range around this in the literature you are reviewing:

# All of the below applies to probabilities as well as utilities, as both are for beta distributions.

# To determine the best way to do this, I open all the sources I mention just above which include a range, se, alpha, beta, etc., and then I apply all the methods I have and see which is best. I'll also be heavily guided by the suggestions Andy made, as he has being doing this work for 2 decades, if he says that a certain way is correct, I'll take him at his word. Lesley also made some suggestions.

# Once you have an alpha and a beta, you can work your way back to a SE:


# per: https://stackoverflow.com/questions/41189633/how-to-get-the-standard-deviation-from-the-fitted-distribution-in-scipy
# As supported by this: https://www.real-statistics.com/binomial-and-related-distributions/beta-distribution/
# and using this here: Jenks, Michelle, et al. "Tegaderm CHG IV securement dressing for central venous and arterial catheter insertion sites: a NICE medical technology guidance." Applied Health Economics and Health Policy 14.2 (2016): 135-149. https://link.springer.com/content/pdf/10.1007/s40258-015-0202-5.pdf I get 0.002096431 or 0.0021, so it looks like they used the mean as their se.


# I demonstrate this in Table 1, row 22,  of: Sharp, Linda, et al. "Cost-effectiveness of population-based screening for colorectal cancer: a comparison of guaiac-based faecal occult blood testing, faecal immunochemical testing and flexible sigmoidoscopy." British journal of cancer 106.5 (2012): 805-816. https://www.nature.com/articles/bjc2011580.pdf also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Betadist utility\bjc2011580.pdf

# CTC sensitivity for CRC Basecase(85%) range(75 – 95%) Beta (Alpha = 50.00, Beta = 8.82)

# a<- 50.00
# b<- 8.82
# Var <-  a * b / ( (a + b)^2 * (a + b + 1) )
# se <- sqrt(Var)
# se
# 0.04616056

# Here's a way to get alpha and beta with just the variance and mean: https://devinincerti.com/2018/02/10/psa.html

# If I wanted to turn my own SE into the variance, I can multiply it by itself, i.e. se*se per: https://r-lang.com/how-to-calculate-square-of-all-values-in-r-vector/#:~:text=To%20calculate%20square%20in%20R,square%20of%20the%20input%20value.

# mean <- 0.85

# std.error <- se
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# 50.01124
# beta
# 8.825513


# myse <- ((0.95) - (0.85)) / 2
# myse
# 0.05

###### Below, I work through the differences that my SE formula lead to, and come to the realisation that an alternative formula informed by Briggs may be better: ######

# The problem is, using this se changes my alpha and beta values, as follows:

# std.error <- myse
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# 42.5
# beta
# 7.5

# The differences between my results on their tables, and their results, may come down to rounding.

# When I generate the SE from the values reported in the paper, I don't have as many decimal points for my SE, i.e., it rounds up to 0.05 from 0.04616056, as below:

# myse <- ((0.95) - (0.85)) / 2
# myse
# 0.05

# The problem is, using this rounded se changes my alpha and beta values, as follows:

# std.error <- myse
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# 42.5
# beta
# 7.5

# Which explains the differences in the alpha and beta I generate from the table.

# What this means, is that I can generate my SE as before, that is 

# se <- ((Maximum) - (Mean)) / 2

# Or se off the minimum if it's further away:

# se <- ((Mean) - (Min)) / 2

# But where there is a range, instead of just a point estimate value for utility, I use the upper end of the range for the maximum, or the lower end of the range for the minimum when generating the SE.

# I've checked this for all the beta values in the Table. Only rows 1, 5, 20, 34 (out of the 17 beta rows) give results that are too different to be accounted for by rounding. To make sure I am 100% correct about this, I've emailed Lesley to ask how she got these distributions, as she is one of the co-authors on the paper. Lesley didnt do the analysis for this paper, and doesnt remember who did...

# When the SE is only different by rounding, the alpha and beta that comes out of the formula after I've worked out the SE for their beta vars from the alpha and beta and plugged it into the formula matches the alpha and beta they started with, showing that the way in which I generate the SE is correct, because my se matches the se they use as below:

# My standard error matches theirs (although involves rounding) and I get the same alpha and beta by including theirs or mine (or, at least I would without rounding) and gives me the same rbeta mean as they started out with. Subsequently, my thought process is this: I can take the approach of generating the SE from the upper range - mean or the mean - the lower range (depending on which is further away), and then plug this into my formula to get an appropriate alpha and beta and use this to make random draws for the mean in the rbeta. I won't produce an alpha and beta that matches what they present in the table, but the SE I recover from their alpha and beta, creates an unmatched version of the alpha and beta that matches the alpha and beta I create. 

# The fact that their standard error matches mine means that the way I am calculating SE in my own methods is correct.
# 
# std.error <- se
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# beta
# 
# u_ME       = rbeta(10000, shape1 =  a, shape2 = b)
# mean(u_ME)

# 
# std.error <- myse
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# beta
# 
# u_ME       = rbeta(10000, shape1 =  a, shape2 = b)
# mean(u_ME)


# Then I create alpha and beta using Koen's code, which is a different way of calculating these vales than what Briggs showed me:

# newalpha =(((mean)^2)*(1-(mean))/((se)^2)-(mean))
# newbeta =((1-(mean))*(((1-(mean))*(mean))/((se)^2)-1))
# newalpha
# [1] 50.01124
# newbeta
# [1] 8.825513

# Gianluca Baio emailed me this code for calculating alpha and beta, which matches the above:

# a <- m * ((m * (1 - m)/s^2) - 1)
# b <- (1 - m) * ((m * (1 - m)/s^2) - 1)

# According to Brett (McQueen, Robert <ROBERT.MCQUEEN@CUANSCHUTZ.EDU> in email): The beta distribution has two shape parameters calculated as alpha = ((mean^2)*(1-mean)/(error)^2)-mean and beta = alpha*((1-mean)/mean).

# Spackman, D. E., & Veenstra, D. L. (2008). A cost-effectiveness analysis of currently approved treatments for HBeAg-positive chronic hepatitis B. Pharmacoeconomics, 26(11), 937-949. http://download.lww.com/wolterskluwer_vitalstream_com/permalink/pcz/a/00019053-920082611-00002.pdf Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Betadist utility\00019053-920082611-00002.pdf



# In the supplementary table to this paper, my SE is almost identical to the SE created in the paper, but because I am working with whole numbers, i.e., I only know 2 decimals of their code, say 0.03, whereas they would know lots more, say 0.031112234331, so my SE has less decimal points than there's, so of course, there will be slight variations when that SE is used for calculating alpha.plus.beta, which in turn has implications for alpha and beta calculated alone from this alpha.plus.beta, i.e., my calculation of these will be very slightly different because I am dividing by a very slightly different SE to create alpha plus beta. What I do see when I plug their longer SE into my formula I get the exact alpha and beta reported in the paper. This means that my formula to calculate alpha and beta is correct. Also, my SE is the exact same as theirs bar a few extra digits in theirs, for reasons described above, meaning that the way I calculate the SE is also perfect. Thus I can use the method described below for calculating SE from a parameter with a range, and thus the alpha and beta from this parameter, and thus the PSA distribution.


# However, I do run into some cells where things are very different:

# a <- 15.039072
# b <-  736.9145
# max <- 0.03
# min <- 0.00
# mean <-0.02

# Var <- a*b/((a+b)^2*(a+b+1))
# se<-sqrt(Var)
# se
# [1] 0.005102

# myse <- ((max)-(mean))/2
# #myse <- ((mean)-(min))/2
# myse
# [1] 0.005

# std.error <- se
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# [1] 752

# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# [1] 15.04
# beta
# [1] 736.9

# u_ME       = rbeta(10000, shape1 =  alpha, shape2 = beta)
# mean(u_ME)
# [1] 0.01997

# std.error <- myse
# alpha.plus.beta <- mean*(1-mean)/(std.error^2)-1 ## alpha + beta (ab)
# alpha.plus.beta
# [1] 783
# alpha <- mean*alpha.plus.beta ## alpha (a)
# beta <- alpha*(1-mean)/mean ## beta(b)
# alpha
# [1] 15.66
# beta
# [1] 767.3

# u_ME       = rbeta(10000, shape1 =  alpha, shape2 = beta)
# mean(u_ME)
# [1] 0.02007


# I create a vector with 1 million entries drawing randomly from between a minimum and a maximum value for the mean and max values featured in the paper:


# possiblemeans <- runif(1000000, 0.012499999999999999, 0.0134999999999999999)
# possiblemaxs <- runif(1000000,  0.022499999999999999, 0.0234999999999999999)

# I create an SE that's made up of all these possible values

# mypossibleses <- ((max)-(mean))/2
# mypossibleses <- ((mean)-(min))/2
# mypossibleses

# The below tells me all the cells that have the included value :

# grep(se, x1)

# So, basically, what I am doing is taking all the possible numbers that could make up the max and the mean, putting them into my formula, and seeing if any permutation will give me the SE they report in their paper. If it does, this simply means that their SE and my SE differ because of the number of decimal places they have.

# a <- 8.16870263
# b <-  54.66747 
# max <- 0.23
# min <- 0.07
# mean <-0.13
# 
# Var <- a*b/((a+b)^2*(a+b+1))
# se<-sqrt(Var)
# se
# [1] 0.04209
# 
# myse <- ((max)-(mean))/2
# #myse <- ((mean)-(min))/2
# myse
# [1] 0.05


# I create a vector with 1 million entries drawing randomly from between a minimum and a maximum valuefor the mean and max values featured in the paper:


# possiblemeans <- runif(1000000, 0.12499999999999999, 0.134999999999999999)
# possiblemaxs <- runif(1000000,  0.22499999999999999, 0.234999999999999999)
# possiblemins <- runif(1000000,  0.06499999999999999, 0.074999999999999999)

# I create an SE that's made up of all these possible values

# mypossibleses <- ((possiblemaxs)-(possiblemeans))/2
# mypossibleses <- ((possiblemeans)-(possiblemins))/2
# mypossibleses

# I create a data frame from this and then I control f to search for the se they created according to their alpha and beta:
# se
# mypossiblesesdf <- data.frame(mypossibleses)


# I don't find it among my values.

# This tells me that no matter what number of decimal places they had information on, I would never be able to generate the SE they generated from the values I am using. When I sort by the size of my data.frame the minimum value I have is 0.04501 and the maximum is 0.05499

# I even tred dividing by all the t-quantiles:

# tq <- c(1,1.376,1.963,3.078,6.31,12.71,31.82,63.66,106.1,159.2,0.816,1.061,1.386,1.886,2.92,4.303,6.965,9.925,12.85,15.76,0.765,0.978,1.25,1.638,2.353,3.182,4.541,5.841,6.994,8.053,0.741,0.941,1.19,1.533,2.132,2.776,3.747,4.604,5.321,5.951,0.727,0.92,1.156,1.476,2.015,2.571,3.365,4.032,4.57,5.03,0.718,0.906,1.134,1.44,1.943,2.447,3.143,3.707,4.152,4.524,0.711,0.896,1.119,1.415,1.895,2.365,2.998,3.499,3.887,4.207,0.706,0.889,1.108,1.397,1.86,2.306,2.896,3.355,3.705,3.991,0.703,0.883,1.1,1.383,1.833,2.262,2.821,3.25,3.573,3.835,0.7,0.879,1.093,1.372,1.812,2.228,2.764,3.169,3.472,3.716,0.697,0.876,1.088,1.363,1.796,2.201,2.718,3.106,3.393,3.624,0.695,0.873,1.083,1.356,1.782,2.179,2.681,3.055,3.33,3.55,0.694,0.87,1.079,1.35,1.771,2.16,2.65,3.012,3.278,3.489,0.692,0.868,1.076,1.345,1.761,2.145,2.624,2.977,3.234,3.438,0.691,0.866,1.074,1.341,1.753,2.131,2.602,2.947,3.197,3.395,0.69,0.865,1.071,1.337,1.746,2.12,2.583,2.921,3.165,3.358,0.689,0.863,1.069,1.333,1.74,2.11,2.567,2.898,3.138,3.326,0.688,0.862,1.067,1.33,1.734,2.101,2.552,2.878,3.113,3.298,0.688,0.861,1.066,1.328,1.729,2.093,2.539,2.861,3.092,3.273,0.687,0.86,1.064,1.325,1.725,2.086,2.528,2.845,3.073,3.251,0.686,0.859,1.063,1.323,1.721,2.08,2.518,2.831,3.056,3.231,0.686,0.858,1.061,1.321,1.717,2.074,2.508,2.819,3.041,3.214,0.685,0.858,1.06,1.319,1.714,2.069,2.5,2.807,3.027,3.198,0.685,0.857,1.059,1.318,1.711,2.064,2.492,2.797,3.014,3.183,0.684,0.856,1.058,1.316,1.708,2.06,2.485,2.787,3.003,3.17,0.684,0.856,1.058,1.315,1.706,2.056,2.479,2.779,2.992,3.158,0.684,0.855,1.057,1.314,1.703,2.052,2.473,2.771,2.982,3.147,0.683,0.855,1.056,1.313,1.701,2.048,2.467,2.763,2.973,3.136,0.683,0.854,1.055,1.311,1.699,2.045,2.462,2.756,2.965,3.127,0.683,0.854,1.055,1.31,1.697,2.042,2.457,2.75,2.957,3.118,0.682,0.853,1.054,1.309,1.696,2.04,2.453,2.744,2.95,3.109,0.682,0.853,1.054,1.309,1.694,2.037,2.449,2.738,2.943,3.102,0.682,0.853,1.053,1.308,1.692,2.035,2.445,2.733,2.937,3.094,0.682,0.852,1.052,1.307,1.691,2.032,2.441,2.728,2.931,3.088,0.682,0.852,1.052,1.306,1.69,2.03,2.438,2.724,2.926,3.081,0.681,0.851,1.05,1.303,1.684,2.021,2.423,2.704,2.902,3.055,0.679,0.849,1.047,1.299,1.676,2.009,2.403,2.678,2.87,3.018,0.679,0.848,1.045,1.296,1.671,2,2.39,2.66,2.849,2.994,0.677,0.845,1.041,1.289,1.658,1.98,2.358,2.617,2.798,2.935,0.674,0.842,1.036,1.282,1.645,1.96,2.326,2.576,2.748,2.878)

# mypossibleses <- ((possiblemaxs)-(possiblemeans))/tq
# mypossibleses
# I create a data frame from this and then I control f to search for the se they created according to their alpha and beta:
# se
# mypossiblesesdf <- data.frame(mypossibleses, tq)

# I ctrl f and searched 0.04209 and got a few hits, but when I went to look at these with the all the digits being displayed none of them matched exactly the long version of their SE, i.e. 0.0420918376330846

# > print(mypossiblesesdf[748327,], digits = 22)

# So, I'm definitely not able to create their SE from the formula I used

# So, they definitely didnt create their SE from the same formula I used.



# The solution:

# [The below fact that there is a better formula may imply that the reason none of the million draws above match the SE recovered from the paper down to the decimal point is because I was using the wrong formula to calculte the SE, and maybe it would have matched given the correct formula from Briggs below]. Plus, even if they report 3 decimal points in the paper, you never know if they did their calculations with even more, say 5 decimal points, and as we saw above, even a few more decimal points can change the SE slightly, and thus the alpha plus beta and then the alpha and beta.

# I take the code provided by Briggs, provided by Devin, and my own assumed code, and I apply it to the Packman paper. Basically, I recover the SE's for this paper by applying the SE recovery code to the alpha and beta they report, then I compare all the different methods I have to get the SE myself to see which one comes out right most of the time.


# Here's my own code, the code provided by Briggs (in email) and the code reported by Devin Incerti (https://hesim-dev.github.io/hesim/articles/markov-cohort.html) when generating the SE:


# b <-c(891.6005,15.41799,736.9145,1257.92,419.1321,48.37248,54.66747,759.661,1161.571,46.55078,371.0238,613.4266,1349.813,1007.815,150.3308,515.7984,1.572009,1.709663,1196.415,44.16013,432.4305,1143.101,478.3303,344.844,318.2237,839.0925,665.8365,668.3704,15334.68,2261.868,210.4119,295.9456,839.0925,295.9456,248.218,50.63905,51.89032,54.66747,265.7833) 

# a <-c(121.581888,51.6167467,15.039072,334.383756,62.6289387,12.09312,8.16870263,3.817392,254.979072,48.4508078,45.8568782,226.883808,449.9375,168.165112,22.4632179,113.224032,29.8681689,2.26629759,280.640564,9.04484592,22.7595,242.476063,28.3755279,38.316,14.64628,6.766875,12.20474,6.751216,15.35003,6.806023,6.731443,6.657262,6.766875,6.657262,6.625944,14.28281,15.76329,8.168703,6.814956)

# max <-c(0.14,0.87,0.03,0.23,0.16,0.30,0.23,0.01,0.21,0.61,0.13,0.29,0.27,0.16,0.18,0.85,1.00,1.00,0.21,0.27,0.07,0.195,0.076,(13.0/100),(6.6/100),(1.6/100),(2.8/100),(2.0/100),(0.2/100),(0.6/100),(6.2/100),(4.4/100),(1.6/100),(4.4/100),(5.2/100),(32.0/100),(33.3/100),(23/100),(5.0/100))

# min <-c(0.10,0.67,0.00,0.19,0.10,0.10,0.07,0.00,0.15,0.41,0.09,0.25,0.23,0.12,0.08,0.79,0.85,0.13,0.17,0.07,0.03,0.155,0.036,(7.0/100),(2.2/100),(0.4/100),(0.8/100),(0.5/100),(0.1/100),(0.015/100),(1.6/100),(1.1/100),(0.4/100),(1.1/100),(1.3/100),(12.0/100),(13.3/100),(6.5/100),(1.3/100))   

# mean <-c(0.12,0.77,0.02,0.21,0.13,0.20,0.13,0.005,0.18,0.51,0.11,0.27,0.25,0.14,0.13,0.18,0.95,0.57,0.19,0.17,0.05,0.175,0.056,(10.00/100),(4.4/100),(0.8/100),(1.8/100),(1.0/100),(0.1/100),(0.3/100),(3.1/100),(2.2/100),(0.8/100),(2.2/100),(2.6/100),(22.0/100),(23.3/100),(13.0/100),(2.5/100))

# Recover the papers SE:

# Just a quick aside that for the values in Table A4, I have to divide these by 100. They start off as just percents, so 10.0 (7.0, 13.0) to reflect 10% with a range of 7 to 13 percent. But, if I want them to be the same as the percentages I have throughout the model, i.e., 0 is 0% and 1 is 100%, and 0.5 is 50%, then I'll need to divide these by 100. This goes equally for any percentages I find in the literature reported as just 10% say, and want to include, they'll need to be divided by 100 to put them in the same units as the 0-1 percentages range I use throughout my model. And the fact that the alpha and beta calculated in this report exactly match the alpha and beta I calculate when I've made this /100) adjustment means that, regardless of the fact that the authors list the percentages as 10.0, 7.0 and 13.0 in Table A4, when they actually went to analyse these, they also applied the /100) adjustment.

# Code to recover the SE's:


# Var <- a*b/((a+b)^2*(a+b+1))
# se<-sqrt(Var)

# My own assumed SE code:
# myse <- ((max)-(mean))/2
# My own assumed SE  code, using the min when the mean isnt centered in the range, so say the min is further from the mean than the max:
# mysemin <- ((mean)-(min))/2

# Devin's code to generate the SE, as described above:
# devinse <- (max - min)/(2 * qnorm(.975))

# Brigs code to generate the SE, with the max - the mean, and then repeated but for the mean minus the min:
# briggsse <- ((max)-(mean))/1.96
# briggssemin <- ((mean)-(min))/1.96

# Here are the results, for the paper:

# myse: ------11-1--10-1--------00-
# devinse: -------------11-1------
# briggsse:11111110111111-111-111-
# altbriggsse:11-1111111111
# wrongbriggsse:1
# logse:
# logse2:
# mysemin:
# briggssemin:111111
# logsemin2
# tqse:

# Obviously, briggsse is the most correct, most of the time (in this Table 1 represents an exact match, and - represents a close but not exact match, when something was wrong I either didnt include it, or included it as a 0 when it was really, really wrong). Only 12.82% of the time it was wrong. So, 87.18% of the time it was correct.

# When the range isnt PERFECTLY centered around the mean, i.e., the min is further away from the mean than the max, it looks like you use altbriggsse to incorporate this wider range, than you would see taking max-mean, what's interesting is that mean-min isnt enough to replicate the results in the study, it has to be max-min (although the fact that mean-min is divided by /1.96 and max-min is divided by /2*1.96 also contributes I'm sure). Devinse does this also, but I don't like it as much because the qnorm(.975) seems a bit less clear on what it's doing, even though > 2 * qnorm(.975) [1] 3.919928 is more or less the exact same as > 2*1.96 [1] 3.92. Nonetheless, Devins SE's are always slightly different to the actual SE when compared to briggsse, which has made me prefer briggsse.

# So, the recipe is simple, use briggsse when the range is centered around the base case value, or point estimate, or mean, or however they describe it. But, When the range isnt PERFECTLY centered around the mean, i.e., the min is further away from the mean than the max, or vice versa (even a little), it looks like you use altbriggsse to incorporate this wider range. I've reviewed studies where Beta values refer to probabilities, and studies where Beta values refer to utilities, and the above still holds regardless. 

# altbriggsse <- (max-min)/(2*1.96)


# wrongbriggsse <- ((max)-(min))/2*1.96

# logse <- (log(max) - log(min)) / 3.92

# 3.92 is just 1.96*2

# logse2 <- (log(max) - log(mean)) / (2*1.96)
# logsemin2 <- (log(mean) - log(min)) / (2*1.96)

# tq <- c(1,1.376,1.963,3.078,6.31,12.71,31.82,63.66,106.1,159.2,0.816,1.061,1.386,1.886,2.92,4.303,6.965,9.925,12.85,15.76,0.765,0.978,1.25,1.638,2.353,3.182,4.541,5.841,6.994,8.053,0.741,0.941,1.19,1.533,2.132,2.776,3.747,4.604,5.321,5.951,0.727,0.92,1.156,1.476,2.015,2.571,3.365,4.032,4.57,5.03,0.718,0.906,1.134,1.44,1.943,2.447,3.143,3.707,4.152,4.524,0.711,0.896,1.119,1.415,1.895,2.365,2.998,3.499,3.887,4.207,0.706,0.889,1.108,1.397,1.86,2.306,2.896,3.355,3.705,3.991,0.703,0.883,1.1,1.383,1.833,2.262,2.821,3.25,3.573,3.835,0.7,0.879,1.093,1.372,1.812,2.228,2.764,3.169,3.472,3.716,0.697,0.876,1.088,1.363,1.796,2.201,2.718,3.106,3.393,3.624,0.695,0.873,1.083,1.356,1.782,2.179,2.681,3.055,3.33,3.55,0.694,0.87,1.079,1.35,1.771,2.16,2.65,3.012,3.278,3.489,0.692,0.868,1.076,1.345,1.761,2.145,2.624,2.977,3.234,3.438,0.691,0.866,1.074,1.341,1.753,2.131,2.602,2.947,3.197,3.395,0.69,0.865,1.071,1.337,1.746,2.12,2.583,2.921,3.165,3.358,0.689,0.863,1.069,1.333,1.74,2.11,2.567,2.898,3.138,3.326,0.688,0.862,1.067,1.33,1.734,2.101,2.552,2.878,3.113,3.298,0.688,0.861,1.066,1.328,1.729,2.093,2.539,2.861,3.092,3.273,0.687,0.86,1.064,1.325,1.725,2.086,2.528,2.845,3.073,3.251,0.686,0.859,1.063,1.323,1.721,2.08,2.518,2.831,3.056,3.231,0.686,0.858,1.061,1.321,1.717,2.074,2.508,2.819,3.041,3.214,0.685,0.858,1.06,1.319,1.714,2.069,2.5,2.807,3.027,3.198,0.685,0.857,1.059,1.318,1.711,2.064,2.492,2.797,3.014,3.183,0.684,0.856,1.058,1.316,1.708,2.06,2.485,2.787,3.003,3.17,0.684,0.856,1.058,1.315,1.706,2.056,2.479,2.779,2.992,3.158,0.684,0.855,1.057,1.314,1.703,2.052,2.473,2.771,2.982,3.147,0.683,0.855,1.056,1.313,1.701,2.048,2.467,2.763,2.973,3.136,0.683,0.854,1.055,1.311,1.699,2.045,2.462,2.756,2.965,3.127,0.683,0.854,1.055,1.31,1.697,2.042,2.457,2.75,2.957,3.118,0.682,0.853,1.054,1.309,1.696,2.04,2.453,2.744,2.95,3.109,0.682,0.853,1.054,1.309,1.694,2.037,2.449,2.738,2.943,3.102,0.682,0.853,1.053,1.308,1.692,2.035,2.445,2.733,2.937,3.094,0.682,0.852,1.052,1.307,1.691,2.032,2.441,2.728,2.931,3.088,0.682,0.852,1.052,1.306,1.69,2.03,2.438,2.724,2.926,3.081,0.681,0.851,1.05,1.303,1.684,2.021,2.423,2.704,2.902,3.055,0.679,0.849,1.047,1.299,1.676,2.009,2.403,2.678,2.87,3.018,0.679,0.848,1.045,1.296,1.671,2,2.39,2.66,2.849,2.994,0.677,0.845,1.041,1.289,1.658,1.98,2.358,2.617,2.798,2.935,0.674,0.842,1.036,1.282,1.645,1.96,2.326,2.576,2.748,2.878)

#tqse <- ((max)-(min))/tq
# sprintf("%.100f",tqse <- ((max)-(min))/tq)
# the above lets me choose how many decimal places to display in this piece of code (I've chosen 100 for some reason).

# Below I review the results:

# se
# myse
# devinse
# briggsse
# altbriggsse
# wrongbriggsse
# logse
# logse2
# mysemin
# briggssemin
# logsemin2
# tqse


# I apply this to other papers:

# Picot, J., et al. "The clinical effectiveness and cost-effectiveness of bortezomib and thalidomide in combination regimens with an alkylating agent and a corticosteroid for the first-line treatment of multiple myeloma: a systematic review and economic evaluation." Health technology assessment (Winchester, England) 15.41 (2011): 1.

# For this, briggsse is always right, while the other SE's are not.   

# Thanks for your email. I’m afraid the details are lost to me in the midst of time. Having a look at our model, we may have used different assumptions to the confidence intervals depending on what data were available, for example assuming a 10%, 20% increase of the mean for the CI. We then would have worked out the standard error from the confidence intervals as you describe in your email [briggse and altbriggse] and the alpha/beta are worked out from the se and mean. The formulas for alpha and beta are taken from Briggs et al. (Decision modelling for health economic evaluation, 2006).



# Sharp, Linda, et al. "Cost-effectiveness of population-based screening for colorectal cancer: a comparison of guaiac-based faecal occult blood testing, faecal immunochemical testing and flexible sigmoidoscopy." British journal of cancer 106.5 (20book12): 805-816.

# For this, I typically get approximations of the SE from the paper.

# Thank you for getting in touch and it’s great that your work on the CRC model is progressing. Unfortunately, I don’t have an answer to your query. I was not actually involved in deriving these estimates and it is also quite a long time ago that I don’t remember who did the actual analysis - it would have been either Linda or Alan.

# I think the approach that you are taking seems valid and reasonable so it may be better for you to use the simple estimate of +/-20%.

# Sorry I am not able to provide more clarification.


# Campbell, Jonathon R., et al. "Cost-effectiveness of latent tuberculosis infection screening before immigration to low-incidence countries." Emerging infectious diseases 25.4 (2019): 661.

# For this, the SE is close, but typically a few percentage points off the recovered SE, so instead of 0.06 we might have 0.05, etc., which shouldnt really change the mean calculation much when doing the beta draws in the PSA.  

# Gao, L., Moodie, M., Mitchell, P. J., Churilov, L., Kleinig, T. J., Yassi, N., … & EXTEND-IA TNK Investigators. (2020). Cost-effectiveness of tenecteplase before thrombectomy for ischemic stroke. Stroke, 51(12), 3681-3689. https://www.ahajournals.org/doi/epdf/10.1161/STROKEAHA.120.029666

# For this, the SE I generate is typically quite different to the SE they report.

# # Lan Gao of "Cost-Effectiveness of Tenecteplase Before Thrombectomy for Ischemic Stroke" didnt use a range, instead he assumed SD=0.1*mean (here the terms SE and SD are being used to refer to the same thing, Gianluca calss SE by SD also). 


#A cluster randomised trial, cost-effectiveness analysis and psychosocial evaluation of insulin pump therapy compared with multiple injections during flexible intensive insulin therapy for type 1 diabetes: the REPOSE Trial https://www.journalslibrary.nihr.ac.uk/hta/hta21200#/abstract

# For this, the SE I recover from the alpha and beta matches their SE exactly, then I generate min and max as MaxMatch <- mean + 2*se and MinMatch <- mean - 2*se, and get the exact same results from myse <- ((max)-(mean))/2, meaning that they used my code to get their SE, briggs and devins SE code is almost identical in the SE it generates.



# https://clas.ucdenver.edu/marcelo-perraillon/sites/default/files/attached-files/lecture_13_uncertainty.pdf also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Betadist utility\lecture_13_uncertainty.pdf

# For this, the altbriggse code gets me SE's that are very similar to the SE's reported for these non-centered parameters.


# Appendix F: Health economics report (2015 work undertaken by NICE Internal Clinical Guidelines) https://www.nice.org.uk/guidance/ng33/evidence/appendix-f-full-he-report-pdf-80851860763

# For this: My SE's from Briggses are very close to the SE's recovered from the study, and when they are not, altbriggses are very close to those recovered from the study and the range is not symmetric around the point estimate, which is the whole reason for using altbriggses anyway.


# Here's the takeaway on the Beta formuala. My SE's are typically very close to the SE's that I recover from the published studies. I've seen first hand, that you need big changes to the SE to affect the mean you draw in the PSA. So, what I'll do is apply the briggse or altbriggse as appropriate, and then when I do the PSA check the mean on the parameter drawn from the PSA draws to make sure the parameter is on average the same as the one we started with, I can do this by doing mean() whatever the parameter is, and that will show me the average value.

# The following paper shows that my methodology above can be applied to papers with 2.5/97.5% percentiles and also shows that the code for myse is used in those rare situations where the mean, min and max are all the exact same number: A dynamic Bayesian Markov model for health economic evaluations of interventions in infectious disease https://sci-hub.ru/10.1186/s12874-018-0541-7 also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Betadist utility\haeussler2018.pdf

# And speaking to Gianluca Baio directly in email, he tells me to calculate SE's in the same way in which I was doing this with myse.

# # I think very similarly to what you did --- but say you think that the info is a range of .25 to .43, then you kind of figure out that the mean is sort of halfway through (~0.34). Then you assume that, approximately, the sd would be around 0.045 (because 0.34 +/- 2*.045 gets you, approximately, in the range you have in the first place). [i.e. brings you to the min and the max you started with].

# mean <-c(0.34)
# min <-c(0.25)
# max <-c(0.43)

# myse <- ((max)-(mean))/2
# myse <- ((0.43)-(0.34))/2

# He says the se should be a number multiplied by two that brings you from the mean to the max or the mean to the min. So, ((0.43)-(0.34)) = 0.09, 0.09/2 = 0.045. The number 0.045 times 2 gives you 0.09, 0.34 + 0.09 gives you 0.43, i.e., the max.


#












############### Below are some incorrect solutions I came up with #############

# Incorrect Solution 1:

## My formulas and the need for a 95% CI:
 
# As per: https://stats.stackexchange.com/questions/550293/how-to-calculate-standard-error-given-mean-and-confidence-interval-for-a-gamma-d/550892#550892
 
 
# "I would like to note that, while those values in the table happen to correspond with ±2σ, the minimum and maximum values do not generally follow such simple formula with mean plus-minus some standard deviation.
 
# In this case, the minimum and maximum values only correspond to the interval μ±2σ because the distribution seems to have been truncated at those values."
 
 
# i.e., my formula will only work if the minimum and maximum values reported in a study are ±2 SE from the mean, i.e. + 2 SE from the mean for the maximum value, and -2 SE from the mean for the minimum value.
 
# And that will be the case any time there is a 95% Confidence Interval:
 

# [ "Since 95% of values fall within two standard deviations of the mean according to the 68-95-99.7 Rule, simply add and subtract two standard deviations from the mean in order to obtain the 95% confidence interval. Notice that with higher confidence levels the confidence interval gets large so there is less precision." https://www.westga.edu/academics/research/vrc/assets/docs/confidence_intervals_notes.pdf also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\gammadist\confidence_intervals_notes.pdf

# A good image can be found as per: https://www.quora.com/What-is-meant-by-one-standard-deviation-away-from-the-mean also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\gammadist\www-quora-com-What-is-meant-by-.pdf]

# So, if I'm taking my costs and a range about these costs from the literature, I just need to make sure the range is a 95% Confidence Interval, like that reported here: Tilson, L., Sharp, L., Usher, C., Walsh, C., Whyte, S., O’Ceilleachair, A., ... & Barry, M. (2012). Cost of care for colorectal cancer in Ireland: a health care payer perspective. The European journal of health economics, 13(4), 511-524. (https://www.researchgate.net/publication/51188456_Cost_of_care_for_colorectal_cancer_in_Ireland_A_health_care_payer_perspective) reports a cost in Table 3 for Colorectal cancer with a 95% CI: € 48,835 (€40,548–€62,582)..

# For "PK test cost MEAN: 400.00 MIN: 300.00 MAX: 500.00" from Table 4, row 3 page 222 in Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R. (2014). Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer. Clinical colorectal cancer, 13(4), 219-225, plugging the SE and mean into the CI = m ± t*SE from Jochen above I get:
 
# 400 + 2*50 = 500
 
# 400 - 2*50 = 300
 
# Which is the same confidence interval as I got in the Table 4 row 3, i.e. 300 and 500.
 
# So, this might be a good way to check if my standard error is correct. Although this will only work if my confidence interval is centred on the mean, i.e. if I am the same distance from the mean to the max and to the min. If my confidence interval is NOT centred on the mean, I can calculate 2 standard errors, one between the lower interval and the mean, and one between the upper interval and the mean, and then apply these in CI = m ± t*SE, knowing that:

# CIupper = m + t*SE

# CIlower = m - t*SE

# So, I've figured out a simple test for whether the max/min reported is ±2σ, i.e., whether it is a 95% CI:
# MaxMatch <- Mean + 2*se 
# MinMatch <- Mean - 2*se
 
# I can then check if the max/min reported in a Table match the above, and if so I know that the max/min reported is ±2σ.
 
# If not, it may be that they used the se off the minimum as it's further away, so try the below again, but with the minimums SE:

# MaxMatch <- Mean + 2*se 
# MinMatch <- Mean - 2*se
 
 
# I can then check if the max/min reported in a Table match the above, and if so I know that the max/min reported is ±2σ.
 
 
# Even if it's not symmetric, this may still be a 95% confidence interval if the mean is at 2 of one of the standard errors from the min or the max, we can see this in action below:
 
 
# > Maximum <- 27655.84
# > Mean <- 11565.60
# > 
# > se <- ((Maximum) - (Mean)) / 2
# > se                                  
# [1] 8045.12
# > 
# > MaxMatch <- Mean + 2*se 
# > MinMatch <- Mean - 2*se
# > MaxMatch
# [1] 27655.84
# > MinMatch
# [1] -4524.64
 
# But still, for these values
 
# FN: Value: 11,565.60 Minimum: 7092.16 Maximum: 27,655.84  Distribution: γ: (2.067, 5596.247)
 
# that are not centered on the mean, this still came out the same as the Table using this se <- ((Maximum) - (Mean)) / 2 method. Because looking at the below, the upper interval (the maximum) is further from the mean at a difference of 16090.24, than the lower interval (the minimum) at a difference between it and the mean of 4473.44 (i.e. the maximum is nearly four times further away from the mean that the minimum).
 
 
# > Maximum <- 27655.84
# > Minimum <- 7092.16
# > Mean <-  11565.60
  
# > Maximum - Mean
# [1] 16090.24
  
# > Mean - Minimum
# [1] 4473.44
 
 
# And when we plug the se in for the further away max we get:
 
 
# > a.cIntervention
# [1] 2.066671
# > b.cIntervention
# [1] 5596.247
 
# Just the same as the values for FN above.
# 

# So, this kind of applies to situations where an interval is reported, when you might want to check if it's a 95% confidence interval, and if not, which interval is furthest from the mean.


# So, I did this in the papers, by recovering the SE from the paper, and building a max and min that was 2 of these SE's from the mean as follows, and comparing this to the max and min reported in the paper:


# Var <- a*b/((a+b)^2*(a+b+1))
# se<-sqrt(Var)

# MaxMatch <- mean + 2*se 
# MinMatch <- mean - 2*se

# I found that the times where the interval wasnt like a 95% CI, i.e., the max and min were not 2 SE's either side of the mean away, was the times that my calculated SE and the papers calculated SE's didnt match. When they were close enough to 2 SE's either side, my formulas would calculate an SE that was a closer approximination to what was reported in the paper, but when they were further away than the 2 SE's either side of the mean, the SE's calculated from my formula would also be further away. The takeaway here is probably that, with a 95% CI, I can use my briggsse formula. Where the range isnt a 95% CI, but is pretty centered on the mean value I can use altbriggsse, when the range isnt a 95% CI, and isnt even slightly centered on the mean value then I should consider treating the mean as a point estimate and ignoring the range, to create my own SE which I know I can rely on.

# Incorrect Solution 2:

# Here's what I originally thought was happening in the Tilson paper, the formula to generate the se is se <- ((Maximum) - (Mean))/2 but the 2 is really a t-quantile with n-1 degrees of freedom [i.e., SE = (CIupper-m)/t], but I use 2 as a simplification because "For large n the quantile approaches 2.0 (well, 1.959964... to be more precise; but using 2.0 is good enough)." I think that the authors of this study actually looked up the t-quantile and divided by that each time, rather than dividing by 2, i.e. "The t-quantile can be looked up for the level of confidence when the total sample size (n) is known.... For instance, the t-quantile for 95% confidence, n=10 and k=2 is 2.3." per: researchgate.net/post/Formula_for_calculate_Standard_errorSE_from_Confidence_IntervalCI There is also information supporting this here: https://stats.stackexchange.com/questions/550293/how-to-calculate-standard-error-given-mean-and-confidence-interval-for-a-gamma-d


# I checked this by dividing by every value in the t-quantile here: https://homepage.cs.uiowa.edu/~jblang/probability.calculators/t.table.htm

# tq <- c(1,1.376,1.963,3.078,6.31,12.71,31.82,63.66,106.1,159.2,0.816,1.061,1.386,1.886,2.92,4.303,6.965,9.925,12.85,15.76,0.765,0.978,1.25,1.638,2.353,3.182,4.541,5.841,6.994,8.053,0.741,0.941,1.19,1.533,2.132,2.776,3.747,4.604,5.321,5.951,0.727,0.92,1.156,1.476,2.015,2.571,3.365,4.032,4.57,5.03,0.718,0.906,1.134,1.44,1.943,2.447,3.143,3.707,4.152,4.524,0.711,0.896,1.119,1.415,1.895,2.365,2.998,3.499,3.887,4.207,0.706,0.889,1.108,1.397,1.86,2.306,2.896,3.355,3.705,3.991,0.703,0.883,1.1,1.383,1.833,2.262,2.821,3.25,3.573,3.835,0.7,0.879,1.093,1.372,1.812,2.228,2.764,3.169,3.472,3.716,0.697,0.876,1.088,1.363,1.796,2.201,2.718,3.106,3.393,3.624,0.695,0.873,1.083,1.356,1.782,2.179,2.681,3.055,3.33,3.55,0.694,0.87,1.079,1.35,1.771,2.16,2.65,3.012,3.278,3.489,0.692,0.868,1.076,1.345,1.761,2.145,2.624,2.977,3.234,3.438,0.691,0.866,1.074,1.341,1.753,2.131,2.602,2.947,3.197,3.395,0.69,0.865,1.071,1.337,1.746,2.12,2.583,2.921,3.165,3.358,0.689,0.863,1.069,1.333,1.74,2.11,2.567,2.898,3.138,3.326,0.688,0.862,1.067,1.33,1.734,2.101,2.552,2.878,3.113,3.298,0.688,0.861,1.066,1.328,1.729,2.093,2.539,2.861,3.092,3.273,0.687,0.86,1.064,1.325,1.725,2.086,2.528,2.845,3.073,3.251,0.686,0.859,1.063,1.323,1.721,2.08,2.518,2.831,3.056,3.231,0.686,0.858,1.061,1.321,1.717,2.074,2.508,2.819,3.041,3.214,0.685,0.858,1.06,1.319,1.714,2.069,2.5,2.807,3.027,3.198,0.685,0.857,1.059,1.318,1.711,2.064,2.492,2.797,3.014,3.183,0.684,0.856,1.058,1.316,1.708,2.06,2.485,2.787,3.003,3.17,0.684,0.856,1.058,1.315,1.706,2.056,2.479,2.779,2.992,3.158,0.684,0.855,1.057,1.314,1.703,2.052,2.473,2.771,2.982,3.147,0.683,0.855,1.056,1.313,1.701,2.048,2.467,2.763,2.973,3.136,0.683,0.854,1.055,1.311,1.699,2.045,2.462,2.756,2.965,3.127,0.683,0.854,1.055,1.31,1.697,2.042,2.457,2.75,2.957,3.118,0.682,0.853,1.054,1.309,1.696,2.04,2.453,2.744,2.95,3.109,0.682,0.853,1.054,1.309,1.694,2.037,2.449,2.738,2.943,3.102,0.682,0.853,1.053,1.308,1.692,2.035,2.445,2.733,2.937,3.094,0.682,0.852,1.052,1.307,1.691,2.032,2.441,2.728,2.931,3.088,0.682,0.852,1.052,1.306,1.69,2.03,2.438,2.724,2.926,3.081,0.681,0.851,1.05,1.303,1.684,2.021,2.423,2.704,2.902,3.055,0.679,0.849,1.047,1.299,1.676,2.009,2.403,2.678,2.87,3.018,0.679,0.848,1.045,1.296,1.671,2,2.39,2.66,2.849,2.994,0.677,0.845,1.041,1.289,1.658,1.98,2.358,2.617,2.798,2.935,0.674,0.842,1.036,1.282,1.645,1.96,2.326,2.576,2.748,2.878)

# And when I checked if my SE now matched the SE I got from the alpha and beta in the paper, it still didnt match.

# myse <- ((max)-(mean))/tq
# myse



# For Table 1 in the following, under utilities if I plug the SE and base case value into my formula for creating alpha and beta and then take randome draws for the mean from rbeta I get the same mean as reported as the base case value in the table. Indicating that the formula I use once I have the SE and mean to get the alpha and beta, and thus the new mean, is correct: Kristin, E., Endarti, D., Khoe, L. C., Taroeno-Hariadi, K. W., Trijayanti, C., Armansyah, A., & Sastroasmoro, S. (2021). Economic evaluation of adding bevacizumab to chemotherapy for metastatic colorectal cancer (mCRC) patients in Indonesia. Asian Pacific Journal of Cancer Prevention: APJCP, 22(6), 1921. also saved here: file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Evidence%20Synthesis/Economic%20Models/Kristin%20et%20al_2021_Economic%20Evaluation%20of%20Adding%20Bevacizumab%20to%20Chemotherapy%20for%20Metastatic.pdf


# So, the whole problems comes down to the manner in which I am generating the SE from the mean and range.


######################## SECTION ON INCORRECT SOLUTIONS ENDS ###################


  u_F       = rbeta(n_runs, shape1 =  50.4, shape2 = 12.6), 
  u_P       = rbeta(n_runs, shape1 = 49.5, shape2 = 49.5), 
  u_D       = 0,
# The utility of being dead doesnt vary, it always is the same utility to be a dead person, which is 0 because we don't have any quality of life, as we are no longer alive. I could have also just set u_D       = u_D, because when you set something equal to itself in here, that also means it's constant.
  u_AE1     = rbeta(n_runs, shape1 =  50.4, shape2 = 12.6), 
  u_AE2     = rbeta(n_runs, shape1 =  50.4, shape2 = 12.6), 
  u_AE3     = rbeta(n_runs, shape1 =  50.4, shape2 = 12.6), 







# Discounting:
  
# A uniform distribution is a probability distribution in which every value between an interval from a to b is equally likely to be chosen. https://www.statology.org/uniform-distribution-r/
  
# "Generates random values that are evenly spread between min and max bounds" - https://docs.oracle.com/cd/E57185_01/CYBUG/apcs03s03s01.html 

# Devin Incerti talks about the Uniform distribution here: "The uniform distribution is useful when there is little data available to estimate a parameter and determine its distribution. It is always preferable to place uncertainty on a parameter even when there is little evidence for it than to assume a fixed value (which gives a false sense of precision). Sampling from the uniform distribution is straightforward." https://devinincerti.com/2018/02/10/psa.html  
  
#Reading the literature, it seems like studies pick a lower bound and upper bound for the discount rate, such that the average (mean of the discount rate) will reach the point estimate they started with: 
  
  # A Trial-Based Assessment of the Cost-Utility of Bevacizumab and Chemotherapy versus Chemotherapy Alone for Advanced Non-Small Cell Lung Cancer https://sci-hub.ru/10.1016/j.jval.2011.04.004 
  
# file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Evidence%20Synthesis/Paper%20Materials%20and%20Methods/Hamdy%20Elsisi%20et%20al_2019_Cost-effectiveness%20of%20sorafenib%20versus%20best%20supportive%20care%20in%20advanced.pdf  
    
# Koen's excel file does this too: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\Matching the model with the evidence Koen
  
# as informed by: https://pubs.wsb.wisc.edu/academics/analytics-using-r-2019/uniform-continuous-version.html also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\3.5 Uniform (Continuous Version) _ Analytics Using R.pdf


#  d_c       = 0.04,
#  d_e       = 0.04,
# If I wanted constant values for the discount rates I would have set them as above.
  
  d_c    = runif(n_runs,  min = 0, max = 0.08),
  d_e    = runif(n_runs,  min = 0, max = 0.08),

  n_cycle   = n_cycle,
  t_cycle   = t_cycle
)

# Inspect the data.frame to see what it looks like (we'll see the first 6 observations):
head(df_PA_input)

# It's a dataframe made up of the 10,000 values I asked be made at the start of this code chunk.

# If I wanted to save the dataframe, I would do this as follows:

#save(df_PA_input, file = "df_PA_input.rda")

















# Histogram of parameters


# I believe they make a histogram of parameters to show the distribution of parameters, and thus indicate which distribution they should be using in their sensitivity analysis. Just like slide 18 of C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\A212ProbabilisticDistributions-210604-154.pdf which is also a HISTOGRAM OF RANDOM DRAWS FOR RELATIVE RISK and the related notes in C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2_Making Models Probabilistic\A2.1.2 Distributions for parameters\notes.txt from [SLIDE 18] onward.

# i.e., "And that's exactly what Andrews done here showing the histogram of random draws from the relative risk parameter, and you see that, in fact, in this case, with quite a lot of information [I guess the nine trials with a total of 2,541 patients pooled], we wouldn't have been that far off if we'd assumed a normally distributed relative risk [i.e. our distribution looks very similar to a normal distribution]. But you can probably detect a very slight skew to that distribution, which reveals it actually as a log normally distributed relative risk."


ggplot(melt(df_PA_input, variable.name = "Parameter"), aes(x = value)) +
  facet_wrap(~Parameter, scales = "free") +
  geom_histogram(aes(y = ..density..)) +
  theme_bw(base_size = 16) + 
  theme(axis.text = element_text(size=8))
```

I need to fix the "\# Histogram of parameters" section above, the below gives advice on doing this:

<https://stackoverflow.com/questions/68416435/rcpp-package-doesnt-include-rcpp-precious-remove>

[https://www.mail-archive.com/rcpp-devel\@lists.r-forge.r-project.org/msg10226.html](https://www.mail-archive.com/rcpp-devel@lists.r-forge.r-project.org/msg10226.html){.uri}

<https://statisticsglobe.com/warning-cannot-remove-prior-installation-in-r>

The above approach worked, I manually deleted the package, installed darthtools from github, chose option 4, i.e. replace rccp and then I was done.

## 09.1 Conduct probabilistic sensitivity analysis

To propagate uncertainty throughout the model, we assigned distributions to any parameters not estimated with certainty.
We consider whether values within a plausible range for input variables alter the conclusions of cost-effectiveness drawn from the original analysis.

To evaluate the robustness of the model, we apply deterministic methods (point estimates with an appropriate range) and probabilistic methods (parameterized distributions) to conduct a sensitivity analysis of our results.
C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf

```{r}

# Running the probabilistic analysis :

# First we need to create data.frames to store the output from the PSA:

df_c <- df_e <- data.frame(
  SoC = rep(NA, n_runs),
  Exp = rep(NA, n_runs)
)

# As you'll see, first we make blank (repeat NA for the number of runs of the simulationn_runs) data.frames for costs (df_c) and effectiveness (df_e) for SoC and the experimental treatment:

head(df_c)

# > head(df_c)
#   SoC Exp
# 1  NA  NA
# 2  NA  NA
# 3  NA  NA
# 4  NA  NA
# 5  NA  NA
# 6  NA  NA

head(df_e)

# > head(df_e)
#   SoC Exp
# 1  NA  NA
# 2  NA  NA
# 3  NA  NA
# 4  NA  NA
# 5  NA  NA
# 6  NA  NA

# We run the Markov model for each set of parameter values from the PSA input dataset (a set of parameters here is the row of parameter values in the PSA data.frame from whatever run number of the nruns in the PSA dataset we are on, i.e. if we were doing this for run number 10 out of 10,000, we would be on row 10 in the data.frame (as each row refers to a run) and we would use all the values for costs and effect that appear in this row when running the Markov model. So, we would be using all the random draws for that PSA run in the Markov model).

# - I read a note indicating that this loop can be run in parallel to decrease the runtime, something to consider in the future if it takes a very long time...

for(i_run in 1:n_runs){

    # Evaluate the model and store the outcomes
  l_out_temp    <- oncologySemiMarkov(l_params_all = df_PA_input[i_run, ], n_wtp = 45000)
# The above is basically saying, apply the oncologySemiMarkov_function, where l_params_all in the function (i.e. the list of parameters the function is to be applied to) is equal to the parameters from the PSA data.frame for the run we are in, and the willingness to pay threshold is 20,000.
  df_c[i_run, ] <- l_out_temp$Cost
  df_e[i_run, ] <- l_out_temp$Effect
# The above says, for the costs and effectiveness data.frames, store the value of costs and effects for each run that we are on in a row that reflects that run (remembering that [ROW,COLUMN], we are putting each cost and effect in a row that relates to the number of the run we are on, so if we are on run 1, i_run = 1 and we store the costs and effects for that run 1 in row 1. Remembering that our oncologySemiMarkov makes the "Cost" and "Effect" parameters at the end of the function, after applying the Markov cost-effectiveness model to all our input data for the cost-effectiveness model (so, things like, cost, utility and probability), so we are just pulling the ouputted Cost and Effect values calculated from a Markov cost-effectiveness model applied to our input data after the input data has been randomly drawn from a PSA (per the PSA input dataframe)).  

  
  # While we're doing this, we might like to display the progress of the simulation:
  if(i_run/(n_runs/10) == round(i_run/(n_runs/10), 0)) { # We've chosen to display progress every 10%
    cat('\r', paste(i_run/n_runs * 100, "% done", sep = " "))
  }
}
```

## 09.2 Create PSA object for dampack

```{r}
# Dampack has a number of functions to summarise and visualise the results of a probabilistic sensitivity analysis. However, the data needs to be in a specific structure before we can use those functions. 

# The 'dampack' package contains multiple useful functions that summarize and visualize the results of a
# probabilitic analysis. To use those functions, the data has to be in a particular structure.
l_PA <- make_psa_obj(cost          = df_c, 
                     effectiveness = df_e, 
                     parameters    = df_PA_input, 
                     strategies    = c("SoC", "Exp"))

# So, basically we make a psa object for Dampack where df_c is the dataframe of costs from the Markov model being applied to the PSA data, and df_e is the data.frame of effectiveness from the Markov model being applied to the PSA dataset, the parameters that are included are those from the PSA analysis, which we fed into df_PA_input above (df_PA_input<-) and the two strategies are SoC and Exp.
```

## 09.2.1 Save PSA objects

```{r}
# If we wanted to save the PSA objects once they have been created above, we could do this as follows (v_names_strats is just another way of including the strategies part from above):
# save(df_PA_input, df_c, df_e, v_names_strats, n_str, l_PA,
#     file = "markov_3state_PSA_dataset.RData")
```

## 09.3.1 Conduct CEA with probabilistic output

In the PSA we computed model outcomes (total discounted costs and QALYs) for the experimental strategy and standard of care for each sampled set of parameter values.
Results were remarkably similar to the deterministic analysis, per the probabilistic league table below [or else I could say, eith ICER of X, incremental costs of Y and incremental effectiveness of Y, i.e. describe the league table, but describe it in words, i.e., a mean incremental cost of xx,xxx (95% CI xx,xxx - xx,xxx) with a mean incremental benefit of x.xx QALYs (95% CI x.xx - x.xx), resulted in a mean ICER of xxx,xxx per QALY (95% CI xxx,xxx - xxx,xxx).], demonstrating the robustness of the model.

The values estimated using means from the PSA (costs, QALYs and ICER) reflect those estimated in the deterministic analysis.

:

```{r}

# First we take the mean outcome estimates, that is, we summarise the expected costs and effects for each strategy from the PSA:
(df_out_ce_PA <- summary(l_PA))


# Calculate incremental cost-effectiveness ratios (ICERs)

# Then we calculate the ICERs from this df_out_ce_PA (summary must be a dampack function doing things under the hood to create a selectable ($) meanCost, meanEffect and Strategy parameter in df_out_ce_PA):
(df_cea_PA <- calculate_icers(cost       = df_out_ce_PA$meanCost, 
                              effect     = df_out_ce_PA$meanEffect,
                              strategies = df_out_ce_PA$Strategy))

# We can view the ICER results from the PSA here:
df_cea_PA

# If we wanted to save the CEA (or leauge) table with ICERs, we would do this as follows:
# As .RData
# save(df_cea_pa, 
#     file = "markov_3state_probabilistic_CEA_results.RData")
# As .csv
# write.csv(df_cea_pa, 
#          file = "markov_3state_probabilistic_CEA_results.csv")


## CEA table in proper format ---- per: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\September-Workshop-main\September-Workshop-main\analysis\cSTM_time_dep_simulation.r
table_cea_PA <- format_table_cea(df_cea_PA) # Function included in "R/Functions.R"; depends on the `scales` package
table_cea_PA
```

## 09.3.2 Cost-Effectiveness Scatter plot

This indicates that what I am displaying here is not the cost-effectiveness plane, but a cost-effectiveness scatter plot: <https://yhec.co.uk/glossary/cost-effectiveness-plane/>

Devin does the traditional cost-effectiveness plane (but DARTH may also offer this in Dampack, and I should dig in there first if I want to draw a more traditional cost-effectiveness plane) <https://hesim-dev.github.io/hesim/articles/cea.html>

Figure 3 here shows the same scatterplt as I make, but parts of it cross the axis, I suspect that my scatterplot would cross the axis if necessary and thus reflect the above planes: Cost-effectiveness of capecitabine and bevacizumab maintenance treatment after first-line induction treatment in metastatic colorectal cancer sci-hub.ru/10.1016/j.ejca.2017.01.019 Actually.
That would never happen.
The axis needs an origin point.
Typically ,the origin point is the SoC's cost and effect, than we can say, look, does Exp fall below this cost (so our points for cost are lower on the graph, below the origin point) or above (so our costs are above this point on the graph, higher up above the origin point) and how does are utility look, higher than the SoC, so further to the right where effectiveness is the numberline along the bottom on the x-axis, or worse than SoC, so to the left of the SoC origin point, so a lower utility as we get further down the numberline and away from the SoC origin point for utility.
It's easiest to understand this while looking at Figure 3 below, where C is the origin point, because it A) reflects what I am saying above, and B) Shows that even the SoC maybe can be made of random draws too (although I'm not super sure on that, because if that value keeps changing then the comparative value of the ExP would be different always, so probably we need to still take the SoC value for the origin point we started with from the deterministic CEA so that we have something to compare our probabilistic ExP values to, but then we show the variability in SoC values around this from the PSA draws for SoC).
Realistically, it's probably best to produce a scatterplot as provided by dampack, and if a reviewer then wants the cost-effectiveness plane, to look into how this can be done in dampack.

This is supported by Figure 3 of: Probabilistic Analysis of Cost-Effectiveness Models: Choosing between Treatment Strategies for Gastroesophageal Reflux Disease Andrew H. Briggs, DPhil, Ron Goeree, MA, Gord Blackhouse, MBA, Bernie J. O'Brien, PhD med.mcgill.ca/epidemiology/courses/EPIB654/Summer2010/EF/example%20PPI.pdf

The cost-effectiveness plane illustrates the distribution of costs and effects for SoC and Exp.

Incremental costs and QALYs with their respective ICERs.

These values are illustrated in a cost-effectiveness plane of incremental costs in the y axis and incremental QALYs in the x axis.

We randomly sampled from each specified parameter distribution in the probabilistic analysis, calculating the expected costs and effects of that combination of parameter values.
This reflects a single replication of model results, in total, 10,000 replications were completed to determine the possible distribution of the resulting ICERs for both SoC and the experimental treatment.
In Figure X we plot the results of this probabilistic sensitivity analysis for these 10,000 replications from the model.

In Figure X we plot a probabilistic sensitivity analysis for 10,000 samples.

[**Legend Underneath:** Fig.
X. Probabilistic Sensitivity Analysis (n = 10,000).
Each dot represents an incremental cost-effectiveness ratio, ellipse represent the 95% confidence incidence.]

Well explained here: *An Introductory Tutorial on Cohort State-Transition Models in R Using a Cost-Effectiveness Analysis Example <https://arxiv.org/pdf/2001.07824.pdf>*

3.2.
Cost-effectiveness analysis The cost-effectiveness outcomes obtained from the DT-STM and the DES model are presented in the incremental cost-effectiveness planes of Fig.
4.
The incremental effectiveness estimates, including their 95% confidence intervals (CI), for CAP-B maintenance therapy compared to the observation strategy are 0.21 (CI: 0.015; 0.430) and 0.18 (CI: 0.006; 0.374) QALYs, and the incremental costs are €35,536 (CI: 19,945; 54,629) and €30,053, (CI: 17,047; 46,132) for the DT-STM and DES model, respectively.
The mean ICERs are €172,443 and €168,383 per QALY gained for the DT-STM and DES model, respectively.
The PSA for both models only demonstrated a small difference in the amount of uncertainty surrounding the mean ICER point-estimates (Fig. 4).
This is illustrated by the magnitude of the 95%-confidence ellipses surrounding these estimates, being slightly smaller for the DES model.
However, as both mean ICER point-estimates and corresponding confidence ellipses are located rather similarly compared to the willingness- to-pay (WTP) threshold, the CEACs for both models are similar (Fig. 5).
per: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Matching the model with the evidence Koen

Also see here for write up:

Sharp, L., Tilson, L., Whyte, S., O'Ceilleachair, A., Walsh, C., Usher, C., ... & Comber, H.
(2012).
Cost-effectiveness of population-based screening for colorectal cancer: a comparison of guaiac-based faecal occult blood testing, faecal immunochemical testing and flexible sigmoidoscopy.
British journal of cancer, 106(5), 805-816.
<https://www.nature.com/articles/bjc2011580.pdf>

```{r}
# Incremental cost-effectiveness plane
plot(l_PA)

# plot(l_PA, xlim = c(9.5, 22.5))
# In the DARTH package R code they use the additional bits above, I can see how they look in my own model, and if I'd like to use them.
```

## 09.4.2 Cost-effectiveness acceptability curves (CEACs) and frontier (CEAF)

Cost-effectiveness acceptability curves describe the probability of cost-effectiveness (i.e. that the ICER is below the WTP value) for each treatment option.

We calculated the probability the novel strategy would be cost-effective under several willingness to pay thresholds and illustrated this with a cost-effectiveness acceptability curve.

We derived cost-effectiveness acceptability curves (CEACs) from the probabilistic sensitivity analysis.
The results are shown in CEACs in Figure X. This describes the probability that the addition of Bevacizumab was cost-effective across increasing willingness-to-pay (WTP) values.
Results demonstrate a 100% probability that this strategy was more cost-effective when the threshold was X, and a xx.xx% probability of cost-effectiveness when the threshold was Y.
These results indicate that base case analysis produces robust results.

Borrowing from: A Cost-Effectiveness Analysis of Currently Approved Treatments for HBeAg-Positive Chronic Hepatitis B <https://sci-hub.ru/10.2165/00019053-200826110-00006> I can say:

The results of the probabilistic sensitivity analysis indicate that treatment X is preferred at cost-effectiveness thresholds less than approximately \$X,000 per QALY, and treatment Y had the highest probability of being optimal above this willingness to pay threshold.

Understand this plot here:

<https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html>

Also saved here:

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\sensitivity analysis help files\\Basic Cost Effectiveness Analysis.pdf

```{r}

# Cost-effectiveness acceptability curve (CEAC):

# We first generate a vector of willingness to pay values (thresholds) to define for which values the CEAC is made:
v_wtp <- seq(0, 50000, by = 100)
# This basically gives you a sequence of willingness to pay thresholds from 0 all the way up to 50,000 euro, increasing by 100 euro each time, so start with 0 euro, go to 100 euro, go to 200 euro, and so on until you hit 50,000. You can chose any max value you like and any increment value you like, for example, in the DARTH material they use the following: v_wtp <- seq(0, 30000, by = 1000)
CEAC_obj <- ceac(wtp = v_wtp, psa = l_PA)


# The below provides details on the regions of highest probability of cost-effectiveness for each strategy
summary(CEAC_obj)

# CEAC and cost-effectiveness acceptability frontier (CEAF) plot
plot(CEAC_obj)
```

## 09.4.3 Plot cost-effectiveness frontier

```{r}
plot(df_cea_PA)
```

## 09.4.4 Expected Loss Curves (ELCs)

The expected loss is the the quantification of the foregone benefits when choosing a suboptimal strategy given current evidence.

```{r}
elc_obj <- calc_exp_loss(wtp = v_wtp, psa = l_PA)
elc_obj
# ELC plot
plot(elc_obj, log_y = FALSE)
```

## 09.4.4 Expected value of perfect information (EVPI)

A value-of-information analysis estimates the expected value of perfect information (EVPI), that is,

Value of information is discussed in the York course and below:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\CDC\_Exclusive_Decision Modeling for Public Health_DARTH

<https://cran.r-project.org/web/packages/dampack/vignettes/voi.html>

There's also a paper on this here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.1.2 Distributions for parameters\\Betadist utility\\EVSI.doc

```{r}
# Expected value of perfect information (EVPI)
EVPI_obj <- calc_evpi(wtp = v_wtp, psa = l_PA)
# EVPI plot
plot(EVPI_obj, effect_units = "QALY")
```

## 09.4.5 Alternative plotting approaches:

You can find alternative plotting approaches here:

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\\September-Workshop-main\\September-Workshop-main\\analysis\\cSTM_time_dep_simulation.R

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\\September-Workshop-main\\September-Workshop-main\\manuscript\\cSTM_Tutorial_TimeDep.Rmd

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\\September-Workshop-main\\September-Workshop-main\\manuscript\\cSTM_Tutorial_TimeDep.R

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\\September-Workshop-main\\September-Workshop-main\\manuscript\\Appendix_code_walkthrough_Markov_Tutorial_Part2.Rmd

There may be information more generally in the R files found in: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R-HTA in LMICs Intermediate R-HTA Modelling Tutorial\\September-Workshop-main\\September-Workshop-main\\

## 08.3.3 Table Describing Parameters

To build a Table describing parameters, you click on the Table button at the top, and design it in there.
Then to include the parameter values automatically as they are updated you fill in the parameter name you want, highlight it in the cell and click \</\> from above to put it in a code block.
You do the same for gamma, etc., put this in code blocks also.

Per: [Putting the value of a variable into a table in R Markdown, rather than it's name - Stack Overflow](https://stackoverflow.com/questions/72902548/putting-the-value-of-a-variable-into-a-table-in-r-markdown-rather-than-its-nam)

Line 320 of this: <https://github.com/DARTH-git/cohort-modeling-tutorial-intro/blob/main/manuscript/cSTM_Tutorial_Intro.Rmd> is informative when viewing the final document on page 7 here: <https://arxiv.org/pdf/2001.07824.pdf>

This also includes within-cycle correction (WCC) using Simpson's 1/3 rule, which I probably don't want to bother applying, but at least I know it's an option.

Some helpful tips on using math notation in R Markdown.

<https://rpruim.github.io/s341/S19/from-class/MathinRmd.html>

This paper says in technical terms which distributions fit and why:

Model Parameter Estimation and Uncertainty Analysis: A Report of the ISPOR-SMDM Modeling Good Research Practices Task Force Working Group--6 <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

| Parameter                                            | Base Case Value       | Minimum Value | Maximum Value | Source        | Distribution |
|------------|------------|------------|------------|------------|------------|
| **Cost (Per Cycle)**                                 |                       |               |               |               |              |
| FOLFOX                                               | `c_PFS_Folfox`        |               |               |               |              |
| FOLFIRI                                              | `c_OS_Folfiri`        |               |               |               |              |
| Bevacizumab                                          | `c_PFS_Bevacizumab`   |               |               |               |              |
| Administration Cost                                  | `administration_cost` |               |               |               |              |
| **Adverse Event Cost**                               |                       |               |               |               |              |
| Leukopenia                                           | `c_AE1`               |               |               |               |              |
| Diarrhea                                             | `c_AE1`               |               |               |               |              |
| Vomiting                                             | `c_AE1`               |               |               |               |              |
|                                                      |                       |               |               |               |              |
| **Adverse Event Incidence - With Bevacizumab**       |                       |               |               |               |              |
| Leukopenia                                           | `p_FA1_Exp`           |               |               |               |              |
| Diarrhea                                             | `p_FA2_Exp`           |               |               |               |              |
| Vomiting                                             | `p_FA3_Exp`           |               |               |               |              |
| **Adverse Event Incidence - Without Bevacizumab**    |                       |               |               |               |              |
| Leukopenia                                           | `p_FA1_STD`           |               |               |               |              |
| Diarrhea                                             | `p_FA2_STD`           |               |               |               |              |
| Vomiting                                             | `p_FA3_STD`           |               |               |               |              |
| **Utility (Per Cycle)**                              |                       |               |               |               |              |
| Progression Free Survival                            | `u_F`                 |               |               |               |              |
| Overall Survival                                     | `u_P`                 |               |               |               |              |
| **Adverse Event Disutility**                         |                       |               |               |               |              |
| Leukopenia                                           | -0.45                 |               |               |               |              |
| Diarrhea                                             | -0.19                 |               |               |               |              |
| Vomiting                                             | -0.36                 |               |               |               |              |
| **Hazard Ratios**                                    |                       |               |               |               |              |
| PFS to OS under the Experimental Strategy            | `HR_FP_Exp`           |               |               | [@smeets2018] |              |
| OS to PFS under the Experimental Strategy            | `HR_PD_Exp`           |               |               | [@smeets2018] |              |
| **Probability of Dying under Second-Line Treatment** | `P_OSD_SoC`           |               |               |               |              |
| **Discount Rate**                                    |                       |               |               |               |              |
| Costs                                                | `d_c`                 |               |               |               |              |
| Outcomes                                             | `d_e`                 |               |               |               |              |

: Table X Model Parameters Values: Baseline, Ranges and Distributions for Sensitivity Analysis

## 09.4.6 Scenario Analysis

> The below study includes scenario analysis at the end:
>
> Rivera, F., Valladares, M., Gea, S., & López-Martínez, N.
> (2017).
> Cost-effectiveness analysis in the Spanish setting of the PEAK trial of panitumumab plus mFOLFOX6 compared with bevacizumab plus mFOLFOX6 for first-line treatment of patients with wild-type RAS metastatic colorectal cancer.
> Journal of Medical Economics, 20(6), 574-584.
> <https://sci-hub.st/10.1080/13696998.2017.1285780> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Evidence Synthesis\\Economic Models\\Rivera et al_2017_Cost-effectiveness analysis in the Spanish setting of the PEAK trial of.pdf

Scenario Analysis: "Often, uncertainty in a parameter may be represented by several discrete values, instead of a continuous range, sometimes called scenario analyses (e.g., evidence from clinical studies, utility surveys, or cost data sets may lead to different values). It is acceptable to report alternative outcomes under each of these discrete assumptions to complement other uncertainty analysis." - Per: Model Parameter Estimation and Uncertainty Analysis C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf

## 09.4.7 Calibration

> The following has a calibration section: Lawrence, D., Maschio, M., Leahy, K. J., Yunger, S., Easaw, J. C., & Weinstein, M. C.
> (2013).
> Economic analysis of bevacizumab, cetuximab, and panitumumab with fluoropyrimidine-based chemotherapy in the first-line treatment of KRAS wild-type metastatic colorectal cancer (mCRC).
> Journal of medical economics, 16(12), 1387-1398.
> <https://www.tandfonline.com/doi/pdf/10.3111/13696998.2013.852097?needAccess=true>
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Model Calibration in R

## References:

Generated automatically when the document is knitted.

Useful Darth publications to cite when using this code:

-   Jalal H, Pechlivanoglou P, Krijkamp E, Alarid-Escudero F, Enns E, Hunink MG. An Overview of R in Health Decision Sciences.
    Med Decis Making.
    2017; 37(3): 735-746.
    <https://journals.sagepub.com/doi/abs/10.1177/0272989X16686559>

-   Alarid-Escudero F, Krijkamp EM, Enns EA, Yang A, Hunink MGM Pechlivanoglou P, Jalal H. Cohort State-Transition Models in R: A Tutorial.
    arXiv:200107824v2.
    2020:1-48.
    <http://arxiv.org/abs/2001.07824>

-   Krijkamp EM, Alarid-Escudero F, Enns EA, Jalal HJ, Hunink MGM, Pechlivanoglou P. Microsimulation modeling for health decision sciences using R: A tutorial.
    Med Decis Making.
    2018;38(3):400--22.
    <https://journals.sagepub.com/doi/abs/10.1177/0272989X18754513>

-   Krijkamp EM, Alarid-Escudero F, Enns E, Pechlivanoglou P, Hunink MM, Jalal H. A Multidimensional Array Representation of State-Transition Model Dynamics.
    Med Decis Making.
    Online First <https://doi.org/10.1177/0272989X19893973>
