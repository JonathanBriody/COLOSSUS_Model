# "
# Maybe that's because the utility originally came from a year in the disease state, and we want it to reflect the proportion of a year that is our cycle length?
# So in the example t_cycle <- 1/4 # cycle length of 3 months (in years) - so maybe their utility originally came from a years utility and they wanted to decrease it to 3 months, aka the cycle lengths, utility.
# If that was the case it would be:
# A year of utility in this state is 0.75, so 3 months should be a quarter of this, should be 0.25 of this. So, t_cycle (0.25) * u_F (0.75) = 0.1875 i.e. your utility for 3 months if u_F is your utility for 12 months.
# v_tu_SoC <- m_M_SoC %*% c(u_F, u_P, u_D) * t_cycle
# I did the maths on his approach, and without *t_cycle the first cycle of each utility value is 0.8, but after *t_cycle it is 0.2, i.e. a quarter of what it was before. Which is why I think again that he is just making the yearly utility lower to match the 3 monthly cycles, i.e. quartering a yearly utility. A quarter of utility for a quarter of a year.
# So, I think he's just trying to generate the QALYs per cycle in both states.
# And slide 25 of C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models kind of proves that.
# In it they say: cycles are 6 months.
# Their R code in:
# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models\markov_smoking_deterministic.R
# Says:
# Now define the QALYS associated with the states per cycle
# QALY associated with 1 - year in the smoking state is Normal(mean = 0.95,  SD = 0.01)
# Divide by 2 as cycle length is 6 months
# state_qalys["Smoking"] <-  0.95 / 2 [I know they divide by 2 but we could have also multiplied by a half, i.e. *0.05].
# QALY associated with 1 - year in the not smoking state is 1 (no uncertainty)
# So all PSA samples have the same value
# Again divide by 2 as cycle length is 6 months
# state_qalys["Not smoking"] <-  1.0 / 2
# I like their approach, they define utility at the start, then they can do the following:
# Now use the cohort vectors to calculate the
# total QALYs for each cycle
# cycle_qalys[i_treatment, ] <-  cohort_vectors[i_treatment, , ] %*% state_qalys[]
# i.e., take the cohort (or Markov trace) for each of the treatment options and multiply it by the state qalys.
# So, I could take where he says: "QALY's are 2-dimensional in that they combine the duration of time and the health utility." as saying "we need to give the patient a utility for this health state that matches how long they were in this health state, i.e., if the utility of one year in this state is x and we know the patient was in this health state for 2 weeks, then we need to give them 2 weeks of x as their utility."
# Andrew calculated QALYs as:
#
# QALYs.SP0 <- trace.SP0%*%state.utilities
# QALYs.SP0
#
# undisc.QALYs.SP0 <- colSums(QALYs.SP0)
# undisc.QALYs.SP0
# However, he described the values that he started off with as:
# The quality of life utilities of a year spent in each of the model health states has been
# estimated to be 0.85, 0.3 and 0.75 for the Successful primary, Revision, and Successful revision health states respectively.
# and said the cycle length is one year, so maybe that's why he doesnt need to multiply by the time passed, because his utilities are for a year spent in this state, whereas when we start our utilies may not necessarily match.
# So, what I think this means is that provided utilities are for the period of the cycle we can do the below, and if they are not for the period of the cycle then we can just convert them like in the smoking file above and then apply them as though they are for the period of the cycle:
v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)
v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)
v_tu_SoC
v_tu_Exp
# The above displays the amount of QALY's gathered in each cycle.
# These are quality adjusted cycles, these are quality adjusted life years where cycles are annual, so I need to consider what this means for utility where cycles are monthly... I suppose it's just quality adjusted monthly utility.
# You can see above that there are no utility differences between the different treatments considered: c(u_H, u_S, u_D), it's just different utilities per the health states people are in.
# If we did want to do different utilities for the health state you are in per the treatment you are on, we could define this in the input parameters and then add this in above when creating the vector of utilities for that treatment.
# There's probably a more elegant way to do this, but if I wanted to add in the once off cost of the COSLOSSUS test, I could do something like the below:
# v_tc_trtA <-  v_tc_trtA+10
# v_tc_trtB <-  v_tc_trtB+100
# I just need to think about this and does it make sense, because it's adding to the cost each cycle, so probably the best indicator would be whether the cost outputted in 07.2 is only higher by the cost of the test when we do this, or if it is higher by a number of other costs, so I can check that below when adding costs in.
# Page 17 of An Introductory Tutorial on Cohort State-Transition Models in R Using a Cost-Effectiveness Analysis Example https://arxiv.org/pdf/2001.07824.pdf suggests that these are total costs expected per individual, and with testing being a one off cost, maybe you could just add it onto the costs you have at the end? Table 4: Total expected discounted QALYs and costs per average individual in the cohort of the Sick-Sicker model by strategy accounting for within-cycle correction. Costs QALYs Standard of care $151,580 20.711 Strategy A $284,805 21.499 Strategy B $259,100 22.184 Strategy AB $378,875 23.137 The total expected discounted QALYs and costs for the Sick-Sicker model under the four strategies accounting for within-cycle correction are shown in Table 4.
# Finally, we'll aggregate these costs and utilities into overall discounted mean (average) costs and utilities.
# Obtain the discounted costs and QALYs by multiplying the vectors of total cost and total utility we created above by the discount rate for each cycle:
# - note first the discount rate for each cycle needs to be defined accounting for the cycle length, as below:
v_dwc <- 1 / ((1 + d_c) ^ ((0:(n_cycle-1)) * t_cycle))
v_dwe <- 1 / ((1 + d_e) ^ ((0:(n_cycle-1)) * t_cycle))
# So, below we take the vector of costs, transposing it [the t() bit] to make it a 1 row matrix and using matrix multiplication [%*%] to multiply it by that discount factor vector, which is what you multiply by the outcome in each cycle to get the discounted value of the outcome for that cycle, and then it will all be summed all together across all cycles [across all cells of the 1 row matrix]. Giving you tc_d_SoC which is a scalar, or a single value, which is the lifetime expected cost for an average person under standard of care in this cohort.
# Discount costs by multiplying the cost vector with discount weights (v_dwc)
# tc_d_SoC  <-  t(v_tc_SoC)  %*% v_dwc
# tc_d_trtA <-  t(v_tc_trtA) %*% v_dwc
# tc_d_trtB <-  t(v_tc_trtB) %*% v_dwc
tc_d_SoC <-  t(v_tc_SoC) %*% v_dwc
tc_d_Exp <-  t(v_tc_Exp) %*% v_dwc
# So, now we have the average cost per person for treatment with standard of care, and the experimental treatment.
# Discount QALYS by multiplying the QALYs vector with discount weights (v_dwe) [probably utilities would be a better term here, as it's monthly health state quality of life, rather than yearly health state quality of life]
# tu_d_SoC  <-  t(v_tu_SoC)  %*% v_dwe
# tu_d_trtA <-  t(v_tu_trtA) %*% v_dwe
# tu_d_trtB <-  t(v_tu_trtB) %*% v_dwe
tu_d_SoC <-  t(v_tu_SoC) %*% v_dwe
tu_d_Exp <-  t(v_tu_Exp) %*% v_dwe
# Store them into a vector -> So, we'll take the single values for cost for an average person under standard of care and the experimental treatment and store them in a vector v_tc_d:
v_tc_d <- c(tc_d_SoC, tc_d_Exp)
v_tu_d <- c(tu_d_SoC, tu_d_Exp)
v_tc_d
v_tu_d
# To make things a little easier to read we might name these values what they are costs for, so we can use the vector of strategy names [v_names_str] to name the values:
names (v_tc_d) <- v_names_strats
v_tc_d
names (v_tu_d) <- v_names_strats
v_tu_d
# For utility, the utility values aren't different for the different states depending on the treatment strategy, i.e. SOC, Experimental Treatment, but the time spent in the states with the associated utility is different due to the treatment you're on, so your utility value will be higher if the treatment keeps you well for longer so that you stay in a higher utility state for longer than a lower utility state, i.e., progression.
# Dataframe with discounted costs and effectiveness
# So then we aggregate them into a dataframe with our discounted costs and utilities, and then we use this to calculate ICERs in: ## 07.3 Compute ICERs of the Markov model
# df_ce <- data.frame(Strategy = v_names_strats,
#                     Cost     = v_tc_d,
#                     Effect   = v_tu_d)
# df_ce
# The discounted costs and QALYs can be summarized and visualized using functions from the 'dampack' package
(df_cea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
effect     = c(tu_d_SoC, tu_d_Exp),
strategies = v_names_strats))
df_cea
# df_cea <- calculate_icers(cost       = df_ce$Cost,
#                           effect     = df_ce$Effect,
#                           strategies = df_ce$Strategy
#                           )
# df_cea
# The above uses the DARTHtools package to calculate our ICERS, incremental cost and incremental effectiveness, and also describes dominance status:
# This uses the "calculate_icers function", which does all the sorting, all the prioritization, and then computes the dominance, and not dominance, etc., and there's a publication on the methods behind this, based on a method from colleagues in Stanford.
# The default view is ordered by dominance status (ND = non-dominated, ED = extended/weak dominance, or D= strong dominance), and then ascending by cost per: https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html
# The icer object can be easily formatted into a publication quality table using the kableExtra package.
# library(kableExtra)
# library(dplyr)
# df_cea %>%
#  kable() %>%
#  kable_styling()
plot(df_cea, effect_units = "QALYs", label = "all")
# plot(df_cea, effect_units = "QALYs")
# When we plot it we have 2 strategies it is possible that something would be off the frontier, would be weakly dominated or strongly dominated, with just a few strategies it's not necessarily that impressive, but with lots of strategies then dampack can be helpful.
#Session buddy has all the tabs on tornado diagrams that I need to read saved.
# A CODE CHUNK WHERE IT IS EASY TO RUN ALL THE CODE IN ONE FILE, WHICH WILL BE TAKEN AND ADDED TO THE ACTUAL TORNADO CODE CHUNKS LOWER DOWN.
p_PD  <- 0.05
u_F <- 0.5
p_FD_SoC  <- 0.05
p_FD_Exp  <- 0.05
p_FA1_SoC  <- p_FA1_STD
p_FA2_SoC  <- p_FA2_STD
p_FA3_SoC  <- p_FA3_STD
# Because of how p_FA1_SoC is made, [i.e., p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD means that I will get a different value of p_FA1_SoC for each cycle based on multiplying a static value of p_FA1_STD by the varying value of p_PFS_SoC] I will get the following error if I include it as it was created. I can address this just like the hazard ratio problem, but at the moment I havent thought about probabilities and the whole (p_PFS_SoC) * p_FA1_STD part too deeply, so instead, I decide to make it non-varying by including just the static value that is multiplied by the changing p_PFS_SoC value as above. This all also holds for p_FA2_SoC and p_FA3_SoC.
# Error in data.frame(pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC",  :
#  arguments imply differing number of rows: 29, 386
# p_A1F_SoC = 0.01
p_A1D_SoC <- 0.00100000000000000000
source(file = "oncologySemiMarkov_function.R")
l_params_all <- list(
coef_weibull_shape_SoC = coef_weibull_shape_SoC,
coef_weibull_scale_SoC = coef_weibull_scale_SoC,
HR_FP_Exp = HR_FP_Exp,
HR_FP_SoC = HR_FP_SoC,
p_FD      = 0.02,
p_PD      = 0.05,
p_A1F_SoC = 0.999,
p_A1D_SoC = 0.00100000000000000000,
p_A2F_SoC = 0.999,
p_A2D_SoC = 0.01,
p_A3F_SoC = 0.999,
p_A3D_SoC = 0.00100000000000000000,
p_FD_SoC  = p_FD_SoC,
p_FD_Exp  = p_FD_Exp,
p_PD_SoC = p_PD_SoC,
p_PD_Exp = p_PD_Exp,
p_FA1_SoC = p_FA1_SoC,
p_FA2_SoC = p_FA2_SoC,
p_FA3_SoC = p_FA3_SoC,
c_F_SoC   = c_F_SoC,
c_F_Exp   = c_F_Exp,
c_P       = c_P,
c_D       = c_D,
c_AE1 = c_AE1,
c_AE2 = c_AE2,
c_AE3 = c_AE3,
u_F = u_F,
u_P = u_P,
u_D = u_D,
u_AE1 = u_AE1,
u_AE2 = u_AE2,
u_AE3 = u_AE3,
d_e       = d_e,
d_c       = d_c,
n_cycle   = n_cycle,
t_cycle   = t_cycle
)
#######################################################################
#    v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D) *t_cycle
#    v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D) *t_cycle
## ~~ In the example code, they multiply utilities by t_cycle to adjust the utility by the time spent in that state, this makes the utility values smaller in their code. In my own code I prefer to set the utility values at the start of the file, for the reasons I explain earlier in Markov_3state.Rmd, however, I will decrease the size of the utilies in my code temporarily so that I can get a box in the tornado diagram for hr_fp_soc and make other adjustments as necessary to the other parameters so that everything I'm including in my code will appear on the tornado diagram and then I will know that my code works properly, once I have actual values to plug in I will no longer do this. SO REMEMBER TO DELETE THESE EDITS TO PARAMETERS WHEN I HAVE MY FINAL VALUES OR THE COST-EFFECTIVENESS RESULTS WILL DIFFER TO THOSE IN THE PRE-SENSITIVITY ANALYSIS ABOVE!!!! ~~ ##
#######################################################################
# Test function
oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)
# I can't use these probabilities because they are from Free to Progressed, but now those probabilities are time-sensitive, so if I try to change these here and include them in the tornado diagram I'll be including too many things, as I explain in my description on hazard ratios.
# Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
# Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC
#
#
# Minimum_p_FP_Exp <- p_FP_Exp - 0.20*p_FP_Exp
# Maximum_p_FP_Exp <- p_FP_Exp + 0.20*p_FP_Exp
# Hazard Ratios:
HR_FP_Exp
Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp
HR_FP_SoC
Minimum_HR_FP_SoC <- HR_FP_SoC - 0.20*HR_FP_SoC
Maximum_HR_FP_SoC <- HR_FP_SoC + 0.20*HR_FP_SoC
# Probability of progressive disease to death:
p_PD
Minimum_p_PD <- p_PD - 0.20*p_PD
Maximum_p_PD <- p_PD + 0.20*p_PD
# Under the assumption that everyone will get the same second line therapy, I give them all the same probability of going from progessed (i.e., OS) to dead, and thus only need to include p_PD here once - because it is applied in oncologySemiMarkov_function.R for both SoC and Exp.
# Probability of going from PFS to Death states under the standard of care treatment and the experimental treatment:
p_FD_SoC
Minimum_p_FD_SoC <- p_FD_SoC - 0.20*p_FD_SoC
Maximum_p_FD_SoC <- p_FD_SoC + 0.20*p_FD_SoC
p_FD_Exp
Minimum_p_FD_Exp<- p_FD_Exp - 0.20*p_FD_Exp
Maximum_p_FD_Exp <- p_FD_Exp + 0.20*p_FD_Exp
# Probability of Adverse Events, from PFS to AE, from AE to PFS and from AE to Death:
# Although these probabilities say _SoC, I make the assumption that everyone has the same probability of AE1, 2 or 3 regardless of what treatment they are under (i.e., SoC or the Experimental). If I decide to get more complicated with treatment specific AE probabilities in the future I can update this to be _SoC and _Exp.
p_FA1_SoC
Minimum_p_FA1_SoC <- p_FA1_SoC - 0.20*p_FA1_SoC
Maximum_p_FA1_SoC <- p_FA1_SoC + 0.20*p_FA1_SoC
p_A1F_SoC
Minimum_p_A1F_SoC <- p_A1F_SoC - 0.20*p_A1F_SoC
Maximum_p_A1F_SoC <- p_A1F_SoC + 0.20*p_A1F_SoC
p_A1D_SoC
Minimum_p_A1D_SoC <- p_A1D_SoC - 0.20*p_A1D_SoC
Maximum_p_A1D_SoC <- p_A1D_SoC + 0.20*p_A1D_SoC
p_FA2_SoC
Minimum_p_FA2_SoC <- p_FA2_SoC - 0.20*p_FA2_SoC
Maximum_p_FA2_SoC <- p_FA2_SoC + 0.20*p_FA2_SoC
p_A2F_SoC
Minimum_p_A2F_SoC <- p_A2F_SoC - 0.20*p_A2F_SoC
Maximum_p_A2F_SoC <- p_A2F_SoC + 0.20*p_A2F_SoC
p_A2D_SoC
Minimum_p_A2D_SoC <- p_A2D_SoC - 0.20*p_A2D_SoC
Maximum_p_A2D_SoC <- p_A2D_SoC + 0.20*p_A2D_SoC
p_FA3_SoC
Minimum_p_FA3_SoC <- p_FA3_SoC - 0.20*p_FA3_SoC
Maximum_p_FA3_SoC <- p_FA3_SoC + 0.20*p_FA3_SoC
p_A3F_SoC
Minimum_p_A3F_SoC <- p_A3F_SoC - 0.20*p_A3F_SoC
Maximum_p_A3F_SoC <- p_A3F_SoC + 0.20*p_A3F_SoC
p_A3D_SoC
Minimum_p_A3D_SoC <- p_A3D_SoC - 0.20*p_A3D_SoC
Maximum_p_A3D_SoC <- p_A3D_SoC + 0.20*p_A3D_SoC
#install.packages("Rmpfr")
#library(Rmpfr)
#
# Minimum_p_A1D_SoC <- mpfr(Minimum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A1D_SoC <- mpfr(Maximum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
#
#
# Minimum_p_A2D_SoC <- mpfr(Minimum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A2D_SoC <- mpfr(Maximum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
#
# Minimum_p_A3D_SoC <- mpfr(Minimum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A3D_SoC <- mpfr(Maximum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default
# Cost:
# If I decide to include the cost of the test for patients I will also need to include this in the sensitivity analysis here:
c_F_SoC
Minimum_c_F_SoC <- c_F_SoC - 0.20*c_F_SoC
Maximum_c_F_SoC <- c_F_SoC + 0.20*c_F_SoC
c_F_Exp
Minimum_c_F_Exp  <- c_F_Exp - 0.20*c_F_Exp
Maximum_c_F_Exp  <- c_F_Exp + 0.20*c_F_Exp
c_P
Minimum_c_P  <- c_P - 0.20*c_P
Maximum_c_P  <- c_P + 0.20*c_P
c_D
Minimum_c_D  <- c_D - 0.20*c_D
Maximum_c_D  <- c_D + 0.20*c_D
c_AE1
Minimum_c_AE1  <- c_AE1 - 0.20*c_AE1
Maximum_c_AE1  <- c_AE1 + 0.20*c_AE1
c_AE2
Minimum_c_AE2  <- c_AE2 - 0.20*c_AE2
Maximum_c_AE2  <- c_AE2 + 0.20*c_AE2
c_AE3
Minimum_c_AE3  <- c_AE3 - 0.20*c_AE3
Maximum_c_AE3  <- c_AE3 + 0.20*c_AE3
# Utilities:
u_F
Minimum_u_F <- u_F - 0.20*u_F
Maximum_u_F <- u_F + 0.20*u_F
u_P
Minimum_u_P <- u_P - 0.20*u_P
Maximum_u_P <- u_P + 0.20*u_P
u_D
Minimum_u_D <- u_D - 0.20*u_D
Maximum_u_D <- u_D + 0.20*u_D
u_AE1
Minimum_u_AE1 <- u_AE1 - 0.20*u_AE1
Maximum_u_AE1 <- u_AE1 + 0.20*u_AE1
u_AE2
Minimum_u_AE2 <- u_AE2 - 0.20*u_AE2
Maximum_u_AE2 <- u_AE2 + 0.20*u_AE2
u_AE3
Minimum_u_AE3 <- u_AE3 - 0.20*u_AE3
Maximum_u_AE3 <- u_AE3 + 0.20*u_AE3
# Discount factor
# Cost Discount Factor
# Utility Discount Factor
d_e
Minimum_d_e <- d_e - 0.20*d_e
Maximum_d_e <- d_e + 0.20*d_e
d_c
Minimum_d_c <- d_c - 0.20*d_c
Maximum_d_c <- d_c + 0.20*d_c
df_params_OWSA <- data.frame(
pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A1D_SoC"),   # names of the parameters to be changed
min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_p_PD, Minimum_p_FD_SoC, Minimum_p_FD_Exp, Minimum_p_FA1_SoC, Minimum_p_A1F_SoC, Minimum_p_FA2_SoC, Minimum_p_A2F_SoC, Minimum_p_FA3_SoC, Minimum_p_A3F_SoC, Minimum_p_A1D_SoC),         # min parameter values
max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_p_PD, Maximum_p_FD_SoC, Maximum_p_FD_Exp, Maximum_p_FA1_SoC, Maximum_p_A1F_SoC, Maximum_p_FA2_SoC, Maximum_p_A2F_SoC, Maximum_p_FA3_SoC, Maximum_p_A3F_SoC, Maximum_p_A1D_SoC)          # max parameter values
)
# I made sure the names of the parameters to be varied and their mins and maxs are in the same order in all the brackets above in order to make sure that the min and max being applied are the min and the max of the parameter I want to consider a min and a max for.
OWSA_NMB  <- run_owsa_det(
# Arguments:
params_range     = df_params_OWSA,     # dataframe with parameters for OWSA
params_basecase  = l_params_all,       # list with all parameters
# params_basecase
nsamp            = 100,                # number of parameter values
# nsamp
FUN              = oncologySemiMarkov, # function to compute outputs
# FUN
outcomes         = c("NMB"),           # output to do the OWSA on
# outcomes
strategies       = v_names_strats,       # names of the strategies
progress = TRUE,
n_wtp            = 20000               # extra argument to pass to FUN to specify the willingness to pay
)
owsa_tornado(owsa = OWSA_NMB, txtsize = 11)
#Session buddy has all the tabs on tornado diagrams that I need to read saved.
# A CODE CHUNK WHERE IT IS EASY TO RUN ALL THE CODE IN ONE FILE, WHICH WILL BE TAKEN AND ADDED TO THE ACTUAL TORNADO CODE CHUNKS LOWER DOWN.
p_PD  <- 0.05
u_F <- 0.5
p_FD_SoC  <- 0.05
p_FD_Exp  <- 0.05
p_FA1_SoC  <- p_FA1_STD
p_FA2_SoC  <- p_FA2_STD
p_FA3_SoC  <- p_FA3_STD
# Because of how p_FA1_SoC is made, [i.e., p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD means that I will get a different value of p_FA1_SoC for each cycle based on multiplying a static value of p_FA1_STD by the varying value of p_PFS_SoC] I will get the following error if I include it as it was created. I can address this just like the hazard ratio problem, but at the moment I havent thought about probabilities and the whole (p_PFS_SoC) * p_FA1_STD part too deeply, so instead, I decide to make it non-varying by including just the static value that is multiplied by the changing p_PFS_SoC value as above. This all also holds for p_FA2_SoC and p_FA3_SoC.
# Error in data.frame(pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC",  :
#  arguments imply differing number of rows: 29, 386
# p_A1F_SoC = 0.01
# p_A1D_SoC <- 0.00100000000000000000
source(file = "oncologySemiMarkov_function.R")
l_params_all <- list(
coef_weibull_shape_SoC = coef_weibull_shape_SoC,
coef_weibull_scale_SoC = coef_weibull_scale_SoC,
HR_FP_Exp = HR_FP_Exp,
HR_FP_SoC = HR_FP_SoC,
p_FD      = 0.02,
p_PD      = 0.05,
p_A1F_SoC = 0.999,
p_A1D_SoC = 0.00100000000000000000,
p_A2F_SoC = 0.999,
p_A2D_SoC = 0.00100000000000000000,
p_A3F_SoC = 0.999,
p_A3D_SoC = 0.00100000000000000000,
p_FD_SoC  = p_FD_SoC,
p_FD_Exp  = p_FD_Exp,
p_PD_SoC = p_PD_SoC,
p_PD_Exp = p_PD_Exp,
p_FA1_SoC = p_FA1_SoC,
p_FA2_SoC = p_FA2_SoC,
p_FA3_SoC = p_FA3_SoC,
c_F_SoC   = c_F_SoC,
c_F_Exp   = c_F_Exp,
c_P       = c_P,
c_D       = c_D,
c_AE1 = c_AE1,
c_AE2 = c_AE2,
c_AE3 = c_AE3,
u_F = u_F,
u_P = u_P,
u_D = u_D,
u_AE1 = u_AE1,
u_AE2 = u_AE2,
u_AE3 = u_AE3,
d_e       = d_e,
d_c       = d_c,
n_cycle   = n_cycle,
t_cycle   = t_cycle
)
#######################################################################
#    v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D) *t_cycle
#    v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D) *t_cycle
## ~~ In the example code, they multiply utilities by t_cycle to adjust the utility by the time spent in that state, this makes the utility values smaller in their code. In my own code I prefer to set the utility values at the start of the file, for the reasons I explain earlier in Markov_3state.Rmd, however, I will decrease the size of the utilies in my code temporarily so that I can get a box in the tornado diagram for hr_fp_soc and make other adjustments as necessary to the other parameters so that everything I'm including in my code will appear on the tornado diagram and then I will know that my code works properly, once I have actual values to plug in I will no longer do this. SO REMEMBER TO DELETE THESE EDITS TO PARAMETERS WHEN I HAVE MY FINAL VALUES OR THE COST-EFFECTIVENESS RESULTS WILL DIFFER TO THOSE IN THE PRE-SENSITIVITY ANALYSIS ABOVE!!!! ~~ ##
#######################################################################
# Test function
oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)
# I can't use these probabilities because they are from Free to Progressed, but now those probabilities are time-sensitive, so if I try to change these here and include them in the tornado diagram I'll be including too many things, as I explain in my description on hazard ratios.
# Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
# Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC
#
#
# Minimum_p_FP_Exp <- p_FP_Exp - 0.20*p_FP_Exp
# Maximum_p_FP_Exp <- p_FP_Exp + 0.20*p_FP_Exp
# Hazard Ratios:
HR_FP_Exp
Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp
HR_FP_SoC
Minimum_HR_FP_SoC <- HR_FP_SoC - 0.20*HR_FP_SoC
Maximum_HR_FP_SoC <- HR_FP_SoC + 0.20*HR_FP_SoC
# Probability of progressive disease to death:
p_PD
Minimum_p_PD <- p_PD - 0.20*p_PD
Maximum_p_PD <- p_PD + 0.20*p_PD
# Under the assumption that everyone will get the same second line therapy, I give them all the same probability of going from progessed (i.e., OS) to dead, and thus only need to include p_PD here once - because it is applied in oncologySemiMarkov_function.R for both SoC and Exp.
# Probability of going from PFS to Death states under the standard of care treatment and the experimental treatment:
p_FD_SoC
Minimum_p_FD_SoC <- p_FD_SoC - 0.20*p_FD_SoC
Maximum_p_FD_SoC <- p_FD_SoC + 0.20*p_FD_SoC
p_FD_Exp
Minimum_p_FD_Exp<- p_FD_Exp - 0.20*p_FD_Exp
Maximum_p_FD_Exp <- p_FD_Exp + 0.20*p_FD_Exp
# Probability of Adverse Events, from PFS to AE, from AE to PFS and from AE to Death:
# Although these probabilities say _SoC, I make the assumption that everyone has the same probability of AE1, 2 or 3 regardless of what treatment they are under (i.e., SoC or the Experimental). If I decide to get more complicated with treatment specific AE probabilities in the future I can update this to be _SoC and _Exp.
p_FA1_SoC
Minimum_p_FA1_SoC <- p_FA1_SoC - 0.20*p_FA1_SoC
Maximum_p_FA1_SoC <- p_FA1_SoC + 0.20*p_FA1_SoC
p_A1F_SoC
Minimum_p_A1F_SoC <- p_A1F_SoC - 0.20*p_A1F_SoC
Maximum_p_A1F_SoC <- p_A1F_SoC + 0.20*p_A1F_SoC
p_A1D_SoC
Minimum_p_A1D_SoC <- p_A1D_SoC - 0.20*p_A1D_SoC
Maximum_p_A1D_SoC <- p_A1D_SoC + 0.20*p_A1D_SoC
p_FA2_SoC
Minimum_p_FA2_SoC <- p_FA2_SoC - 0.20*p_FA2_SoC
Maximum_p_FA2_SoC <- p_FA2_SoC + 0.20*p_FA2_SoC
p_A2F_SoC
Minimum_p_A2F_SoC <- p_A2F_SoC - 0.20*p_A2F_SoC
Maximum_p_A2F_SoC <- p_A2F_SoC + 0.20*p_A2F_SoC
p_A2D_SoC
Minimum_p_A2D_SoC <- p_A2D_SoC - 0.20*p_A2D_SoC
Maximum_p_A2D_SoC <- p_A2D_SoC + 0.20*p_A2D_SoC
p_FA3_SoC
Minimum_p_FA3_SoC <- p_FA3_SoC - 0.20*p_FA3_SoC
Maximum_p_FA3_SoC <- p_FA3_SoC + 0.20*p_FA3_SoC
p_A3F_SoC
Minimum_p_A3F_SoC <- p_A3F_SoC - 0.20*p_A3F_SoC
Maximum_p_A3F_SoC <- p_A3F_SoC + 0.20*p_A3F_SoC
p_A3D_SoC
Minimum_p_A3D_SoC <- p_A3D_SoC - 0.20*p_A3D_SoC
Maximum_p_A3D_SoC <- p_A3D_SoC + 0.20*p_A3D_SoC
#install.packages("Rmpfr")
#library(Rmpfr)
#
# Minimum_p_A1D_SoC <- mpfr(Minimum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A1D_SoC <- mpfr(Maximum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
#
#
# Minimum_p_A2D_SoC <- mpfr(Minimum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A2D_SoC <- mpfr(Maximum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
#
# Minimum_p_A3D_SoC <- mpfr(Minimum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A3D_SoC <- mpfr(Maximum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default
# Cost:
# If I decide to include the cost of the test for patients I will also need to include this in the sensitivity analysis here:
c_F_SoC
Minimum_c_F_SoC <- c_F_SoC - 0.20*c_F_SoC
Maximum_c_F_SoC <- c_F_SoC + 0.20*c_F_SoC
c_F_Exp
Minimum_c_F_Exp  <- c_F_Exp - 0.20*c_F_Exp
Maximum_c_F_Exp  <- c_F_Exp + 0.20*c_F_Exp
c_P
Minimum_c_P  <- c_P - 0.20*c_P
Maximum_c_P  <- c_P + 0.20*c_P
c_D
Minimum_c_D  <- c_D - 0.20*c_D
Maximum_c_D  <- c_D + 0.20*c_D
c_AE1
Minimum_c_AE1  <- c_AE1 - 0.20*c_AE1
Maximum_c_AE1  <- c_AE1 + 0.20*c_AE1
c_AE2
Minimum_c_AE2  <- c_AE2 - 0.20*c_AE2
Maximum_c_AE2  <- c_AE2 + 0.20*c_AE2
c_AE3
Minimum_c_AE3  <- c_AE3 - 0.20*c_AE3
Maximum_c_AE3  <- c_AE3 + 0.20*c_AE3
# Utilities:
u_F
Minimum_u_F <- u_F - 0.20*u_F
Maximum_u_F <- u_F + 0.20*u_F
u_P
Minimum_u_P <- u_P - 0.20*u_P
Maximum_u_P <- u_P + 0.20*u_P
u_D
Minimum_u_D <- u_D - 0.20*u_D
Maximum_u_D <- u_D + 0.20*u_D
u_AE1
Minimum_u_AE1 <- u_AE1 - 0.20*u_AE1
Maximum_u_AE1 <- u_AE1 + 0.20*u_AE1
u_AE2
Minimum_u_AE2 <- u_AE2 - 0.20*u_AE2
Maximum_u_AE2 <- u_AE2 + 0.20*u_AE2
u_AE3
Minimum_u_AE3 <- u_AE3 - 0.20*u_AE3
Maximum_u_AE3 <- u_AE3 + 0.20*u_AE3
# Discount factor
# Cost Discount Factor
# Utility Discount Factor
d_e
Minimum_d_e <- d_e - 0.20*d_e
Maximum_d_e <- d_e + 0.20*d_e
d_c
Minimum_d_c <- d_c - 0.20*d_c
Maximum_d_c <- d_c + 0.20*d_c
df_params_OWSA <- data.frame(
pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A1D_SoC"),   # names of the parameters to be changed
min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_p_PD, Minimum_p_FD_SoC, Minimum_p_FD_Exp, Minimum_p_FA1_SoC, Minimum_p_A1F_SoC, Minimum_p_FA2_SoC, Minimum_p_A2F_SoC, Minimum_p_FA3_SoC, Minimum_p_A3F_SoC, Minimum_p_A1D_SoC),         # min parameter values
max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_p_PD, Maximum_p_FD_SoC, Maximum_p_FD_Exp, Maximum_p_FA1_SoC, Maximum_p_A1F_SoC, Maximum_p_FA2_SoC, Maximum_p_A2F_SoC, Maximum_p_FA3_SoC, Maximum_p_A3F_SoC, Maximum_p_A1D_SoC)          # max parameter values
)
# I made sure the names of the parameters to be varied and their mins and maxs are in the same order in all the brackets above in order to make sure that the min and max being applied are the min and the max of the parameter I want to consider a min and a max for.
OWSA_NMB  <- run_owsa_det(
# Arguments:
params_range     = df_params_OWSA,     # dataframe with parameters for OWSA
params_basecase  = l_params_all,       # list with all parameters
# params_basecase
nsamp            = 100,                # number of parameter values
# nsamp
FUN              = oncologySemiMarkov, # function to compute outputs
# FUN
outcomes         = c("NMB"),           # output to do the OWSA on
# outcomes
strategies       = v_names_strats,       # names of the strategies
progress = TRUE,
n_wtp            = 20000               # extra argument to pass to FUN to specify the willingness to pay
)
owsa_tornado(owsa = OWSA_NMB, txtsize = 11)
sink("output.txt", split=TRUE)
savehistory("C:/Users/Jonathan/OneDrive - Royal College of Surgeons in Ireland/COLOSSUS/R Code/GitHub/COLOSSUS_Model/history.Rhistory")
