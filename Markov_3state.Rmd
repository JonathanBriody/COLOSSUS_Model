---
title: 'Exploratory Cost-Effectiveness Analysis of Treatment Based on New Diagnostic Tests vs Standard of Care in Microsatellite Stable RAS Mutant Metastatic Colorectal Cancer Patients'
author: "Jonathan Briody (1) | Kathleen Bennett (1) | Lesley Tilson (2)"
output:
  pdf_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: sentence
bibliography: references.bib
---

(1) Data Science Centre, RCSI University of Medicine and Health Sciences, Dublin, Ireland
(2) National Centre for Pharmacoeconomics, St James Hospital, Dublin 8, Ireland

IF THIS BECOMES THE BEVACIZUMAB STUDY WE CAN INCOPORATE SOME OF THE BELOW:

<https://www.angiopredict.com/go/about_us>

Smeets, D., Miller, I. S., O'Connor, D. P., Das, S., Moran, B., Boeckx, B., ... & Byrne, A. T.
(2018).
Copy number load predicts outcome of metastatic colorectal cancer patients receiving bevacizumab combination therapy.
*Nature communications*, *9*(1), 1-16.

# **Abstract:**

> I'll initially draft this as informed by:
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> Clinical colorectal cancer, 13(4), 219-225.
> <https://www.sciencedirect.com/science/article/abs/pii/S1533002814000978>
>
> **[TICK HERE:]**
>
> and:
>
> Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> Journal of Medical Economics, 24(1), 339-344.
> <https://www.tandfonline.com/doi/pdf/10.1080/13696998.2021.1888743>
>
> **[TICK HERE:]**
>
> and:
>
> Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
> (2019).
> Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
> Journal of Medical Economics, 22(2), 163-168.
> <https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>
>
> **[TICK HERE:]**
>
> and here:
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Ramalingam, S. S., ... & Flowers, C. R.
> (2015).
> Necitumumab in metastatic squamous cell lung cancer: establishing a value-based cost.
> JAMA oncology, 1(9), 1293-1300.
>
> Paper saved here: <file:///C:/Users/Jonathan/Dropbox/PC/Downloads/13%20Goldstein.pdf> and supplementary material saved here: <file:///C:/Users/Jonathan/Dropbox/PC/Downloads/Neci%20supplemental%20material%20071815.pdf>
>
> **[TICK HERE:]**
>
> and not forgetting I have some relevant stuff in Grammarly
>
> **[TICK HERE:]**
>
> Then I'll have a look at the other open journal articles in my browser windows
>
> **[TICK HERE:]**

**Background:** Colorectal cancer is the third most common cancer in Europe.
Microsatellite stable RAS mutant metastatic colorectal cancer (MSS RAS mt mCRC) [some details about this specific cancer]....
**Objectives:** The objective of this study was to compare the cost-effectiveness of a personalised medicine approach (evaluation of predictive biomarkers for targeted therapy) with standard of care treatment FOLFOX (5-FU, leucovorin, and oxaliplatin) in patients with MSS RAS mt mCRC in Germany, Ireland and Spain.

**Materials and Methods:** We developed a three-state Markov model with monthly cycles to estimate the incremental cost-effectiveness ratio (ICER) of biomarkers for targeted therapies versus FOLFOX for patients with MSS RAS mt mCRC from a healthcare payer perspective in the countries under study (Germany, Ireland and Spain).
Progression risks, cause-specific mortality and probabilities of adverse events were taken from the literature.
Costs for treatment administration and management of adverse events were derived from published data and survey of consortium hospitals in the participating countries.
Utilities applied to calculate the quality-adjusted life years (QALYs) were obtained from a review of the literature [I may want to return to this to update this with the country specific utility plan myself and Kathleen made].
To test the robustness of the results one-way sensitivity analysis and probabilistic sensitivity analysis was applied.

**Results:** Predictive biomarkers with targeted therapy provided X QALYS at a cost of €X,000 compared with standard of care (FOLFOX without testing).
Compared to standard of care, the ICER of predictive biomarkers with targeted therapy provided was EUR X,000/QALY from a European health care perspective, which was above the threshold used to define cost-effectiveness in the countries studied, which provided X QALYS at a cost of €X,000.
The incremental cost-effectiveness ratio (ICER) was €X,000 per QALY.
The ICER was below the cost-effectiveness threshold (\<€X,000 per QALY) in all univariate and multivariate sensitivity analyses.
One-way sensitivity analysis demonstrated that the initial results were generally robust.
Cost-effectiveness acceptability curves from the probabilistic sensitivity analysis show that the probability that predictive biomarkers with targeted therapy was cost-effective was 100% when the threshold was \<€X,000 per QALY.

**Conclusion:** When compared to standard of care, predictive biomarkers with targeted therapy is cost effective for patients with MSS RAS mt mCRC in Germany, Ireland and Spain at a €X,000 per QALY threshold.
However, because of the exploratory nature of this analysis, the cost effectiveness profile of these biomarkers should be evaluated in further comparative studies.

**Keywords:** Colorectal neoplasms, cost effectiveness analysis, Quality-adjusted life years, Markov model, mCRC, precision medicine, biomarkers, assays,

JEL CLASSIFICATION CODES

I19, I1, I, I11, I1, I,

[*Possible Journal Homes:*]{.ul}

Value in Health: BEST POSSIBLE HOME, HIGHEST RATED JOURNAL, FOCUSED ON CEA, ETC.,

Journal of Medical Economics <https://www.tandfonline.com/doi/pdf/10.1080/13696998.2021.1888743>

Clinical Colorectal Cancer <https://www-sciencedirect-com.proxy.library.rcsi.ie/science/article/pii/S1533002814000978>

Studies in Health Technology andInformatics

Search "New Journal: Oxford Open Economics" in my gmail, it's a new Journal and thus may be interested in this work.

We can increase our publication numbers by publishing a short summary of our study in: PharmacoEconomics & Outcomes News - per the description "Stay up to date with the exciting world of health economics. Solve the problem of reading and monitoring the vast volumes of literature necessary to stay informed on the latest issues in pharmacoeconomics and outcomes research by reading our concise news summaries, compiled in an easy-to-read format." <https://www.springer.com/journal/40274>

**Correspondence:**

Jonathan Briody, PhD, RCSI University of Medicine and Health Sciences, Beaux Lane House, Lower Mercer St, Dublin 2, Ireland.
Email: [jonathanbriody\@rcsi.ie](mailto:jonathanbriody@rcsi.ie){.email}

**Funding Information:**

This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 754923.
The material presented and views expressed here are the responsibility of the author(s) only.
The EU Commission takes no responsibility for any use made of the information set out.

\newpage

# Introduction

We used the World Health Organization's Choosing Interventions that are Cost--Effective (WHO-CHOICE) suggestion that "interventions for a given country or region that cost less than three times average per capita income per disability-adjusted life-year (DALY) averted are considered costeffective; and those that exceed this level are considered not cost-effective" (estimation from CAPMAS)29.
The three times GDP per capita in Egypt in 2017 was EGP106,000, which corresponds to an incremental cost-effectiveness ratio (ICER) threshold of USD 41,372 (2017)30.
Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
(2019).
Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
Journal of Medical Economics, 22(2), 163-168.
<https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>

Maybe for \*\*Germany\*\*, where they don't appear to have a cost-effectiveness threshold, I can use the GDP per capita approach I've read about (somewhere) or some range of this.
Rough description of this here: Marseille, E., Larson, B., Kazi, D. S., Kahn, J. G., & Rosen, S.
(2014).
Thresholds for the cost--effectiveness of interventions: alternative approaches.
Bulletin of the World Health Organization, 93, 118-124.
<https://www.who.int/bulletin/volumes/93/2/14-138206/en/> Health Opportunity cost may also factor into this: Ochalek, J., Lomas, J., & Claxton, K.
(2018).
Estimating health opportunity costs in low-income and middle-income countries: a novel approach and evidence from cross-country data.
BMJ Global Health, 3(6).
<https://gh.bmj.com/content/3/6/e000964>

With almost 2 million new cases diagnosed annually, colorectal cancer is the third most commonly diagnosed cancer in men, the second most commonly diagnosed in women [@vabi2021], and the second leading cause of cancer deaths worldwide [@nwaokorie2021].
Following advances in primary and adjuvant treatments, survival time continues to improve in colorectal cancer, with a 5-year survival rate of 64%, however, this is not reflected in metastatic colorectal cancer (mCRC), which has a dramatically lower 5-year survival rate of just 12% [@siegel2019].

As one of the most commonly mutated oncogenes in cancer, RAS mutations appear in around half of mCRC patients, and in microsatellite stable (MSS) mCRC are associated with more aggressive tumor biology and poorer prognosis [@patelli2021].
Current front-line treatment for mCRC does not differ significantly from colorectal cancer regardless of mutational status, comprising of 5-fluoruracil (5-FU)-based standard of care chemotherapy (e.g. FOLFOX, FOLFIRI, FOLFOXFIRI) in combination with bevacizumab.
However, there is emerging evidence that targeted therapy may prolong overall survival for mCRC patients, as previously seen in non-metastatic colorectal cancer [@xie2020].

Cancerous cells may be addressed by targeted therapy which directly inhibits cell proliferation, differentiation, and migration.
Similarly, the micro-environment of the tumor, including it's local blood vessels and immune cells, may be modified in order to disrupt growth and to improve immune surveillance and response [@tiwari2018].
Beginning with Cetuximab in 2004, numerous targeted therapies for colorectal cancer have successfully come to market in the last two decades (Bevacizumab, Panitumumab, Ziv-aflibercept, Regorafenib, Ramucirumab, Pembrolizumab, Bivolumabm Ipilimumab, etc.,).
The number of pathways offering possible opportunities for targeted therapy is similarly extensive, including epidermal growth factor receptor (EGFR), vascular endothelial growth factor/vascular endothelial growth factor receptor (VEGF/VEGFR), and hepatocyte growth factor/mesenchymal--epithelial transition factor (HGF/c-MET), among others [@xie2020].

*DRUG X* is a *DETAILS ABOUT DRUG X HERE* that *HOW DRUG X WORKS HERE* [citation to support this].
This experimentally identified targeted agent is in preclinical status or in phase I trials, but has been proven to be efficient in clinical studies [amend if this isnt the case], and ... [citations to support this].
The results of X demonstrate a median overall survival (OS) benefit of X months (Y months vs. Z months) and a median progression-free survival (PFS) benefit of X months for DRUG X when compared with placebo (X months vs. X months).

> Fruquintinib is a VEGFR inhibitor that blocks new blood vessel growth associated with tumor proliferation5 .
> In 2018, it was approved to treat Chinese patients with mCRC who had experienced failure of second-line treatment.
> The results of the FRESCO trial demonstrated a median overall survival (OS) benefit of 2.7 months (9.3 months vs. 6.6 months) and a median progression-free survival (PFS) benefit of 1.9 months for fruquintinib when compared with placebo (3.7 months vs. 1.8 months)6.
>
> \- Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.

Because targeted therapy is most associated with prolonged survival, rather than cure, the cost--effectiveness of such therapy must be understood when compared to current standard of care chemotherapy, which is assumed to be significantly less costly than producing targeted agents and testing necessary genetic markers, particularly under the provision of multiple targeted agents [@dekker2019].

Limited data exist regarding these novel therapies, with no research comparing the cost-effectiveness of these targeted therapies with standard of care, thus, we developed a general decision-analytic framework applied to mCRC to model the cost-effectiveness of biomarkers for targeted therapies in mCRC.
We constructed a Markov model using a hypothetical cohort of patients with mCRC to conduct an exploratory cost-effectiveness analysis of targeted treatment based on new diagnostic tests vs standard of care in MSS RAS mt mCRC patients.

# Materials and Methods

#### Model Structure

We constructed Markov models to calculate the lifetime costs and outcomes of each treatment strategy.
As per Figure 1, in this model we established the following health states: (1) first line treatment prior to progression; (2) second line treatment following progression; (3) death.
We applied published estimates of costs associated with each health state in the countries studied [CITE].
All cost estimates were in 2022 Euros [maybe discuss how you converted to this?
HIQA has something on this I think].
To apply country-specific state utility values we...DETAIL HERE THE APPROACH WE TOOK AS DISCUSSED WITH KATHLEEN [CITE].
The model was programmed in R 4.1.2 using RStudio 2021.09.2+382 and was informed by the state-transition model framework developed by the Decision Analysis in R for Technologies in Health (DARTH) workgroup [@alarid-escudero2021] [@jalal2017] [@krijkamp2018] [@krijkamp2020].
The Markov model simulated the health state transitions following a diagnosis of metastatic cancer.
A cycle length of 1 month was chosen, per the time between administration of chemotherapy doses.
A 5-year time horizon was also chosen, this was appropriate to estimate life expectancy in this patient population [CITE].
For standard of care, we considered 2 lines of treatment, first line (FOLFOX) and second line (FOLFIRI).
For the comparator we also considered 2 lines of treatment (NEW THERAPY), then (FOLFIRI).
In both instances we made use of treatment-specific utility values.
At the end of each cycle, patients remained in their current health state, or transitioned to a more progressive state.
To reflect that transitions between states may occur at any point within each cycle, we applied a half-cycle correction to calculating life-years (LYs) {I AM DEFINITELY NOT GOING TO DO THIS, I JUST PUT IT IN HERE AS SOMETHING TO THINK ABOUT, CYCLES ARE ONLY A MONTH SO A CORRECTION OF 2 WEEKS IS PRETTY STUPID}.

> [A three-state Markov model (Figure 1) with monthly cycle could also have been used to represent the progression of mCRC, including PFS, progressive disease (PD), and death.
> All patients entered the model either receiving FOLFOX therapy or NOVEL therapy.
> After treatment discontinuation due to progession, best supportive care (BSC) was modeled.
> \*\*--> So, here even doing that they still got a treatment that needs to be modelled in PFS].\*\*
>
> Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.

The intention-to-treat (ITT) population we modeled in this study is MSS RAS mt mCRC patients in Germany, Ireland and Spain, from a healthcare payer perspective in these countries.
Subsequently the analysis only included direct medical costs.
Effectiveness is reflected in life-years (LYs) and quality adjusted life years (QALYS).
Per the countries studied, a discount rate of 4% per annum was applied to cost input and health output parameters [citation here].
The primary outcome of the model was the total direct costs, LYs, QALYS, and the incremental cost-effectiveness ratio (ICER).

#### Patients and Treatment Regimens

In the absence of a randomised controlled trial making direct comparisons between standard of care and the proposed novel therapy, it was necessary to perform an indirect comparison using the data from two published studies [CITE].
We made use of published results from one study to estimate the clinical course for patients treated under standard of care and another to estimate outcomes for patients under novel therapy [@goldstein2014].
The standard of care arm was based on a study be ..et al., {FILL IN THE BLANKS HERE ON THE STUDY YOU BASED THIS ON} who compared...[cite].
The novel therapy arm was based on a X, Y, Z, study by ....et al., While these 2 studies did not have identical target populations...

> The PK arm was based on a phase II nonrandomized study by Capitain et al., who compared PK FOLFOX with BSA FOLFOX.9 The BSA arm was based on a phase III randomized study by Tournigand et al.14 Although the 2 studies did not have identical target populations, the survival outcomes of the baseline BSA arm of the 2 studies are comparable (Table 1).9,14,15 Moreover, the median PFS and OS were also similar to the FOLFOX arm in a similar study.15
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> Given that there was no single RCT with all treatment arms, it was necessary to perform an indirect comparison using the data from the FRESCO trial and the CONCUR trial6, 11.
> The baseline treatment for the indirect comparison was BSC taken from the FRESCO trial.
> The clinical effectiveness in patients in the BSC arm was also available from the CONCUR trial.
> This, however, was not considered appropriate for the baseline treatment because the survival curves reported by the CONCUR trial were distorted in the paper and were not suitable for recreating individual patient data.
> However, the hazard ratios (HRs) calculated by original data would not be affected by the distorted figures.
> The implicit assumption underlying the indirect comparison was that the baseline patient characteristics in the two studies were reasonably similar, which was shown to be true (Appendix Table 1).

[I THINK WE CAN TAKE THEIR APPROACH IN OUR MODEL, DO DIGITISING AND PARAMETRIC SURVIVAL ANALYSIS FOR STANDARD OF CARE, THEN TAKE A HAZARD RATIO FROM ANNETTE AND THE COLOSSUS STUDY FOR THE NEW INTERVENTION, THERE'S A LITTLE MORE ON HR'S BELOW and the following R code applies hazard ratios after doing parametric survival analysis, similar to what I plan to do: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\ISPOR WEBINAR Health Economic Modelling in R\\ISPOR_webinar_R-master\\ISPOR_webinar_R-master\\oncologySemiMarkov_illustration.R].

> The FRESCO trial was used to model the probabilities of remaining in the PFS state or experiencing disease progression in the fruquintinib arm.
> This research recreating individual patients data (IPD) from Kaplan--Meier (KM) survival curves in FRESCO by the method published by Guyot et al.12.
> To predict the events of death and progression beyond the observation period, several parametric models were fit in different distributions, including exponential, Weibull, Gompertz, generalized gamma log-normal and log-logistic.
> After comparing the goodness-of-fit of different parametric models by the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) (Appendix Table 2), assessing the conformities of parametric models to the original KM survival curves in the FRESCO trial and considering about proportional hazard model used for the indirect comparison, the Weibull distribution model was chosen for the base case analysis (Appendix Figure 1 and Appendix Figure 2).
>
> We used the PFS curves to estimate the transition probability from PFS state to PFS state and the transition probability from PFS state to another states (PD and death) and used the OS curve to estimate the overall mortality.
> The model assumed the transition probability from PFS state to death was estimated by the background mortality obtained from Chinese government data, which based on actuarial estimates of mortality among the general population in China.
> According to the background mortality and the transition probability from PFS state to another states (PD and death), we calculated the transition probability from PFS state and PD.
> Moreover, the transition probability from PD state to PD state could calculated by overall mortality and background mortality.
>
> We also calculated the transition probabilities in the BSC arm by a Weibull-distribution hazard function using the survival data from the FRESCO trial.
> The model assumed that the survival curves of BSC both in FRESCO and CONCUR would be identical if the patients in BSC arm was identical, and the HR of regorafenib versus BSC would not change even if the patients receiving BSC in CONCUR became exactly same as the patients receiving BSC in FRESCO.
> Hence,we adjusted the transition probabilities of the regorafenib arm using the transition probabilities of the BSC arm modeled from the FRESCO trial and the HR of regorafenib versus BSC taken from the CONCUR trial.
>
> Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.

> There is a discussion on indirect comparisons and how best to do these in this short piece: Martin-Broto, J.
> (2015).
> Indirect comparisons in cost-effectiveness analysis: are we being naïve?.
> Clinical and Translational Oncology, 17(1), 85-86.
> <https://sci-hub.se/10.1007/s12094-014-1256-9>
>
> This also talks about indirect comparisons Hollmann, S., Alloul, K., Attard, C., & Kavan, P. (2014).
> An Indirect Treatment Comparison and Cost-Effectiveness Analysis Comparing Folfirinox with Nab-Paclitaxel Plus Gemcitabine for First-Line Treatment for Patients with Metastatic Pancreatic Cancer.
> Annals of Oncology, 25, ii11.
> <https://sci-hub.se/10.1093/annonc/mdu164.18>
>
> This applies an indirect comparison in more detail: Tamoschus, D., Draexler, K., Chang, J., Ngai, C., Madin-Warburton, M., & Pitcher, A.
> (2017).
> Cost-effectiveness analysis of regorafenib for gastrointestinal stromal tumour (GIST) in Germany.
> Clinical Drug Investigation, 37(6), 525-533.
> <https://sci-hub.se/10.1007/s40261-017-0514-3>
>
> I'm not even sure if I would classify what we are doing as an indirect comparison, as we don't have data for the novel therapy at all (that I know of), but I'll read the above articles and then make a call on that.
> Likely I'll make the call to follow the approach of: Guan, X., Li, H., Xiong, X., Peng, C., Wang, N., Ma, X., & Ma, A.
> (2021).
> Cost-effectiveness analysis of fruquintinib versus regorafenib as the third-line therapy for metastatic colorectal cancer in China.
> *Journal of Medical Economics*, *24*(1), 339-344.
> and use a HR, but I'll see.
>
> The following also shows how to do the above: Purmonen, T., Martikainen, J. A., Soini, E. J., Kataja, V., Vuorinen, R. L., & Kellokumpu-Lehtinen, P. L.
> (2008).
> Economic evaluation of sunitinib malate in second-line treatment of metastatic renal cell carcinoma in Finland.
> Clinical therapeutics, 30(2), 382-392.
> - <https://sci-hub.se/10.1016/j.clinthera.2008.02.013>
>
> ***Or we could try:***
>
> The transition probabilities were derived from the median time to radiologic progression and median OS obtained from previously published studies18,20 following recommended methods21.
> - Per: Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
> (2019).
> Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
> Journal of Medical Economics, 22(2), 163-168.
> <https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>
>
> I think the below also takes transition probabilities just from the literature, although I'd have to read it and see:
>
> Pierse, T., O'Neill, S., Dinneen, S. F., & O'Neill, C.
> (2021).
> A simulation study of the economic and health impact of a diabetes prevention programme in Ireland.
> *Diabetic Medicine*, *38*(6), e14540.

#### Transition Probabilities

> ##### Life Tables:
>
> Following significant review of the literature on CEA of mCRC it's pretty clear that it's extremely uncommon for published studies to make use of life tables, most likely because of the short life-span of mCRC patients.
> However, if I did need to make use of Life Tables for this model, I would utilise the following:
>
> WRITING:
>
> Non mCRC death was captured as a competing risk in the model.
> We use life tables in each country studied to estimate the background mortality, these data were derived from [CITE WHERE YOU GOT THE LIFE TABLES] for GIVE THE YEAR. The starting age of the cohort was X years, which is the median age of incident mCRC diagnosis, as reported by [SOME TRUST WORTHY CANCER REGISTRATION BODY].
>
> CODE:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.3 Practical exercise R
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\\August_24\\4_cSTM - time-dependent models_material
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\\August_25\\3_cSTM - history dependence_material
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A2_Making Models Probabilistic\\A2.3 Practical exercise R
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A3_Presenting Simulation Results\\A3.3 Practical exercise R

# Parametric Survival Analysis:

A GREAT EXPLANATION OF PARAMETRIC SURVIVAL ANALYSIS, MATHEMATICAL NOTATION AND CODE CAN BE FOUND HERE: <https://rpubs.com/mbounthavong/survival_analysis_in_r> ALSO SAVED HERE: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\GitHub\\COLOSSUS_Model\\rpubs-com-mbounthavong-survival_analysis_in_r

Progression and mortality probabilities were derived from the PFS and OS curves of the X study [CITE].
Data points were extracted from survival plots using Engauge Digitizer software, version 12.2, and these data points were used to fit parametric survival models [@mitchell2022].

To produce estimates of progression and mortality outside of the follow-up time in X study several parametric models were fit to a number of distributions, including exponential, Weibull, Gompertz, generalized gamma log-normal and log-logistic.
Statistical analyses demonstrated a good fit was provided for both curves by Weibull models, according to the Akaike information criterion (AIC), the Bayesian information criterion (BIC), Schwarz Bayesian criterion and visual inspection of the relative conformity of parametric models to the original Kaplan-Meier survival curves (Table A1 in the appendix, Figure A1 in the appendix).
Weibull models have been demonstrated to be more suitable for modeling events occurring in mCRC [@goldstein2014a], and support the application of proportional hazard models [@guan2021] such that the Weibull distribution model was chosen for the base case analysis.
Therefore, we estimated the transition probabilities for each cycle based on the fitted Weibull survival models.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = T)
# To knit this document every time it is run, you can change `eval` to `TRUE` in the above.
```

```{r}
rm(list = ls())  
# clear memory (removes all the variables from the work space)
```

# 01 Load parametric packages

```{r}

# Check whether the required packages are installed and, if not, install the missing packages
# - the 'pacman' package is used to conveniently install other packages
if (!require("pacman")) install.packages("pacman"); library(pacman)
p_load("flexsurv", "MASS", "dplyr", "devtools", "scales", "ellipse", "ggplot2", "lazyeval", "igraph", "ggraph", "reshape2", "knitr", "stringr", "diagram")   
p_load_gh("DARTH-git/dampack", "DARTH-git/darthtools")
```

# 02 Individual Data for Parametric Code

```{r}
# Load the individual patient data for the time-to-progression (TTP) that you recovered by digitising published survival curves
load(file = "df_TTP.RData")
# Load the individual patient data for the time-to-death (TTD) that you recovered by digitising published survival curves
load(file = "df_TTD.RData")
# At the moment I don't have time-to-death (TTD) data, and creating a fake dataset by duplicating time-to-progression (TTP) won't work as R will recognise the duplication and refuse to load the data, but when I DO have time-to-death (TTD) data I'll load it in here.

# At the moment this is just nonsense data and needs to be replaced when I have figured out the issues described at the end of this section.

# When I do have data, the data needs to be set up to include a column for the time (in the example this is time in years) and a status indicator whether the time corresponds to an event, i.e. progression (status = 1), or to the last time of follow up, i.e. censoring (status = 0),

# "What is Censoring? Censoring in a study is when there is incomplete information about a study participant, observation or value of a measurement. In clinical trials, it's when the event doesn't happen while the subject is being monitored or because they drop out of the trial." - https://www.statisticshowto.com/censoring/#:~:text=What%20is%20Censoring%3F,drop%20out%20of%20the%20trial.

# https://en.wikipedia.org/wiki/Survival_analysis#:~:text=or%20q%20%3D%200.99.-,Censoring,is%20common%20in%20survival%20analysis.

# At the bottom of the parametric survival code I think about how time from individual patient data may be changed to match the time of our cycles.

# I think the digitiser will give me the time in the context of the survival curves I am digitising, i.e., time in weeks, or time in months or time in years.

# Then I will have to set-up my time accordingly in the R code so that my cycle length is at the same level as the individual patient data.

# That is, in Koen's example:

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\ISPOR WEBINAR Health Economic Modelling in R\ISPOR_webinar_R-master

# the data includes a column for the time (in years)
# t_cycle <- 1/4      # cycle length of 3 months (in years)                                # n_cycle <- 60       # number of cycles (total runtime 15 years)
  

# So I would have my colum for time, in the [TIME] the graph I was digitising used.
# Then I would create my cycle length of X weeks (in [TIME] the graph I was digitising used)
# Then I would have my number of cycles that would add up to give me a total runtime of how long I want to run the model for.
# So, above Koen wanted to run the model for 15 years, but his cycles were in 3 months, or each cycle was a quarter of a year, so 60 quarters, or 60 3 month cycles, is 15 years.
```

# 03 Parametric Survival Analysis Model Plan

```{r}

# We will implement a semi-Markov model with time-dependent transition probabilities, via a parametric survival model fitted to some individual patient data for the time-to-progression (TTP) and time-to-death (TTD) for standard of care.

# A hazard ratio for the new intervention therapy vs. the standard of care will then be applied to obtain transition probabilities for the new experimental strategy.





```

# Time-to-Progression (TTP):

# 04 Parametric Survival Analysis itself:

```{r}

# We use the 'flexsurv' package to fit several commonly used parametric survival distributions.

# The data needs to be set up to include a column for the time (in years) and a status indicator whether the time corresponds to an event, i.e. progression (status = 1), or to the last time of follow up, i.e. censoring (status = 0).


# It looks like Koen is applying the flexsurvreg formula to individuals who experience progression (i.e. ~1):

head(df_TTP)

l_TTP_SoC_exp      <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "exp")
l_TTP_SoC_gamma    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "gamma")
l_TTP_SoC_gompertz <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "gompertz")
l_TTP_SoC_llogis   <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "llogis")
l_TTP_SoC_lnorm    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "lnorm")
l_TTP_SoC_weibull  <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTP, dist = "weibull")


```

# 05 Inspecting the fits:

```{r}

# And this would make sense per the below diagram - which looks at the proportion of individuals who have the event, i.e. progression.

# Inspect fit based on visual fit
colors <- rainbow(6)
plot(l_TTP_SoC_exp,       col = colors[1], ci = FALSE, ylab = "Event-free proportion", xlab = "Time in years", las = 1)
lines(l_TTP_SoC_gamma,    col = colors[2], ci = FALSE)
lines(l_TTP_SoC_gompertz, col = colors[3], ci = FALSE)
lines(l_TTP_SoC_llogis,   col = colors[4], ci = FALSE)
lines(l_TTP_SoC_lnorm,    col = colors[5], ci = FALSE)
lines(l_TTP_SoC_weibull,  col = colors[6], ci = FALSE)
legend("right",
       legend = c("exp", "gamma", "gompertz", "llogis", "lnorm", "weibull"),
       col    = colors,
       lty    = 1,
       bty    = "n")


# Koen says "# Weibull has the best visual and numerical fit" but I don't see what it's visually being compared to in this graph, I will have to learn about this in the C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv folder.

# The time is in years which makes sense, the data you are drawing from is in years so that's what you want to make comparisons to.


```

```{r}

# Compare the fit numerically based on the AIC
(v_AIC <- c(
  exp      = l_TTP_SoC_exp$AIC,
  gamma    = l_TTP_SoC_gamma$AIC,
  gompertz = l_TTP_SoC_gompertz$AIC,
  llogis   = l_TTP_SoC_llogis$AIC,
  lnorm    = l_TTP_SoC_lnorm$AIC,
  weibull  = l_TTP_SoC_weibull$AIC
))

# Weibull has the best visual and numerical fit

# I will have to learn what a good value of AIC is, and, because flexsurvreg provides AIC I will have to see what other numerical measures, such as BIC, it provides.

```

# 06 Saving the survival parameters for use in the model:

```{r}

# Saving the survival parameters ----

# The 'flexsurv' package return the coefficients, which need to be transformed for use in the base R functions, but that will be done when the coefficients actually are used, for the time being we will just save the survival parameters from the distribution we decide to use. 

# NB, if we are not going with Weibull then we may have to save something specific to the distribution that is not shape or scale - we can look into this if we don't use Weibull.

l_TTP_SoC_weibull
# Calling a flexsurvreg parameter like this allows you to see here that Weibull is the shape and the scale, so if we do go with another distribution we can see what it's version of shape and scale are and use these instead.

l_TTP_SoC_weibull$coefficients

coef_weibull_shape_SoC <- l_TTP_SoC_weibull$coefficients["shape"]
coef_weibull_scale_SoC <- l_TTP_SoC_weibull$coefficients["scale"]

```

# Time-to-Dead (TTD):

# 07 Parametric Survival Analysis itself:

```{r}

# We use the 'flexsurv' package to fit several commonly used parametric survival distributions.

# The data needs to be set up to include a column for the time (in years) and a status indicator whether the time corresponds to an event, i.e. progression (status = 1), or to the last time of follow up, i.e. censoring (status = 0).


# It looks like Koen is applying the flexsurvreg formula to individuals who experience progression (i.e. ~1):

# I duplicate the time to progression data frame as time to dead here, I'll REALLY need to make sure to delete this line when I have actual time to dead data or else I'll be running my analysis on the time to progression data twice and thinking I'm running it on time to dead:

df_TTD <- df_TTP

head(df_TTD)

l_TTD_SoC_exp      <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "exp")
l_TTD_SoC_gamma    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "gamma")
l_TTD_SoC_gompertz <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "gompertz")
l_TTD_SoC_llogis   <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "llogis")
l_TTD_SoC_lnorm    <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "lnorm")
l_TTD_SoC_weibull  <- flexsurvreg(formula = Surv(time, status) ~ 1, data = df_TTD, dist = "weibull")

```

# 08 Inspecting the fits:

```{r}

# And this would make sense per the below diagram - which looks at the proportion of individuals who have the event, i.e. progression.

# Inspect fit based on visual fit
colors <- rainbow(6)
plot(l_TTD_SoC_exp,       col = colors[1], ci = FALSE, ylab = "Event-free proportion", xlab = "Time in years", las = 1)
lines(l_TTD_SoC_gamma,    col = colors[2], ci = FALSE)
lines(l_TTD_SoC_gompertz, col = colors[3], ci = FALSE)
lines(l_TTD_SoC_llogis,   col = colors[4], ci = FALSE)
lines(l_TTD_SoC_lnorm,    col = colors[5], ci = FALSE)
lines(l_TTD_SoC_weibull,  col = colors[6], ci = FALSE)
legend("right",
       legend = c("exp", "gamma", "gompertz", "llogis", "lnorm", "weibull"),
       col    = colors,
       lty    = 1,
       bty    = "n")


# Koen says "# Weibull has the best visual and numerical fit" but I don't see what it's visually being compared to in this graph, I will have to learn about this in the C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv folder.

# The time is in years which makes sense, the data you are drawing from is in years so that's what you want to make comparisons to.

```

```{r}

# Compare the fit numerically based on the AIC
(v_AIC <- c(
  exp      = l_TTD_SoC_exp$AIC,
  gamma    = l_TTD_SoC_gamma$AIC,
  gompertz = l_TTD_SoC_gompertz$AIC,
  llogis   = l_TTD_SoC_llogis$AIC,
  lnorm    = l_TTD_SoC_lnorm$AIC,
  weibull  = l_TTD_SoC_weibull$AIC
))

# Weibull has the best visual and numerical fit

# I will have to learn what a good value of AIC is, and, because flexsurvreg provides AIC I will have to see what other numerical measures, such as BIC, it provides.

```

# 09 Saving the survival parameters for use in the model:

```{r}

# Saving the survival parameters ----

# The 'flexsurv' package return the coefficients, which need to be transformed for use in the base R functions, but that will be done when the coefficients actually are used, for the time being we will just save the survival parameters from the distribution we decide to use. 

# NB, if we are not going with Weibull then we may have to save something specific to the distribution that is not shape or scale - we can look into this if we don't use Weibull.

l_TTD_SoC_weibull
# Calling a flexsurvreg parameter like this allows you to see here that Weibull is the shape and the scale, so if we do go with another distribution we can see what it's version of shape and scale are and use these instead.

l_TTD_SoC_weibull$coefficients

coef_TTD_weibull_shape_SoC <- l_TTD_SoC_weibull$coefficients["shape"]
coef_TTD_weibull_scale_SoC <- l_TTD_SoC_weibull$coefficients["scale"]
```

[**I need to figure out:**]{.ul}

If I'm building the R code to be just standard of care versus experimental care I need to get rid of the treatment A - Treatment B stuff.

m_M\_Exp

[**How to extrapolate the parametric models beyond the KM curve**]{.ul}

1 How to deal with time in parametric survival analysis, that is, when I digitise for a certain period of time, how do I convert it to the time period I want to look at in my own analysis?
[there may be a clue to this by how they handle time here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\Parametric Survival Analysis\\flexsurv\\THE FLEXSURV EXAMPLE CODE EXPLAINED.txt OS_TimeYears\<-AVAL\*(1/12) it looks like they wanted overal survival time in years, and so converted AVAL by 1/12]

2 How to extrapolate the parameteric models beyond the KM curve they came from.

3 What does the flexsurvreg code mean?

Use the below (and above quote section) for digitising, engauge, parametric survival modelling, etc., There is helpful R code on doing this here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\ISPOR WEBINAR Health Economic Modelling in R and I think I should go through all the R code I have with a fine toothcomb to look for further resources in R on how to do this.
Because this R code specificaly has: "# 2. SURVIVAL ANALYSIS survival analysis of individual patient data"

> Read the below in tangent with:
>
> Cost-Effectiveness Analysis of Regorafenib for Metastatic Colorectal Cancer Daniel A. Goldstein, Bilal B. Ahmad, Qiushi Chen, Turgay Ayer, David H. Howard, Joseph Lipscomb, Bassel F. El-Rayes, and Christopher R. Flowers <https://sci-hub.se/10.1200/JCO.2015.61.9569>
>
> and also this article: Peng, Z., Hou, X., Huang, Y., Xie, T., & Hua, X.
> (2020).
> Cost-effectiveness analysis of fruquintinib for metastatic colorectal cancer third-line treatment in China.
> BMC cancer, 20(1), 1-8.
> <https://link.springer.com/article/10.1186/s12885-020-07486-w>
>
> and:
>
> Ewara, E. M., Zaric, G. S., Welch, S., & Sarma, S.
> (2014).
> Cost-effectiveness of first-line treatments for patients with KRAS wild-type metastatic colorectal cancer.
> Current Oncology, 21(4), 541-550.
> <https://sci-hub.st/10.3747/co.21.1837>
>
> Progression Risk and Mortality Progression risks and cause-specific mortalities for each arm were derived from the PFS and OS curves of the corresponding study.
> Engauge Digitizer16 was used to extract the data points from each PFS and OS plot, and then used to fit parametric survival models.
> Statistical analyses demonstrated that Weibull and log-logistic models provided a good fit for all curves according to the Akaike information criterion, Schwarz Bayesian criterion,17 and visual inspection (Supplemental Table 1 in the online version).
> Weibull models permit the hazard rate to increase over time and are thus more suitable for modeling events occurring early during follow-up periods.
> We therefore selected the Weibull model for all survival curves in this analysis.
> Next, we estimated the risk for each cycle based on the fitted survival models.
> In particular, because OS is defined as the period between the start of treatment and death, we computed the cause-specific mortality rates, p1(t), at cycle t based on the fitted Weibull OS model, OS(t), using the formula:
>
> p1ðtÞ ¼ Pðt T t þ 1Þ PðT tÞ ¼ ðOSðtÞ  OSðt  1ÞÞ OSðtÞ : (1)
>
> PFS is the period between the start and progression of disease or death.
> We computed the combined risks p2(t) based on the Weibull PFS model using Formula 1 and computed p2(t)  p1(t) as the estimate of the progression risks.
> Estimates of mortality and progression risk beyond the follow-up time in the clinical trials were extrapolated based on the fitted survival models.
> The overall mortality was defined according to the maximum value of cause-specific mortality and the background mortality.
> We use the US life tables to estimate the background mortality for each age group separately.18 **In each run of the model simulation, the initial age was sampled from the distribution of age at diagnosis for mCRC in the Surveillance, Epidemiology, and End Results database since the year 2000. [So, rather than picking a starting age, it looks like they randomly sampled an age for competing background mortality].**
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> I discussed parametric survival analysis with Andy here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.3 Practical exercise R

**[So, I think my takeway is that I need to do digitising and maybe parametric survival analysis on some existing published data - per C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Parametric Survival Analysis and the rest of my York training material THEN I'll need to include Age specific all cause mortality per: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH because I don't have my own clinical data, these are both good sources to consider seeing as they also didnt have their own data, I'll also have to think about the age of the cohort if I include competing mortality, as below:].**

> **All done below:**
>
> [*"Starting age of the cohort was 60 years, which is the median age of incident breast cancer cases as reported by The Netherlands Cancer Registration [28]." per: Kip, M., Monteban, H., & Steuten, L. (2015). Long-term cost--effectiveness of Oncotype DX® versus current clinical practice from a Dutch cost perspective. Journal of comparative effectiveness research, 4(5), 433-445. C:/Users/Jonathan/Dropbox/PC/Downloads/kip2015%20(1).pdf*]{.ul}
>
> [*"Nonbreast cancer death was captured as a competing risk in the model. These data were derived from The Netherlands Cancer Registration and based on Dutch female life tables for 2007--2009 [28,33]. Kip, M., Monteban, H., & Steuten, L. (2015). Long-term cost--effectiveness of Oncotype DX® versus current clinical practice from a Dutch cost perspective. Journal of comparative effectiveness research, 4(5), 433-445." [file:///C:/Users/Jonathan/Dropbox/PC/Downloads/kip2015%20(1).pdf](file:///C:/Users/Jonathan/Dropbox/PC/Downloads/kip2015%20(1).pdf)*]{.ul}
>
> [*Patients had a starting age of 56 years Salem, A., Men, P., Ramos, M., Zhang, Y. J., Ustyugova, A., & Lamotte, M. (2021). Cost--effectiveness analysis of empagliflozin compared with glimepiride in patients with Type 2 diabetes in China. Journal of Comparative Effectiveness Research, 10(6), 469-480.*]{.ul}

#### Utility Estimates

To present health outcomes in terms of QALYS, we adjusted survival time by quality of life using a Health Related Quality of Life (HR-QOL) score on a 0-1 utility scale.
The quality of life utilities of a cycle spent in each of the model health states has been estimated to be 0.85 for the first line health state, and 0.65 for the second line health state in the published literature on quality of life utility of patients with mCRC without complications [@ramsey2000] [We want to be clear, per Craig, D., McDaid, C., Fonseca, T., Stock, C., Duffy, S., & Woolacott, N.
(2009).
Are adverse effects incorporated in economic models?
An initial review of current practice.
*Health Technology Assessment (Winchester, England)*, *13*(62), 1-71., that our utilities come from patients without adverse events, otherwise the disutilty of these adverse events have already been counted, so we want utility before any adverse events occur, and then we can decrease this with disutility for these adverse events per the studies AND MAYBE EXPLAIN MY PLAN WITH KATHLEEN HERE TO GET COUNTRY SPECIFIC UTILITY VALUES].
We assumed in the model that utility was only related to health state, and not to therapy.

> We established the probability of Grade 3 and 4 adverse events (AEs) from the trials used for the models.15,20 Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> To do this, they basically went to the studies by:
>
> Capitain et al, 2012
>
> Hochster et al, 2008
>
> Bennouna et al, 2013
>
> And went to the tables on "Incidence of Grade X and Grade Y Adverse Events", and took the percentages reported in these studies to use in their own table under "Adverse Event Incidence"
>
> Then, according to: [[https://en.wikipedia.org/wiki/Incidence\_(epidemiology](https://en.wikipedia.org/wiki/Incidence\_(epidemiology))](https://en.wikipedia.org/wiki/Incidence_(epidemiology)) because we have explicitly an "incidence" we are allowed use this as a probability in our analysis, so, having filled in the "Adverse Event Incidence" section in our Table on model parameters like they do, we can then use these "incidence" values as probabilities.
>
> Remember that Leukopenia = Febrile Neutropenia.

The probability of Grade 3/4 adverse events (AEs) was informed by the literature [CITE].

AEs can reduce patients' utility so we applied disutility estimates for these temporary health states to each major AE based on data from Aballea at al.21 These included febrile neutropenia, vomiting, and diarrhea.
We assumed the duration of each AE was 5 days.
For each AE, the average disutility was weighted by the incidence reported in the clinical study.

Disutilities associated with adverse events (AEs), including fatigue, hand-foot syndrome, diarrhea, and hypertension, were obtained from published data in the literature13.
The duration of AEs was estimated based on Expert Consultation.
From the opinions of many doctors in China, regardless of patients in the arm of fruquintinib or regorafenib, hand-foot syndrome would last for 14 days and the disutility was 0.116.
Hypertension would last for five days and the disutility was 0.
Diarrhea would last for five days and the disutility was 0.1038 .
The duration-adjusted disutility was subtracted from the baseline utility to calculate the overall utility of each health state.

> Baseline quality of life utility of Chinese patients with T2D without complications was 0.876 [[**19**](https://www.futuremedicine.com/reader/content/17e4aa15614/10.2217/cer-2020-0284/format/epub/EPUB/xhtml/index.xhtml#B19)] and BMI-related disutility applied per BMI point increase was 0.0061 [[**20**](https://www.futuremedicine.com/reader/content/17e4aa15614/10.2217/cer-2020-0284/format/epub/EPUB/xhtml/index.xhtml#B20)].
> For acute events, a disutility was subtracted from the base utility, leading to a final utility score recorded for the patient experiencing the event.
> In case, a patient suffered different multiple events (e.g., a myocardial infarction [MI] and a stroke), the model followed the minimum approach, recording the utility value of the health state with the lowest individual score, while still applying a disutility related to [adverse]{.ul} events.
> The full list of utilities and disutilities can be retrieved from [**Supplementary Table 2**](https://www.futuremedicine.com/doi/suppl/10.2217/cer-2020-0284).
>
> Salem, A., Men, P., Ramos, M., Zhang, Y. J., Ustyugova, A., & Lamotte, M.
> (2021).
> Cost--effectiveness analysis of empagliflozin compared with glimepiride in patients with Type 2 diabetes in China.
> *Journal of Comparative Effectiveness Research*, *10*(6), 469-480.
>
> If a patient suffered multiple adverse events, I think they just applied the disutility that was the worst to their baseline utility, rather than trying to combine disutilities - which would probably be messy.
>
> We assumed that treatment-induced complications---grade 3 or 4---are mutually exclusive---a patient can have only 1 complication at a time
>
> Refaat, T., Choi, M., Gaber, G., Kiel, K., Mehta, M., Gradishar, W., & Small Jr, W.
> (2014).
> Markov model and cost-effectiveness analysis of bevacizumab in HER2-negative metastatic breast cancer.
> American journal of clinical oncology, 37(5), 480-485.
> <https://sci-hub.se/10.1097/COC.0b013e31827e4e9a>
>
> This is an alternative approach that may be cleaner, assume patients can only have one adverse event per cycle, and that way you don't have to find out which is worse and apply that one.

Lots of adverse events in studies to consider here:

"The frequencies of occurrence of AEFIs were expressed through probabilities, incidence, rates, or relative risk mentioned in 35 of the included studies [27--34,36,38-- 46,49,51,52,55--57,59,61,63,64,68,72,73,75--78]."

\- Fens, T., de Boer, P. T., van Puijenbroek, E. P., & Postma, M. J.
(2021).
Inclusion of Safety-Related Issues in Economic Evaluations for Seasonal Influenza Vaccines: A Systematic Review.
Vaccines 2021, 9, 111.

<https://sci-hub.se/10.3390/vaccines9020111>

###### Adverse Events Occurring:

> Have a look at my emails to Kathleen and Daniel re: Adverse events, utilities during adverse events, etc.,
>
> "The frequencies of occurrence of AEFIs were expressed through probabilities, incidence, rates, or relative risk mentioned in 35 of the included studies [27--34,36,38-- 46,49,51,52,55--57,59,61,63,64,68,72,73,75--78]."
>
> \- Per:
>
> Fens, T., de Boer, P. T., van Puijenbroek, E. P., & Postma, M. J.
> (2021).
> Inclusion of Safety-Related Issues in Economic Evaluations for Seasonal Influenza Vaccines: A Systematic Review.
> Vaccines 2021, 9, 111.
>
> <https://sci-hub.se/10.3390/vaccines9020111>
>
> To convert reported incidence rates to probabilities read:
>
> <https://sphweb.bumc.bu.edu/otlt/mph-modules/ep/ep713_diseasefrequency/EP713_DiseaseFrequency5.html>
>
> Also saved here:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.1.3 Key concepts in time-dependent transition probabilities\\Relationship of Incidence Rate to Cumulative Incidence (Risk).pdf
>
> The HERC Material here:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\HERC Cost Effectiveness Analysis Course\\Estimating Transition Probabilities
>
> Probability converting is here per the York course:
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Advanced Course\\A1_Advanced Markov Modelling\\A1.1.3 Key concepts in time-dependent transition probabilities\\notes
>
> This may also be helpful
>
> C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Decision Modelling - Foundation Course\\F2_Decision Trees\\F2.1.2 Probabilities & conditioning\\Probabilities and Conditioning Notes

These guys add adverse events as a State in the Markov model, so I could probably just do that:

Nebuloni, D. R., Mak, M. P., Souza, F. H., Saragiotto, D. F., Júlio, T., De Castro Jr, G., ... & Hoff, P. M.
(2013).
Modified FLOX as first-line chemotherapy for metastatic colorectal cancer patients in the public health system in Brazil: Effectiveness and cost-utility analysis.
Molecular and Clinical Oncology, 1(1), 175-179.
<https://sci-hub.st/10.3892/mco.2012.12>

Goulart, B., & Ramsey, S.
(2011).
A trial-based assessment of the cost-utility of bevacizumab and chemotherapy versus chemotherapy alone for advanced non-small cell lung cancer.
Value in Health, 14(6), 836-845.
[\<https://sci-hub.se/10.1016/j.jval.2011.04.004>](https://sci-hub.se/10.1016/j.jval.2011.04.004){.uri}

> The duration-adjusted disutility was subtracted from the baseline utility to calculate the overall utility of each health state.
>
> Cost-Effectiveness Analysis of Regorafenib for Metastatic Colorectal Cancer Daniel A. Goldstein, Bilal B. Ahmad, Qiushi Chen, Turgay Ayer, David H. Howard, Joseph Lipscomb, Bassel F. El-Rayes, and Christopher R. Flowers <https://sci-hub.se/10.1200/JCO.2015.61.9569>
>
> Progression Risk and Mortality Progression risks and cause-specific mortalitie

###### Range around things:

> The below, and most of Table 4 in Goldstein, Daniel A., Qiushi Chen, Turgay Ayer, David H. Howard, Joseph Lipscomb, R. Donald Harvey, Bassel F. El-Rayes, and Christopher R. Flowers.
> 2014.
> "Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colorectal Cancer." Clinical Colorectal Cancer 13 (4): 219--25.
> [\<https://doi.org/10.1016/j.clcc.2014.09.007.>](https://doi.org/10.1016/j.clcc.2014.09.007.){.uri}
>
> seems to suggest I can determine my upper and lower limit simply as 20% of my point estimate, most, if not all, of the time.
> Also, Goldstein take the 20% approach and apply this to the Beta distribution for utilities, so it must be possible to apply to both the Beta distribution and the normal distribution application I saw in the study from Eygpt.
>
> "All model inputs were varied 20% below and above the base case for costs, and confidence intervals or standard errors extracted from published sources were used for the probabilities, adverse events, and utilities (Table 1)."
>
> The transition probabilities had a max of 20% over the point estimate, a min of 20% under the point estimate, and a Gamma distribution for transition probabilities.
>
> \- Per: Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J.
> (2019).
> Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
> Journal of Medical Economics, 22(2), 163-168.
> [\<https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432>](https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432){.uri}
>
> The below takes the max and min as 20% of the mean for costs, utility, applying a gamma and beta distribution respectively (negative beta for disutilities that were negative percentages).
> For the discount factor it looks like they just took a minimum of no discount factor and a maximum of 5% and applied a uniform distribution.
>
> Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
> (2014).
> Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
> *Clinical colorectal cancer*, *13*(4), 219-225.
>
> ***I like that Goldstein et al., report the actual distribution they derived from their Value, Minimum and Maximum, I think I'll review the other papers by Goldstein and co-authors to see if they repeat that elsewhere, as it supports checking your methods against their own.***
>
> Transition probabilities used in the analysis
>
> The transition probabilities, had a min 20% below the base case value and a max 20% above this.
>
> Wu, Q., Wang, X., Zhang, M., Liao, W., Wang, F., & Li, Q.
> (2020).
> Cost-effectiveness analysis of bevacizumab plus paclitaxel versus bevacizumab plus capecitabine for HER2-negative locally recurrent or metastatic breast cancer.
> Oncology Research and Treatment, 43(4), 153-159.
> [\<https://www.karger.com/Article/Pdf/505932>](https://www.karger.com/Article/Pdf/505932){.uri}

**Costing Data in Spain:**

Rivera, F., Valladares, M., Gea, S., & López-Martínez, N.
(2017).
Cost-effectiveness analysis in the Spanish setting of the PEAK trial of panitumumab plus mFOLFOX6 compared with bevacizumab plus mFOLFOX6 for first-line treatment of patients with wild-type RAS metastatic colorectal cancer.
Journal of Medical Economics, 20(6), 574-584.
<https://sci-hub.st/10.1080/13696998.2017.1285780>

#### Sensitivity Analysis

I should do this in tangent with the two papers using 20%.

`` {# {r setup, include=FALSE} # knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, eval = T) # # To knit this document every time it is run, you can change `eval` to `TRUE` in the above. ``

```{r}
# rm(list = ls())  
# clear memory (removes all the variables from the work space)
```

# 01 Load packages

`{# {r} # if (!require('pacman')) install.packages('pacman'); library(pacman)  # # use this package to conveniently install other packages # # load (install if required) packages from CRAN # p_load("diagram", "dampack", "reshape2") # # library(devtools) # devtools is necessary to install from github. # # install_github("DARTH-git/darthtools", force = TRUE) # Uncomment if there is a newer version # p_load_gh("DARTH-git/darthtools")`

# 02 Load functions

```{r}
# all functions are in the darthtools package

# There is a functions RMD for the PSA stuff below, instead of calling it in: ## 08.2 Load PFS-PFSer Markov model function, I could just place it here, and place all the necessary packages above and then I would only ever need 1 R Markdown document for the entire study. Will think about doing this.


```

# To use dampack, review this before you start:

<https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/dsa_generation.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/psa_generation.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

<https://cran.r-project.org/web/packages/dampack/vignettes/voi.html>

<https://cran.r-project.org/web/packages/dampack/dampack.pdf>

<https://syzoekao.github.io/CEAutil/#43_one-way_sensitivity_analysis>

also saved as pdf files here:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS\_Model

And:

"An Introductory Tutorial on Cohort State-Transition Models in R Using a Cost-Effectiveness Analysis" <https://arxiv.org/pdf/2001.07824.pdf> [also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS\_Model\An Introductory Tutorial on Cohort StateTransition.pdf]

The dampack package was based on the following textbook:

[file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine\_%20integrating%20evidence%20and%20values.pdf](file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine_%20integrating%20evidence%20and%20values.pdf)

If I find any parts of the package confusing I can email:

Maintainer: Greg Knowlton \<knowl193\@umn.edu>

The official website is here: <https://cran.r-project.org/web/packages/dampack/>

IT'S ALSO VERY IMPORTANT TO CONSIDER WHAT BEARING THE INCLUSION OF ADDITIONAL MARKOV BUBBLES WILL HAVE ON THE FUNCTIONS USED HERE, WHICH ASSUMEDLY WERE BUILT WITH A 3 STATE MODEL IN MIND, AND NOW WE HAVE ADDITIONAL TUNNEL STATES.

# 03 Input model parameters

```{r}
## General setup


# Why ordering, V_names_states v_tc_SoC, v_tc_Exp,  v_tu_SoC and v_tu_Exp matters:

# The ordering of V_names_states has an influence on the tornado diagram and the reported cost-effectiveness results.

# So, I need to ensure I correctly order V_names_states all the way through from the start.

# m_P_Exp and m_P_SoC both use v_names_states to set up the names of the rows and columns of their matrices. 

# As does m_M_SoC and m_M_Exp.

# Then m_M_SoC and  m_M_Exp use the row column names to fill in the 100% of the cohort (or 1) in PFS and the 0% of the cohort in AE1, AE2, AE3, OS and DEAD in wave 1. m_P_Exp and m_P_SoC also both use the row and column names to fill in the transition probabilities between PFS and OS, etc.,

# Then m_M_SoC and m_M_Exp are matrix multiplied by m_P_Exp and m_P_SoC:

# for(i_cycle in 1:(n_cycle-1)) {
#  m_M_SoC[i_cycle + 1, ] <- m_M_SoC[i_cycle, ] %*% m_P_SoC[ , , i_cycle]
#  m_M_Exp[i_cycle + 1, ] <- m_M_Exp[i_cycle, ] %*% m_P_Exp[ , , i_cycle]
# }

# The way this matrix multiplication works is that the value in box 1 for m_M_SoC is multiplied by the value in box 1 for m_P_SoC and so on. If box 1 for m_M_SoC and m_P_SoC is the 1 (i.e. 100% of people in the PFS state when this all starts), multiplied by the PFS state probabilities, i.e., the probability of going from the PFS state into other states like so:

#          PFS       AE1       AE2       AE3          OS        Dead
# PFS  0.974925 0.0194985 0.0194985 0.0194985 0.005074995 0.020000000

# Then things will multiply correctly and we're multiplying PFS transition probabilities by the 100% of the cohort in the PFS state. 

# Even, if ordering v_names_states  <- c("PFS", "AE1", "AE2", "AE3", "OS", "Dead")   is not in the order as above, i.e., v_names_states  <- c("OS", "AE1", "AE2", "AE3", "PFS", "Dead") , then the m_M_SoC will have 0, 0, 0, 0, 1, 0, because everyone still starts in the PFS state, and m_P_SoC will have  "OS", "AE1", "AE2", "AE3", "PFS", "Dead", with the right probabilities put in the right spots, so when you matrix multiply it things will multiply out fine.

# However, this is not the case for ordering costs and utilities:  


# v_tc_SoC <- m_M_SoC %*% c(c_F_SoC, c_AE1, c_AE2, c_AE3, c_P, c_D)
# v_tc_Exp <- m_M_Exp %*% c(c_F_Exp, c_AE1, c_AE2, c_AE3, c_P, c_D)


# v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)
# v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)

# As you can see, in both cases the ordering is set manually by how we enter things in the concatenated brackets, so in the case above where ordering v_names_states  <- c("PFS", "AE1", "AE2", "AE3", "OS", "Dead") is ordered differently, i.e., v_names_states  <- c("OS", "AE1", "AE2", "AE3", "PFS", "Dead") when we matrix multiply costs and utilities by m_M_SoC  and m_M_Exp above we will be multiplying the utility of being in the progression free state u_F by the matrix of individuals in the OS state, which will clearly be a smaller number of individuals in the first few waves, and multiplying the utility of the OS state (u_P) by the larger number of individuals actually in the PFS state <- c("OS", "AE1", "AE2", "AE3", "PFS", "Dead"). We'll be doing the same thing with costs. What this will mean is more people getting the OS costs and OS utility and fewer people getting the PFS costs and the PFS utility, which will in turn have consequences for the cost-effectiveness analysis results with more OS costs and more OS utility being considered in the equation that compares costs and utilities.

# I've confirmed all of the things I say about by changing the ordering first of v_names_states, and then of the cost and utility concatenations. Changing the ordering of v_names_states did nothing to the CEA results or Tornado diagram provided I changed the ordering of utilities and costs to match this, changing the ordering of utilities and costs changed both unless I changed them to be in an order that matched the changed ordering of v_names_states.



# Here we define all our model parameters, so that we can call on these parameters later during our model:

t_cycle <- 1/2      # cycle length of 2 weeks (in [[months]] - this is assuming the survival curves I am digitising will be in [[months]] if they are in another period I will have to represent my cycle length in that period instead).                                  
n_cycle        <- 120                            
# We set the number of cycles to 120 to reflect 5 years broken down into fortnightly cycles
v_names_cycles  <- paste("cycle", 0:n_cycle)    
# So here, we just name each cycle by the cycle its on, going from 0 up to the number of cycles there are, here 120.
v_names_states  <- c("PFS", "AE1", "AE2", "AE3", "OS", "Dead")  
# These are the health states in our model, PFS, Adverse Event 1, Adverse Event 2, Adverse Event 3, OS, Death.
n_states        <- length(v_names_states)        
# We're just taking the number of health states from the number of names we came up with, i.e. the number of names to reflect the number of health states 

# Strategy names
v_names_strats     <- c("Standard of Care",         
                     #"EPI Assay", ## AKA Treatment A
                     #"HDX Assay") ## AKA Treatment B 
                     "Experimental Treatment") ## AKA Treatment B 
               # store the strategy names
n_str           <- length(v_names_strats)           
# number of strategies



# TRANSITION PROBABILITIES: Time-To-Transition - TTP:


# Time-dependent transition probabilities are obtained in four steps
# 1) Defining the cycle times
# 2) Obtaining the event-free (i.e. survival) probabilities for the cycle times for SoC
# 3) Obtaining the event-free (i.e. survival) probabilities for the cycle times for Exp based on a hazard ratio
# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. survival) probabilities

# 1) Defining the cycle times
(t <- seq(from = 0, by = t_cycle, length.out = n_cycle + 1))

# I think here we're saying, at each cycle how many of the time periods our individual patient data is measured at have passed? Here our individual patient data is in months, so we have 0 in cycle 0, 0.5 or half a month in cycle 1, and so on.

# Having established that allows us to obtain the transition probabilities for the time we are interested in for our cycles from this longer period individual patient data, so where the individual patient data is in months and our cycles are in fortnight or half months, this allows us to obtain transition probabilities for these fortnights.

# 2) Obtaining the event-free (i.e. survival) probabilities for the cycle times for SoC
# S_FP_SoC - survival of progression free to progression, i.e. not going to progression, i.e. staying in progression free.
# Note that the coefficients [that we took from flexsurvreg earlier] need to be transformed to obtain the parameters that the base R function uses


S_FP_SoC <- pweibull(
  q     = t, 
  shape = exp(coef_weibull_shape_SoC), 
  scale = exp(coef_weibull_scale_SoC), 
  lower.tail = FALSE
)

head(cbind(t, S_FP_SoC))

#        t  S_FP_SoC
# [1,] 0.0 1.0000000
# [2,] 0.5 0.9948214
# [3,] 1.0 0.9770661
# [4,] 1.5 0.9458256
# [5,] 2.0 0.9015175
# [6,] 2.5 0.8454597


# Having the above header shows that this is probability for surviving in the F->P state, i.e., staying in this state, because you can see in time 0 100% of people are in this state, meaning 100% of people hadnt progressed and were in PFS, if this was instead about the progressed state (i.e. OS), there should be no-one in this state when the model starts, as everyone starts in the PFS state, and it takes a while for people to reach the OS state.


# 3) Obtaining the event-free (i.e. survival) probabilities for the cycle times for Experimental treatment (aka the novel therapy) based on a hazard ratio.
# So here we basically have a hazard ratio for the novel therapy that says you do X much better under the novel therapy than under standard of care, and we want to apply it to standard of care from our individual patient data to see how much improved things would be under the novel therapy.
# (NB - if we ultimately decide not to use a hazard ratio, I could probably just create my transition probabilities for the experimental therapy from individual patient data that I have digitised from patients under this novel therapy).
# Here our hazard ratio is 0.6, I can change that for our hazard ratio.
# - note that S(t) = exp(-H(t)) and, hence, H(t) = -ln(S(t))
# that is, the survival function is the expoential of the negative hazard function, per:
# https://faculty.washington.edu/yenchic/18W_425/Lec5_survival.pdf
# and: 
# https://web.stanford.edu/~lutian/coursepdf/unit1.pdf
# Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv
# And to multiply by the hazard ratio it's necessary to convert the survivor function into the hazard function, multiply by the hazard ratio, and then convert back to the survivor function, and then these survivor functions are used for the probabilities.
HR_FP_Exp <- 0.6
H_FP_SoC  <- -log(S_FP_SoC)
H_FP_Exp  <- H_FP_SoC * HR_FP_Exp
S_FP_Exp  <- exp(-H_FP_Exp)

head(cbind(t, S_FP_SoC, H_FP_SoC, H_FP_Exp, S_FP_Exp))


head(cbind(t, S_FP_SoC, H_FP_SoC))



    # I want to vary my probabilities for the one-way sensitivity analysis, particularly for the tornado       plot of the deterministic sensitivity analysis. 
    
    # The problem here is that df_params_OWSA doesnt like the fact that a different probability for each       cycle (from the time-dependent transition probabilities) gives 122 rows (because there are 60 cycles,      two treatment strategies and a probability for each cycle). It wants the same number of       rows as      there are probabilities, i.e., it would prefer a probability of say 0.50 and then a max and a      min     around that.
    
    # To address this, I think I can apply this mean, max and min to the hazard ratios instead, knowing        that when run_owsa_det is run in the sensitivity analysis it calls this function to run and in this        function the hazard ratios generate the survivor function, and then these survivor functions are used      to generate the probabilities (which will be cycle dependent).
    
    # This is fine for the hazard ratio for the experimental strategy, I can just take:
    
    # HR_FP_Exp as my mean, and:
    
    # Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
    # Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp
    
    # For min and max.
    
    # For standard of care there was no hazard ratio, because we took these values from the survival curves     directly, and didnt vary them by a hazard ratio, like we do above.
    
    # To address this, I create a hazard ratio that is exactly one.
    
    # hazard ratio

    # A measure of how often a particular event happens in one group compared to how often it happens in       another group, over time. In cancer research, hazard ratios are often used in clinical trials to           measure survival at any point in time in a group of patients who have been given a specific treatment      compared to a control group given another treatment or a placebo. A hazard ratio of one means that         there is no difference in survival between the two groups. A hazard ratio of greater than one or less      than one means that survival was better in one of the groups. https://www.cancer.gov/publications/dictionaries/cancer-terms/def/hazard-ratio

    # Thus, I can have a hazard ratio where the baseline value of it gives you the survival curves, and        thus the probabilities, from the actual survival curves we are drawing from, and where the min and max     will be 1 +/- 0.20, which will give us probabilities that are 20% higher or lower than the probabilities from the actual survival curves that we are drawing from in the parametric survival analysis to get transitions under standard of care.
    
    # To do this, I just have to add a hazard ratio to the code that creates the transition probabilities      under standard of care as below, then I can add that hazard ratio, and it's max and min, to the            deterministic sensitivity analysis and vary all the probabilities by 20%.
        

    # So here we basically have a hazard ratio that is equal to 1, so it leaves things unchanged for           patients, and we want to apply it to standard of care from our individual patient data to leave things     unchanged in this function, but allow things to change in the sensitivity analysis.
      
    # Here our hazard ratio is 1, things are unchanged.

# So, first we create our hazard ratio == 1
HR_FP_SoC <- 1

# (I'm creating the below as new parameters, i.e. putting "nu" infront of them, in case keeping the name the same causes a problem for when I want to use them in the deterministic sensivity analysis, i.e., if I generate a parameter from itself - say var_name = var_name exactly, then there may be some way in which R handles code that won't let this work, or will take one parameter before the other, or something and stop the model from executing correctly).

# Then, we create our hazard function for SoC:
NU_S_FP_SoC <- S_FP_SoC
NU_H_FP_SoC  <- -log(NU_S_FP_SoC)
# Then, we multiply this hazard function by our hazard ratio, which is just 1, but which gives us the      opportunity to apply a hazard ratio to standard of care in our code and thus to have a hazard ratio for     standard of care for our one way deterministic sensitivity analysis and tornado diagram.
NUnu_H_FP_SoC  <- NU_H_FP_SoC * HR_FP_SoC
# Again, I was worried that with overlap when creating parameters I would have a problem with the deterministic sensivity analysis so I call it NU again to make it a "new" parameter again.
NU_S_FP_SoC  <- exp(-NUnu_H_FP_SoC)

head(cbind(t, NU_S_FP_SoC, NUnu_H_FP_SoC))



# NU_H_FP_SoC  <- -log(NU_S_FP_SoC)
# # Then, we multiply this hazard function by our hazard ratio, which is just 1, but which gives us the      opportunity to apply a hazard ratio to standard of care in our code and thus to have a hazard ratio for     standard of care for our one way deterministic sensitivity analysis and tornado diagram.
# NU_H_FP_SoC  <- NU_H_FP_SoC * HR_FP_SoC
# # 
# NU_S_FP_SoC  <- exp(-NU_H_FP_SoC)
# 
# head(cbind(t, NU_S_FP_SoC, NU_H_FP_SoC))












# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. survival) probabilities

# Now we can take the probability of being in the PFS state at each of our cycles, as created above, from 100% (i.e. from 1) in order to get the probability of NOT being in the PFS state, i.e. in order to get the probability of moving into the progressed state, or the OS state.
 

p_FP_SoC <- p_FP_Exp <- rep(NA, n_cycle)

# First we make the probability of going from progression-free (F) to progression (P) blank (i.e. NA) for all the cycles in standard of care and all the cycles under the experimental strategy.


for(i in 1:n_cycle) {
  p_FP_SoC[i] <- 1 - NU_S_FP_SoC[i+1] / NU_S_FP_SoC[i]
  p_FP_Exp[i] <- 1 - S_FP_Exp[i+1] / S_FP_Exp[i]
}




# Then we generate our transition probability under standard of care and under the experimental treatement using survival functions that havent and have had the hazard ratio from above applied to them, respectively.


# The way this works is the below, you take next cycles probability of staying in this state, divide it by this cycles probability of staying in this state, and take it from 1 to get the probability of leaving this state. 

# > head(cbind(t, S_FP_SoC))
#        t  S_FP_SoC
# [1,] 0.0 1.0000000
# [2,] 0.5 0.9948214
# [3,] 1.0 0.9770661
# [4,] 1.5 0.9458256
# [5,] 2.0 0.9015175
# [6,] 2.5 0.8454597
# > 1-0.9948214/1.0000000
# [1] 0.0051786
# > 0.9770661/0.9948214
# [1] 0.9821523
# > 1-0.9821523
# [1] 0.0178477

p_FP_SoC

#> p_FP_SoC
#  [1] 0.005178566 0.017847796 0.031973721 0.046845943 0.062181645
p_FP_Exp






 # NOW I NEED TO REPEAT THE ABOVE, BUT THIS TIME FOR OS TO DEATH.

# AT THE MOMENT I'M JUST LEAVING THIS CHUNK OF CODE ALONE, AS I DON'T THINK I'LL BE USING IT...

# TRANSITION PROBABILITIES: Time-To-Dead TTD


# Time-dependent transition probabilities are obtained in four steps
# 1) Defining the cycle times [we already did this above]
# 2) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for SoC
# 3) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for Exp based on a hazard ratio if we think we will be applying a hazard ratio in the OS -> Death setting. Probably not, probably what we'll be doing is saying that once you get into the OS state under the experimental strategy, you recieve the same second-line treatment as standard of care again and thus your event-free (i.e. overall survival) probabilities for the cycle times are the same as for SoC. - I'll code in both options here and I can make a decision when applying this.
# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. overall survival) probabilities

# 1) Defining the cycle times
(t <- seq(from = 0, by = t_cycle, length.out = n_cycle + 1))

# 2) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for SoC
# S_PD_SoC - survival of progression to dead, i.e. not going to dead, i.e. staying in progression.
# Note that the coefficients [that we took from flexsurvreg earlier] need to be transformed to obtain the parameters that the base R function uses


S_PD_SoC <- pweibull(
  q     = t, 
  shape = exp(coef_TTD_weibull_shape_SoC), 
  scale = exp(coef_TTD_weibull_scale_SoC), 
  lower.tail = FALSE
)

head(cbind(t, S_PD_SoC))



# Having the above header shows that this is probability for surviving in the P->D state, i.e., staying in this state, because you should see in time 0 0% of people are in this state, meaning 100% of people hadnt gone into the progressed state and were in PFS, which make sense in this model, the model starts with everyone in PFS, no-one starts the model in OS, and it takes a while for people to reach the OS state.


# 3) Obtaining the event-free (i.e. overall survival) probabilities for the cycle times for Experimental treatment (aka the novel therapy) based on a hazard ratio.
# So here we basically have a hazard ratio for the novel therapy that says you do X much better under the novel therapy than under standard of care, and we want to apply it to standard of care from our individual patient data to see how much improved things would be under the novel therapy.

# Here our hazard ratio is 0.6, I can change that for our hazard ratio.
# - note that S(t) = exp(-H(t)) and, hence, H(t) = -ln(S(t))
# that is, the survival function is the expoential of the negative hazard function, per:
# https://faculty.washington.edu/yenchic/18W_425/Lec5_survival.pdf
# and: 
# https://web.stanford.edu/~lutian/coursepdf/unit1.pdf
# Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv
# And to multiply by the hazard ratio it's necessary to convert the survivor function into the hazard function, multiply by the hazard ratio, and then convery back to the survivor function, and then these survivor functions are used for the probabilities.
HR_PD_Exp <- 0.6
H_PD_SoC  <- -log(S_PD_SoC)
H_PD_Exp  <- H_PD_SoC * HR_PD_Exp
S_PD_Exp  <- exp(-H_PD_Exp)

head(cbind(t, S_PD_SoC, H_PD_SoC, H_PD_Exp, S_PD_Exp))


# If I decide that, as I said,  once you get into the OS state under the experimental strategy, you recieve the same second-line treatment as standard of care again and thus your event-free (i.e. overall survival) probabilities for the cycle times are the same as for SoC, then I can use the following coding - which is just repeating what I did for standard of care but this time giving it to the experimental stratgey:

S_PD_Exp <- pweibull(
  q     = t, 
  shape = exp(coef_TTD_weibull_shape_SoC), 
  scale = exp(coef_TTD_weibull_scale_SoC), 
  lower.tail = FALSE
)

head(cbind(t, S_PD_Exp))

# I've coded in both options here and I can make a decision when applying this.




# 4) Obtaining the time-dependent transition probabilities from the event-free (i.e. overall survival) probabilities

# Now we can take the probability of being in the OS state at each of our cycles, as created above, from 100% (i.e. from 1) in order to get the probability of NOT being in the OS state, i.e. in order to get the probability of moving into the deda state.


p_PD_SoC <- p_PD_Exp <- rep(NA, n_cycle)

# First we make the probability of going from progression (P) to dead (D) blank (i.e. NA) for all the cycles in standard of care and all the cycles under the experimental strategy.


for(i in 1:n_cycle) {
  p_PD_SoC[i] <- 1 - S_PD_SoC[i+1] / S_PD_SoC[i]
  p_PD_Exp[i] <- 1 - S_PD_Exp[i+1] / S_PD_Exp[i]
}


# Then we generate our transition probability under standard of care and under the experimental treatement using survival functions that havent and have had the hazard ratio from above applied to them, respectively. [If we decide not to apply a hazard ratio for the experimental strategy going from progression to dead then neither may have a hazard ratio applied to them].


# The way this works is, you take next cycles probability of staying in this state, divide it by this cycles probability of staying in this state, and take it from 1 to get the probability of leaving this state. 

p_PD_SoC

p_PD_Exp


# Time-constant transition probabilities [ADVERSE EVENTS]:


# To create transition probabilities from longer time periods I can use the information in this email to Daniel:

# Inquiry re: Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer
# - https://outlook.office.com/mail/id/AAQkAGI5OWU0NTJkLTEzMjgtNGVhOS04ZGZiLWZkOGU1MDg3ZmE5MAAQAHQCBS2m%2B%2FVAjAc%2FWSCjQEQ%3D


# There may also be some relevant information in the below:


## Transition probabilities and hazard ratios


# "Note: To calculate the probability of dying from S1 and S2, use the hazard ratios provided. To do so, first convert the probability of dying from healthy, p_HD , to a rate; then multiply this rate by the appropriate hazard ratio; finally, convert this rate back to a probability. Recall that you can convert between rates and probabilities using the following formulas: r = − log(1 − p) and p = 1 − e ( − rt ) . The package darthtools also has the functions prob_to_rate and rate_to_prob that might be of use to you." per: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_25\3_cSTM - history dependence_material\Download exercise handout 

# ?rate_to_prob will tell you more about this function.
# ?prob_to_rate will tell you more about this function.

# As will the 50 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv


# The above also describes how to convert probabilities for different time scales, i.e., convert a probability for 5 years to 1 year, etc., and how to convert data that exists as a rate to a probability for use in a Markov model.


# The intial probabilities before I decided to do parametric survival analysis:

# p_HS_SoC  <- 0.05  # probability of becoming OS when PFS, conditional on surviving, under standard of care
# p_HS_trtA <- 0.04  # probability of becoming OS when PFS, conditional on surviving, under EPI Assay
# p_HS_trtB <- 0.02  # probability of becoming OS when PFS, conditional on surviving, under HDX Assay
# p_SD      <- 0.1   # probability of dying          
# p_HD      <- 0.01  # probability of dying when PFS


# H = HEALTHY (PFS) -> HS MEANS HEALTHY TO SICK, HD -> MEANS HEALTHY TO DEAD.
# S = SICK (OS) -> SD MEANS SICK TO DEAD.
# D = DEAD (DEAD) 

# trtA -> Means the first intervention I am studying, i.e. treatment A or the first assay.
# trtB -> Means the second intervention I am studying, i.e. treatment B or the second assay.

# To add age specific mortality to our model, we would use this #03 input model parameters of:

# "C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modeling for Public Health_DARTH\5_Nov_29\4_Cohort state-transition models (cSTM) - time-dependent models_material\Markov_3state_time"

# with the 55 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\Live Session Recording\Live Session Recording August 24th.mp4

# and this would allow us to create a vector of transition probabilities for p_HD above, i.e., from PFS to dead, that is a little bit larger at each cycle, starting at our chosen minimum value at the first cycle and increasing each cycle until it reaches our chosen maximum value at the last cycle.

# Alternatively, C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\4_cSTM - time-dependent models_material shows you how to use a life-table, as does the material from the York course, but I really think there's no need to get that detailed in our own analysis.




# Now that I am doing parametric survival analysis, the only probabilities I need to generate here are the adverse event probabilities, I generate them as conditional probabilities per chunk 20:


# Probability of going to the adverse event state from the progression free state:


p_FA1_STD     <- 0.02   # probability of adverse event 1 when progression-free under SOC
p_A1_D_STD    <- 0.1    # probability of dying when in adverse event 1 under SOC

p_FA2_STD     <- 0.02   # probability of adverse event 2 when progression-free under SOC
p_A2_D_STD    <- 0.1    # probability of dying when in adverse event 2 under SOC

p_FA3_STD     <- 0.02   # probability of adverse event 3 when progression-free under SOC
p_A3_D_STD    <- 0.1    # probability of dying when in adverse event 3 under SOC


p_FA1_EXPR     <- 0.02   # probability of adverse event 1 when progression-free under EXPR
p_A1_D_EXPR    <- 0.1    # probability of dying when in adverse event 1 under EXPR

p_FA2_EXPR     <- 0.02   # probability of adverse event 2 when progression-free under EXPR
p_A2_D_EXPR    <- 0.1    # probability of dying when in adverse event 2 under EXPR

p_FA3_EXPR     <- 0.02   # probability of adverse event 3 when progression-free under EXPR
p_A3_D_EXPR    <- 0.1    # probability of dying when in adverse event 3 under EXPR

p_PD_SoC
p_PD_Exp

p_FP_SoC
p_FP_Exp

p_FD_SoC   <- 0.02 # Probability of dying when progression-free:
p_FD_Exp   <- 0.02 # Probability of dying when progression-free:

# When digitising a progression-free survival curve everyone on that curve is on PFS, and if they leave that curve it is because their disease has progressed (so they've gone from progression free to progression, or from PFS to OS) or they've died:

# "Progression-free survival (PFS) is defined as the time from random assignment in a clinical trial to disease progression or death from any cause." https://www.ncbi.nlm.nih.gov/books/NBK137763/

# "Progression-free survival (PFS) is "the length of time during and after the treatment of a disease, such as cancer, that a patient lives with the disease but it does not get worse". https://en.wikipedia.org/wiki/Progression-free_survival#:~:text=Progression-free%20survival%20(PFS),it%20does%20not%20get%20worse".

# Time to Progression: 

# "The length of time from the date of diagnosis or the start of treatment for a disease until the disease starts to get worse or spread to other parts of the body. In a clinical trial, measuring the time to progression is one way to see how well a new treatment works. Also called TTP." https://www.cancer.gov/publications/dictionaries/cancer-terms/def/time-to-progression

# Koen's data is time to progression data, so we can't figure out time to death from it. It may be possible to do this with the data I end up using, or as advised by the C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\flexsurv folder, but at the moment I'll just set PFS to Dead as a time-constant transition probability as he does.

# I could probably apply parametric survival models to OS data, as all individuals from the OS curve were under standard of care, and get free to dead that way from the data.


# PFS -> PFS:

p_PFS_SoC  <-   (1 - p_FD_SoC) * (1 - p_FP_SoC)
p_PFS_Exp  <-   (1 - p_FD_Exp) * (1 - p_FP_Exp)

# First, I need to create progression free survival probabilities under standard of care.

# It's 1- p_FD_SoC, because if you're not going from healthy to dead then you're staying in healthy (i.e. you're not going from PFS to Dead, so you're staying in PFS) so this captures all the people leftover in PFS after those going to dead, and again for 1- p_FP_SoC, if you're not going from healthy to sick, the only other way out of healthy per the transition probability matrix, then you're staying in PFS, so 1- p_FP_SoC takes away all the people who went from healthy to sick, and leaves behind all the people who stayed in healthy (or takes away all the people who went to OS and leaves behind all the people that went to PFS per my model).


# PFS -> OS:

p_PFS_OS_SoC    <- (1 - p_FD_SoC) *      p_FP_SoC
p_PFS_OS_Exp    <- (1 - p_FD_Exp) *      p_FP_Exp

# We have to get the probability for going from PFS to OS, by getting all the people left in PFS after the people who went to death were gone, and then we multiply this probability by the probability of going from healthy to sick, or PFS to OS, because it's the probability conditional on being alive, the probability conditional on being left in the PFS state after the other people who went to the death state are gone, so the probability of going from PFS to OS, conditional on surviving.


# p_PFS_OS_SoC is the probability of transitioning from healthy to sick, conditional on surviving, so it's defined as a conditional probability. So, what the Markov model wants is : m_P_SoC["PFS", "OS"] what's the overall probability of transitioning from PFS to OS, and so that is actually not a conditional probability, which is why we multiply the probability of surviving (1 - p_HD) *      p_HS_SoC by the probability of going to OS conditional on surviving, because the transition in the model should be the marginal not the conditional, i.e. you want to have the end probability.


#So, because you took a probability that was conditional from the literature, or digitised a PFS curve to get a transition probability - and those probabilities come from people who were necessarily still in the PFS state when you digitise a PFS curve, so those are transition probabilities conditional on being in the PFS (or healthy) state - you need to do the multiplication to give you a probability you can put in your transition matrix that reflects that the probability you are working with is conditional.

# Adverse event transition probabilities have all been conditional, this is to reflect that probabilities on transitioning from adverse event to death will necessarily be conditional probabilities - as they come from individuals who were studied in the adverse event group - so they are necessarily probabilities conditional on experiencing the adverse event. Likewise, the probability of experiencing the adverse event when under treatment is conditional on being in PFS for the treatment in the first place, because it will come from reports of adverse events in studies of the treatments given that we look at. 


# Which means conditional probabilities kind of don't matter too much, because the probability won't be applied to anyone who isnt in the state it doesnt necessarily need to be conditional after all. And I guess it would have never been applied to anyone who wasnt in the state, because probabilities are only applied to people who are in the state when the transition probability matrix is multiplied by the cohort trace of people in the state.



p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD


# Probability of AE1 when PFS, conditional on surviving, under standard of care



p_A1D_SoC  <- 0.001 

# Probability of going from AE1 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.



p_A1F_SoC  <- 1-p_A1D_SoC

# Probability of returning from AE1 to PFS is 100% minus the people who have gone into the dead state.




# I repeat this for the other adverse events and for both standard of care and the experimental (novel) treatment:





# Probability of A2 when PFS, conditional on surviving, under standard of care
p_FA2_SoC  <- (p_PFS_SoC) * p_FA2_STD
# Probability of going from A2 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
p_A2D_SoC  <- 0.001 
# Probability of returning from A2 to PFS is 100% minus the people who have gone into the dead state.
p_A2F_SoC  <- 1-p_A2D_SoC

# Probability of A3 when PFS, conditional on surviving, under standard of care
p_FA3_SoC  <- (p_PFS_SoC) * p_FA3_STD
# Probability of going from A3 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
p_A3D_SoC  <- 0.001 
# Probability of returning from A3 to PFS is 100% minus the people who have gone into the dead state.
p_A3F_SoC  <- 1-p_A3D_SoC






# Probability of A1 when PFS, conditional on surviving, under standard of care
p_FA1_Exp  <- (p_PFS_Exp) * p_FA1_EXPR
# Probability of going from A1 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
p_A1D_Exp  <- 0.001 
# Probability of returning from A1 to PFS is 100% minus the people who have gone into the dead state.
p_A1F_Exp  <- 1-p_A1D_Exp

# Probability of A2 when PFS, conditional on surviving, under standard of care
p_FA2_Exp  <- (p_PFS_Exp) * p_FA2_EXPR
# Probability of going from A2 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
p_A2D_Exp  <- 0.001 
# Probability of returning from A2 to PFS is 100% minus the people who have gone into the dead state.
p_A2F_Exp  <- 1-p_A2D_Exp

# Probability of A3 when PFS, conditional on surviving, under standard of care
p_FA3_Exp  <- (p_PFS_Exp) * p_FA3_EXPR
# Probability of going from A3 to death, at the moment this is just a dummy value, I expect this to be a really low value source from the literature.
p_A3D_Exp  <- 0.001 
# Probability of returning from A3 to PFS is 100% minus the people who have gone into the dead state.
p_A3F_Exp  <- 1-p_A3D_Exp


# These all have to be conditional on survival, i.e. conditional on being in PFS, this is to ensure that I am only applying adverse events to the people still in the PFS, after transitions etc., have happened. --> This is not how conditional probabilities work, I have the correct interpretation of them elsewhere in this code file.






















## Health State Values (AKA State rewards)
# Costs and utilities  
# Basically the outcomes we are interested in coming out of this model, so we'll look at the cohorts costs over the time horizon and the quality adjusted life years in our cohort over this time horizon.

# Costs

c_F_SoC       <- 400   # cost of one cycle in PFS state under standard of care
c_F_Exp       <- 1000  # cost of one cycle in PFS state under the experimental treatment 
c_F_Ex_trtA       <- 1000  # cost of one cycle in PFS state under the experimental treatment A 
c_F_Ex_trtB       <- 1000  # cost of one cycle in PFS state under the experimental treatment
c_P       <- 1000  # cost of one cycle in progression state (I assume in OS everyone gets the same treatment so it costs everyone the same to be treated).
c_D       <- 0     # cost of one cycle in dead state


# We define the costs of the adverse events:


c_AE1 <- 100
c_AE2 <- 100
c_AE3 <- 100



# Above is the cost for each state, PFS, OS and dead,

# c_trtA    <- 800   # cost of EPI Assay in PFS state (ONCE OFF COST)
# c_trtB    <- 1500  # cost of HDX Assay in PFS state (ONCE OFF COST)
# 
# # We make the cost of the assays above so that when we have treatment strategies we can add this cost of treatment to anyone whose being treated when they receive the treatment.


# Then we define the utilities per health states.


u_F       <- 0.8     # utility when PFS 
u_P       <- 0.5   # utility when OS
u_D       <- 0     # utility when dead


# We define the utilities in the adverse event states:

u_AE1 <- 0.5
u_AE2 <- 0.5
u_AE3 <- 0.5



# Discounting factors
d_c             <- 0.04                          
# discount rate for costs (per year)
d_e             <- 0.04                          
# discount rate for QALYs (per year)

# discount rate per cycle equal discount of costs and QALYs by 4%

# Discount weight (equal discounting is assumed for costs and effects)

# Actually I've updated this and do this later on now

# v_dwc <- 1 / (1 + d_c) ^ (0:n_cycle) 
# v_dwe <- 1 / (1 + d_e) ^ (0:n_cycle) 

# So, we create a discount weight vector above, to understand the way this works I'll have to return to my York notes on discounting


```

Discount rate for costs and utilities I need to return to it more generally with the notes I wrote from York on discounting.
Also I set up 3 strategies, i.e. standard of care, EPI Assay and HDX Assay, to reflect the two assays under study, but it would be very easy to change this to just 2 strategies above.

I also want to add in the costing per the York model, as this broke costs down before adding them, although I can compare this to the York approach to costing in their published article, i.e. are costs in that article combined before they are added to the model or afterwards?

There are some things that I would like to appear in the code chunks, but not in the knitted document.
To do this I can go to:

<https://stackoverflow.com/questions/47710427/how-to-show-code-but-hide-output-in-rmarkdown>

and

<https://stackoverflow.com/questions/48286722/rmarkdown-how-to-show-partial-output-from-chunk?rq=1>

Although I can probably just use:

knitr::opts_chunk\$set(echo = TRUE, warning = FALSE, message = FALSE, eval = T)

But set echo = false

Or even better, click the gear on each code chunk and decide if I would like that code chunk to show things or not.

If I was interested in how to add adverse events to the model, Eva describes how to create an additional state that is Sick+AdverseEvent here:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv

## Draw the state-transition cohort model

```{r}

diag_names_states  <- c("PFS", "AE1", "AE2", "AE3", "OS", "Dead")  

m_P_diag <- matrix(0, nrow = n_states, ncol = n_states, dimnames = list(diag_names_states, diag_names_states))

m_P_diag["PFS", "PFS" ]  = ""
m_P_diag["PFS", "AE1" ]     = ""
m_P_diag["PFS", "AE2" ]     = ""
m_P_diag["PFS", "AE3" ]     = ""
m_P_diag["PFS", "OS" ]     = "" 
m_P_diag["PFS", "Dead" ]     = ""
m_P_diag["OS", "OS" ]     = ""
m_P_diag["OS", "Dead" ]     = ""
m_P_diag["AE1", "PFS" ]     = ""
m_P_diag["AE2", "PFS" ]     = ""
m_P_diag["AE3", "PFS" ]     = ""
m_P_diag["AE1", "Dead" ]     = ""
m_P_diag["AE2", "Dead" ]     = ""
m_P_diag["AE3", "Dead" ]     = ""
m_P_diag["Dead", "Dead" ]     = ""
layout.fig <- c(1, 4, 1) # <- changing the numbers here changes the diagram layout, so mess with these until I'm happy. It basically decides how many bubbles will be on each level, so here 1 bubble, followed by 3 bubbles, followed by 2 bubbles, per the diagram for 1, 3, 2.
plotmat(t(m_P_diag), t(layout.fig), self.cex = 0.5, curve = 0, arr.pos = 0.64,  
        latex = T, arr.type = "curved", relsize = 0.85, box.prop = 0.9, 
        cex = 0.8, box.cex = 0.7, lwd = 0.6, main = "Figure 1")
```

# 04 Define and initialize matrices and vectors

After setting up our parameters above, we initialise our structure below.

This is where we will store all of the model output, and all the things that we need to track over time as we are simulating the progression of this cohort through this disease process.

## 04.1 Cohort trace

```{r}


# WHEN COMING BACK TO COMPARE: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\Parametric Survival Analysis\ISPOR WEBINAR Health Economic Modelling in R\ISPOR_webinar_R-master\ISPOR_webinar_R-master\oncologySemiMarkov_illustration to this Rmarkdown document, a big difference is that this document creates a cycle 0 - whereas the comparison from the ISPOR_webinar_R uses -1 to get into cycle 0 where necessary. I have decided to follow the ISPOR way, because I am interested in duplicating their parametric analysis, so I need to bear this difference in mind as I go through this document.

# Also, continue from Verbose in this and in Koens file

# Markov cohort trace matrix ----

# Initialize matrices to store the Markov cohort traces for each strategy

# - note that the number of rows is n_cycle + 1, because R doesn't use index 0 (i.e. cycle 0)  --> What we mean here, is that when we do our calculations later they need to be for cycle-1 to reflect cycle 0.
m_M_SoC <- m_M_Exp  <-  matrix(
  data = NA, 
  nrow = n_cycle,  
  ncol = n_states, 
  dimnames = list(paste('Cycle', 1:n_cycle), v_names_states)
)

## Initial state vector
# We create an inital vector where people start, with everyone (1 = 100% of people) starting in PFS below:
# v_s_init <- c("PFS" = 1, "OS" = 0, "Dead" = 0)
# v_s_init

# There are cases where you can have an initial illness prevalence, so you would start some people in the sick state and some people in the healthy state, but above we're looking at people with mCRC, so we'll start everyone in PFS.


## Initialize cohort trace for cSTM (cohort state transition model) for all strategies (the strategies are the treatment strategies SOC, treatment A and Treatment B).
# So, basically we are creating a matrix to trace how the cohort is distributed across the health states, over time. 

# A matrix is necessary because there are basically two dimensions to this, the number of time cycles, which will be our rows, and then the number of states - to know which proportion of our cohort is in each state at each time:

# m_M_SoC <- matrix(0, 
#                   nrow = (n_cycles + 1), ncol = n_states, 
#                   dimnames = list(v_names_cycles, v_names_states))
# Above instead of having to bother with -1 throughout the analysis they create a cycle 0.

# Store the initial state vector in the first row of the cohort trace
# m_M_SoC[1, ] <- v_s_init
## Initialize cohort traces
## So, above I made the cohort trace for standard of care, because in my analysis all my patients start in the PFS state, I can duplicate that below to create the cohort trace for treatment A and treatment B.
# m_M_trtA <- m_M_trtB <- m_M_SoC # structure and initial states remain the same

# This gives us three matrices, m_M_trtA, m_M_trtB and m_M_SoC, that we can fill in with our simulations of how patients transitions between health states under each treatment strategy.

# In the first row of the markov matrix [1, ] put the value at the far end, i.e. "<-1" and "<-0" under the colum "PFS" [ , "PFS"], repeating this for "OS", "AE1", "AE2" "AE3" and "Dead".


# Specifying the initial state for the cohorts (all patients start in PFS)
m_M_SoC[1, "PFS"] <- m_M_Exp[1, "PFS"] <- 1
m_M_SoC[1, "AE1"] <- m_M_Exp[1, "AE1"] <- 0
m_M_SoC[1, "AE2"] <- m_M_Exp[1, "AE2"] <- 0
m_M_SoC[1, "AE3"] <- m_M_Exp[1, "AE3"] <- 0
m_M_SoC[1, "OS"]  <- m_M_Exp[1, "OS"]  <- 0
m_M_SoC[1, "Dead"]<- m_M_Exp[1, "Dead"]  <- 0

# Inspect whether properly defined
head(m_M_SoC)
head(m_M_Exp)
#head(m_M_Exp_trtB)
```

## 04.2 Transition probability matrix

```{r}

## If there were time varying transition probabilities, i.e. the longer you are in the model there are changes in your transition probability into death as you get older, etc., you would build a transition probability array, rather than a transition probability matrix, per: 

# 04.2 of:

# "C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modeling for Public Health_DARTH\5_Nov_29\4_Cohort state-transition models (cSTM) - time-dependent models_material\Markov_3state_time"

# with the 1hour: 02minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\Live Session Recording\Live Session Recording August 24th.mp4


## Initialize transition probability matrix, [i.e. build the framework or empty scaffolding of the transition probability matrix]
# all transitions to a non-death state are assumed to be conditional on survival
# - starting with standard of care
# - note that these are now 3-dimensional matrices because we are including time.
 

# m_P_SoC  <- matrix(0,
#                    nrow = n_states, ncol = n_states,
#                    dimnames = list(v_names_states, v_names_states)) # define row and column names
# m_P_SoC


# Initialize matrices for the transition probabilities
# - note that these are now 3-dimensional matrices (so, above we originally included dim = nrow and ncol, but now we also include n_cycle - i.e. the number of cycles).
# - starting with standard of care
m_P_SoC <- array(
  data = 0,
  dim = c(n_states, n_states, n_cycle),
  dimnames = list(v_names_states, v_names_states, paste0("Cycle", 1:n_cycle))
  # define row and column names - then name each array after which cycle it's for, i.e. cycle 1 all the way through to cycle 120. So Cycle 1 will have all of our patients in PFS, while cycle 120 will have most people in the dead state.
)

head(m_P_SoC)


m_P_Exp <- array(
  data = 0,
  dim = c(n_states, n_states, n_cycle),
  dimnames = list(v_names_states, v_names_states, paste0("Cycle", 1:n_cycle))
  # define row and column names - then name each array after which cycle it's for, i.e. cycle 1 all the way through to cycle 120. So Cycle 1 will have all of our patients in PFS, while cycle 120 will have most people in the dead state.
)

head(m_P_Exp)





```

Fill in the transition probability matrix:

```{r}

# It looks like I will need to decide if I have to go back to the start of putting everything in the model and put stuff in for not just the experimental strategy, but for experimental strategy 1 AND experimental strategy 2 individually.


# Setting the transition probabilities from PFS based on the model parameters
  # So, when individuals are in PFS what are their probabilities of going into the other states that they can enter from PFS?
m_P_SoC["PFS", "PFS", ] <- p_PFS_SoC
m_P_SoC["PFS", "AE1", ]     <- p_FA1_SoC
m_P_SoC["PFS", "AE2", ]     <- p_FA2_SoC
m_P_SoC["PFS", "AE3", ]     <- p_FA3_SoC
m_P_SoC["PFS", "OS", ]     <- p_PFS_OS_SoC
m_P_SoC["PFS", "Dead", ]            <- p_FD_SoC


# Setting the transition probabilities from OS
m_P_SoC["OS", "OS", ] <- 1 - p_PD_SoC
m_P_SoC["OS", "Dead", ]        <- p_PD_SoC

# Setting the transition probabilities from Dead
m_P_SoC["Dead", "Dead", ] <- 1


# Setting the transition probabilities from AE1
m_P_SoC["AE1", "PFS", ] <- p_A1F_SoC
m_P_SoC["AE1", "Dead", ] <- p_A1D_SoC

# Setting the transition probabilities from AE2
m_P_SoC["AE2", "PFS", ] <- p_A2F_SoC
m_P_SoC["AE2", "Dead", ] <- p_A2D_SoC

# Setting the transition probabilities from AE3
m_P_SoC["AE3", "PFS", ] <- p_A3F_SoC
m_P_SoC["AE3", "Dead", ] <- p_A3D_SoC


m_P_SoC

# Using the transition probabilities for standard of care as basis, update the transition probabilities that are different for the experimental strategy


m_P_Exp["PFS", "PFS", ] <- p_PFS_Exp
m_P_Exp["PFS", "AE1", ]     <- p_FA1_Exp
m_P_Exp["PFS", "AE2", ]     <- p_FA2_Exp
m_P_Exp["PFS", "AE3", ]     <- p_FA3_Exp
m_P_Exp["PFS", "OS", ]     <- p_PFS_OS_Exp
m_P_Exp["PFS", "Dead", ]            <- p_FD_Exp


# Setting the transition probabilities from OS
m_P_Exp["OS", "OS", ] <- 1 - p_PD_Exp
m_P_Exp["OS", "Dead", ]        <- p_PD_Exp

# Setting the transition probabilities from Dead
m_P_Exp["Dead", "Dead", ] <- 1


# Setting the transition probabilities from AE1
m_P_Exp["AE1", "PFS", ] <- p_A1F_Exp
m_P_Exp["AE1", "Dead", ] <- p_A1D_Exp

# Setting the transition probabilities from AE2
m_P_Exp["AE2", "PFS", ] <- p_A2F_Exp
m_P_Exp["AE2", "Dead", ] <- p_A2D_Exp

# Setting the transition probabilities from AE3
m_P_Exp["AE3", "PFS", ] <- p_A3F_Exp
m_P_Exp["AE3", "Dead", ] <- p_A3D_Exp

m_P_Exp


```

```{r}

# An explanation of conditional probabilities:

## Standard of Care
# from PFS
# m_P_SoC["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_SoC)

# It's 1- p_HD, because if you're not going from healthy to dead then you're staying in healthy (i.e. you're not going from PFS to Dead, so you're staying in PFS) so this captures all the people leftover in PFS after those going to dead, and again for 1- p_HS_SoC, if you're not going from healthy to sick, the only other way out of healthy per the transition probability matrix, then you're staying in PFS, so 1- p_HS_SoC takes away all the people who went from healthy to sick, and leaves behind all the people who stayed in healthy (or takes away all the people who went to OS and leaves behind all the people that went to PFS per my model).

# m_P_SoC["PFS", "OS"]    <- (1 - p_HD) *      p_HS_SoC

# This is because, when setting up the input parameters we didn't have a value for staying in progression free survival, so we have to calculate it, i.e. above we have to get the probability for going from PFS to OS, by getting all the people left in PFS after the people who went to death were gone, and then we multiply this probability by the probability of going from healthy to sick, or PFS to OS, because it's the probability conditional on being alive, the probability conditional on being left in the PFS state after the other people who went to the death state are gone, so the probability of going from PFS to OS, conditional on surviving.



# m_P_SoC["PFS", "OS"]    <- (1 - p_HD) *      p_HS_SoC

# p_HS_SoC is the probability of transitioning from healthy to sick, conditional on surviving, so it's defined as a conditional probability. So, what the Markov model wants is : m_P_SoC["PFS", "OS"] what's the overall probability of transitioning from PFS to OS, and so that is actually not a conditional probability, which is why we multiply the probability of surviving (1 - p_HD) *      p_HS_SoC by the probability of going to OS conditional on surviving, because the transition in the model should be the marginal not the conditional, i.e. you want to have the end probability.


#So, because you took a probability that was conditional from the literature, or digitised a PFS curve to get a transition probability - and those probabilities come from people who were necessarily still in the PFS state when you digitise a PFS curve, so those are transition probabilities conditional on being in the PFS (or healthy) state - you need to do the multiplication to give you a probability you can put in your transition matrix that reflects that the probability you are working with is conditional.


# Adverse event transition probabilities have all been conditional, this is to reflect that probabilities on transitioning from adverse event to death will necessarily be conditional probabilities - as they come from individuals who were studied in the adverse event group - so they are necessarily probabilities conditional on experiencing the adverse event. Likewise, the probability of experiencing the adverse event when under treatment is conditional on being in PFS for the treatment in the first place, because it will come from reports of adverse events in studies of the treatments given that we look at. 


# Which means conditional probabilities kind of don't matter too much, because the probability won't be applied to anyone who isnt in the state it doesnt necessarily need to be conditional after all. And I guess it would have never been applied to anyone who wasnt in the state, because probabilities are only applied to people who are in the state when the transition probability matrix is multiplied by the cohort trace of people in the state.




# m_P_SoC["PFS", "Dead"]    <-      p_HD

# Your probability of going from healthy to dead is not conditional on surviving, per the input parameters section.


# from OS
# m_P_SoC["OS", "OS"] <- 1 - p_SD
# m_P_SoC["OS", "Dead"] <-     p_SD

# Per the input parameters, your probability of going from sick to dead is also not conditional on surviving, so we dont need to multiply our probability conditional on surviving by the number of survivors as above.

# That's why though, our PFS to OS probability calculated through multiplication above is so close to the 0.05 in the input parameters, i.e. it's 0.0495, because we multiply it by 1 - p_HD or 1-0.01, which is basically multiplying it by a number very close to 1, so the probability remains very close to where it started off. The probability is conditional on the numbers of people in the alive state, we calculate that to be nearly 100% of people, so the probability remains the same. 


# m_P_SoC["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_SoC)
# And the first one (commented out above) is the same, it's the probability of going from healthy to healthy which, because it's a conditional probability, is the number of individuals left in healthy multiplied by the probability of going from healthy to health conditional on surviving to be in healthy (so, to get a conditional probability we multiply the numbers who have survived by the probability -> here we don't have a probability for healthy to healthy, but we do have a probability for health to sick, so taking 1 - this gives us the probability of not going into sick, i.e. of staying in healthy instead). 

# from Dead
# m_P_SoC["Dead", "Dead"] <- 1

# Once you're in dead you stay in dead, you can't come back to life.


# For handiness, we just take the treatment matrix that already exists for standard of care above, copy it as treatment A and treatment B, and then copy over it with new values specific to treatment A and treatment B, assuming that the probability of going from PFS to dead and staying in OS and going from OS to dead are the same as for standard of care (dead to dead is definitely the same as no one leaves dead) that's likely a reasonable assumption for our model, as once people progress we can assume they all go on to the same next treatment, standard of care progression treatment with the same likelihood of staying in OS and of going to the dead state, regardless of the novel therapy they were on:

# ## EPI Assay
# m_P_trtA <- m_P_SoC
# m_P_trtA["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_trtA)
# m_P_trtA["PFS", "OS"]    <- (1 - p_HD) *      p_HS_trtA
# 
# ## HDX Assay
# m_P_trtB <- m_P_SoC
# m_P_trtB["PFS", "PFS"] <- (1 - p_HD) * (1 - p_HS_trtB)
# m_P_trtB["PFS", "OS"]    <- (1 - p_HD) *      p_HS_trtB



# You can also think of it, per the 1hour: 05minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_24\Live Session Recording\Live Session Recording August 24th.mp4


# m_P_SoC["PFS", "OS"]    <- (1 - p_HD) [<- if you don't die] *      p_HS_SoC [then you have a risk of getting sick]

# It wont always be necessary to do this, i.e., to include conditional probabilities, it will depend on the data used and how things were estimated.

# This approach is taken for conditional probabilities, it's probably more relevant for modelling people to really, really old ages, where their mortality gets really high (so this is particularly relevant when you have age specific mortality), where you still have this high mortality of getting sick, you can end up in problems with the probabilities where the probability of leaving the state gets too high, because you haven't properly adjusted for these really high mortality rates, as people get really old.

# So, what this is doing is saying, if you don't die, then it's the remaining people who experience the probability of getting sick, so you're not adding probabilities together, which can be problematic, because sometimes you can have probabilities come to greater than one (i.e., larger than 100%).

# It's always worth considering, if you're extrapolating this risk of illness that's pretty high, on top of a cohort that also has a high risk of death, maybe that's not the right extrapolation, but this is another kind of fail safe to avoid that.

# Depending on how the data is estimated, this a correct interpretation of the probability estimate of going from one state to another (p_HS_SoC, or healthy to sick, or PFS to OS). 


# Using conditional probabilities more generally is also discussed around the 1:04 hour mark of:

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv



```

I may want to get rid of the conditional probabilities above, replacing them with standard transition probabilities per my York training.

Check if transition probability matrices are valid.

```{r}
# This is a check in the DARTH tools package that all the transition probabilities are in [0, 1], i.e., no probabilities are greater than 100%.
# This works as follows, according to line 205 of: https://github.com/DARTH-git/cohort-modeling-tutorial-intro/blob/main/analysis/cSTM_time_indep.R
# 
# ## Check if transition probability matrices are valid ----
# #* Functions included in "R/Functions.R". The latest version can be found in `darthtools` package
# ### Check that transition probabilities are [0, 1] ----
# check_transition_probability(m_P,      verbose = TRUE)  # m_P >= 0 && m_P <= 1
# check_transition_probability(m_P_strA, verbose = TRUE)  # m_P_strA >= 0 && m_P_strA <= 1
# check_transition_probability(m_P_strB, verbose = TRUE)  # m_P_strB >= 0 && m_P_strB <= 1
# check_transition_probability(m_P_strAB, verbose = TRUE) # m_P_strAB >= 0 && m_P_strAB <= 1
# ### Check that all rows sum to 1 ----
# check_sum_of_transition_array(m_P,      n_states = n_states, n_cycles = n_cycles, verbose = TRUE)  # rowSums(m_P) == 1
# check_sum_of_transition_array(m_P_strA, n_states = n_states, n_cycles = n_cycles, verbose = TRUE)  # rowSums(m_P_strA) == 1
# check_sum_of_transition_array(m_P_strB, n_states = n_states, n_cycles = n_cycles, verbose = TRUE)  # rowSums(m_P_strB) == 1
# check_sum_of_transition_array(m_P_strAB, n_states = n_states, n_cycles = n_cycles, verbose = TRUE) # rowSums(m_P_strAB) == 1
# 




# UNCOMMENT THIS! check_transition_probability(m_P_SoC,  verbose = TRUE)
# UNCOMMENT THIS! check_transition_probability(m_P_Exp,  verbose = TRUE)
#check_transition_probability(m_P_trtA, verbose = TRUE)
#check_transition_probability(m_P_trtB, verbose = TRUE)
# Check that all rows sum in each matrix sum to 1 -> which we know is a necessary condition for transition probability matrices.
# UNCOMMENT THIS! check_sum_of_transition_array(m_P_SoC,  n_states = n_states, n_cycles = n_cycle, verbose = TRUE)
# UNCOMMENT THIS! check_sum_of_transition_array(m_P_Exp,  n_states = n_states, n_cycles = n_cycle, verbose = TRUE)
#check_sum_of_transition_array(m_P_trtA, n_states = n_states, n_cycles = n_cycle, verbose = TRUE)
#check_sum_of_transition_array(m_P_trtB, n_states = n_states, n_cycles = n_cycle, verbose = TRUE)

# This error message:
#   
#   Error in check_sum_of_transition_array(m_P_SoC, n_states = n_states, n_cycles = n_cycle,  : 
#   This is not a valid transition array 
# 
# Probably reflects that: 
#   
#   > m_P_SoC
# , , Cycle1
# 
#           PFS          OS        Dead       AE1       AE2       AE3
# PFS  0.974925 0.005074995 0.020000000 0.0194985 0.0194985 0.0194985
# [1] 1.058495
# 
# That is, that I've put just any old probability value in at the moment and they are summing to be larger than one, when I've changed this to the actual probabilities I'm sure it'll be OK.
# 
# Same with:
# 
#   , , Cycle28
# 
#            PFS        OS     Dead        AE1        AE2        AE3
# PFS  0.5804491 0.3995509 0.020000 0.01160898 0.01160898 0.01160898
# OS   0.0000000 0.5922950 0.407705 0.00000000 0.00000000 0.00000000
# Dead 0.0000000 0.0000000 1.000000 0.00000000 0.00000000 0.00000000
# AE1  0.9990000 0.0000000 0.001000 0.00000000 0.00000000 0.00000000  
#   
#   > 0.5804491 + 0.3995509 + 0.020000 + 0.01160898 + 0.01160898 + 0.01160898
# [1] 1.034827

# Inspect whether properly defined
# - note that we inspect for the first two cycles
m_P_SoC[ , , 1:2]
m_P_Exp[ , , 1:2]

# Does it visually sum to 1?

```

# 05 Run Markov model

Looking in the below where more than 100% can be in a health state (1.02 in PFS in cycle 4), it seems likely that it's the transitions from adverse events into PFS that could be causing the problems with my probabilities not summing to 1 as per verbose, as the only way to get into PFS is through one of the AE's.
I'll focus on this possibility when repairing the transition probabilities.

PFS AE1 AE2 AE3 OS Dead

Cycle 4 1.0233086 0.01933076 0.01933076 0.01933076 0.031888685 0.06024724

```{r}
# for (t in 1:n_cycles){  # Use a for loop to loop through the number of cycles, basically we'll calculate the cohort distribution at the next cycle [t+1] based on the matrix of where they were at time t, matrix multiplied by the transition probability matrix for the current cycle (constant for us as we use a constant transition probability matrix, rather than a transition probability array).
# We do this for each treatment, as they all have different transition probability matrices. 
#  m_M_SoC [t + 1, ] <- m_M_SoC [t, ] %*% m_P_SoC   # estimate the state vector for the next cycle (t + 1)
#  m_M_trtA[t + 1, ] <- m_M_trtA[t, ] %*% m_P_trtA  # estimate the state vector for the next cycle (t + 1)
#  m_M_trtB[t + 1, ] <- m_M_trtB[t, ] %*% m_P_trtB  # estimate the state vector for the next cycle (t + 1)
# }
# head(m_M_SoC)  # print the first few lines of the matrix for standard of care (m_M_SoC)


# Above I was originally using a transition probability matrix, because I was using a time constant transition probability. But now, because my transition probabilities come from the survival curves, and thus change over time, I am using a transition probability array. 

# Thus, I adapt the above to reflect that my transition probability selected has to come from a certain cycle, i.e. from a certain time, and then be multiplied by the number of people in the matrix, the amount of the cohort in the matrix, at that cycle, i.e. at that time. Thats why below I pick the third dimension of the array, not row, not column, but time: R,C,T i.e.   ->  , ,i_cycle



# So here I once again create the Markov cohort trace by looping over all cycles
# - note that the trace can easily be obtained using matrix multiplications
# - note that now the right probabilities for the cycle need to be selected, like I explained above.
for(i_cycle in 1:(n_cycle-1)) {
  m_M_SoC[i_cycle + 1, ] <- m_M_SoC[i_cycle, ] %*% m_P_SoC[ , , i_cycle]
  m_M_Exp[i_cycle + 1, ] <- m_M_Exp[i_cycle, ] %*% m_P_Exp[ , , i_cycle]
}


head(m_M_SoC)  # print the first few lines of the matrix for standard of care (m_M_SoC)
head(m_M_Exp)  # print the first few lines of the matrix for experimental treatment(m_M_Exp)
```

# 06 Compute and Plot Epidemiological Outcomes

## 06.1 Cohort trace

```{r}

# So, we'll plot the above Markov model for standard of care (m_M_SoC) to show our cohort distribution over time, i.e. the proportion of our cohort in the different health states over time.

# If I wanted to do the same for Treatment A and Treatment B, I would just copy this code chunk and replace m_M_SoC with m_M_trtA and m_M_trtB

# matplot(m_M_SoC, type = 'l', 
        # ylab = "Probability of state occupancy",
        # xlab = "Cycle",
        # main = "Cohort Trace", lwd = 3)  # create a plot of the data
# legend("right", v_names_states, col = c("black", "red", "green"), 
       # lty = 1:3, bty = "n")  # add a legend to the graph

# plot a vertical line that helps identifying at which cycle the prevalence of OS is highest
# abline(v = which.max(m_M_SoC[, "OS"]), col = "gray")
# The vertical line shows you when your sick (OS) population is the greatest that it will ever be, but it can be changed from which.max to other things (so it is finding which cycle the proportion sick is the highest and putting a vertical line there).

# So, you can see in the graph everyone starts in the PFS state, but that this falls over time as people progress and leave this state, then you see OS start to peak up but then fall again as people leave this state to go into the dead state, which is an absorbing state and by the end will include everyone.

# Plotting the Markov cohort traces
matplot(m_M_SoC, 
        type = "l", 
        ylab = "Probability of state occupancy",
        xlab = "Cycle",
        main = "Makrov Cohort Traces",
        lwd  = 3,
        lty  = 1) # create a plot of the data
matplot(m_M_Exp, 
        type = "l", 
        lwd  = 3,
        lty  = 3,
        add  = TRUE) # add a plot of the experimental data ontop of the above plot
legend("right", 
       legend = c(paste(v_names_states, "(SOC)"), paste(v_names_states, "(Exp)")), 
       col    = rep(c("black", "red", "green", "blue", "yellow", "orange"), 2), 
       lty    = c(1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3), # Line type, full (1) or dashed (3), I have entered this 12 times here because we have 6 lines under standard of care (6 full lines) and 6 lines under experimental treatment (6 dashed lines)
       lwd    = 3,
       bty    = "n")

# plot a vertical line that helps identifying at which cycle the prevalence of OS is highest
abline(v = which.max(m_M_SoC[, "OS"]), col = "gray")
abline(v = which.max(m_M_Exp[, "OS"]), col = "black")
# The vertical line shows you when your sick (OS) population is the greatest that it will ever be, but it can be changed from which.max to other things (so it is finding which cycle the proportion sick is the highest and putting a vertical line there).

# (It's probably not necessary for my own analysis and I can comment these two lines out if I'm not going to use it).

# So, you can see in the graph everyone starts in the PFS state, but that this falls over time as people progress and leave this state, then you see OS start to peak up but then fall again as people leave this state to go into the dead state, which is an absorbing state and by the end will include everyone.


# There is a lot going on with this plot, and that gives me two options for when I do it with the actual data:

# 1. Do two separate plots for soc and exp, like in the plot I commented out at the start of this chunk.

# 2. copy your soc and exp matrixes but get rid of the adverse event states in the copy in order to simplify things, i.e. reasssign all the people in the "adverse event" states to be in the PFS state, because we know that only people from PFS were allowed to be in an adverse event.


```

## 06.2 Overall Survival (OS)

Although in the context of my analysis this would be PFS + OS because it is drawn from the DARTH model where healthy and sick make up OS, while dead means not OS (obviously).

```{r}
# v_os <- 1 - m_M_SoC[, "Dead"]    # calculate the overall survival (OS) probability
# v_os <- rowSums(m_M_SoC[, 1:2])  # alternative way of calculating the OS probability

# I could do my own version of this and chose just to look at pfs, rather than column 1 and 2 to look at anyone not dead.

# i.e. v_os <- (m_M_SoC[, 1])

# best practice would be to rename v_os if I am looking at something that isnt os, i.e. v_pfs and to of course update the table legend, bearing in mind that yet again this is all for standard of care, and that if I wanted to know this for treatment a and/or treatment b I would need to replace the Markov model matrix above.


# plot(v_os, type = 'l', 
#     ylim = c(0, 1),
#     ylab = "Survival probability",
#     xlab = "Cycle",
#     main = "Overall Survival")  # create a simple plot showing the OS

# add grid 
# grid(nx = n_cycles, ny = 10, col = "lightgray", lty = "dotted", lwd = par("lwd"), 
#     equilogs = TRUE) 

# Calculating and plotting overal survival (OS)
v_OS_SoC <- 1 - m_M_SoC[, "Dead"]
v_OS_Exp <- 1 - m_M_Exp[, "Dead"]

plot(v_OS_SoC, 
     type = "l",
     ylim = c(0, 1),
     ylab = "Survival probability",
     xlab = "Cycle",
     main = "Overall Survival",
     lwd  = 3) # create a simple plot showing the OS
lines(v_OS_Exp,
      lty = 3,
      lwd = 3)
legend("right",
       legend = c("SoC", "Exp"),
       lty    = c(1, 3),
       lwd    = 3,
       bty    = "n")

# add grid - completely optional, see if it looks nicer to leave this code in or out:
grid(nx = n_cycle, ny = 10, col = "lightgray", lty = "dotted", lwd = par("lwd"), 
     equilogs = TRUE) 













# Per 1:12 hour mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv

# Often you have a survival curve as input to your model [I guess from a published study], and having that survival curve you need to parameterise your model so that you match that survival curve, and that would be a process of, potentially of calibration, if you can't use the parameters directly in your model.

# So, you could produce your survival curve and compare it to curves from trials, etc., to calibrate your model. So. we'll probably do this and have it in an appendix section.

# For calibration purposes, you want to make sure that your model is outputting something that's comparable to the publications out there on actual data on the same type of patients.

# Part of being comparable to the real world is that if there is a censoring process in actual patient data, then you could incorporate this process into the model to reflect that in your model and to ensure that your own model is comparable to the existing models which may be losing people due to censoring, etc., and which you'll then need to incorporate into your model to be comparable. 


# Another interesting thing you can do is, plot this to ask is that reasonable, does that make sense that this many people are alive after this amount of time? Is the OS what I would expect it to be?


```

## 06.2.1 Life Expectancy (LE)

```{r}
v_le <- sum(v_OS_SoC)  # summing probability of OS over time  (i.e. life expectancy)

# Basically we are summing all the alive states over time through over all the cycles, so 

# v_os <- rowSums(m_M_SoC[, 1:2])

# Is basically the PFS and OS added together.

# Also bear in mind that this is life expectancy under standard of care, and not under either of the new treatments, per: # v_os <- rowSums(m_M_SoC[, 1:2]) above.

v_le


# So, if this gives a value of [1] 24.26038, that is [1] 24.26038 cycles, where in the context of our model, cycles are months, so the life expectancy for our population of patients is 24 months.


# Discounted life expectancy:

# If you wanted discounted life expectancy, if you were using life years and you wanted them discounted for your health economic outcomes, you could apply the discount rates - the discount factors - to the vector for overall survival [v_os] and then take it's sum [add it up] as above to get life expectancy that is discounted.

# As the 1:12 hour mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop _ DARTH\August_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv



```

## 06.3 Disease prevalence

```{r}

# Disease prevalence is the proportion who are sick divided by the proportion who are alive, so it's necessary to account for the fact that some of the cohort have died, so you only calculate prevalence among people who are alive,in the diagram you can see it plateauing over time, even though the number of people in the OS (or "progressed") state have gone up and come down over time and this is because this is prevalence as a proportion of those who are alive, and there are few people who are still alive by cycle 60.

# Probably looks a bit funny dividing OS by v_os below, but it's necessary to remember that v_os is PFS + OS.

# So, I guess in our context you can think of this as progression prevalence over time:

v_prev <- m_M_SoC[, "OS"]/v_OS_SoC
plot(v_prev,
     ylim = c(0, 1),
     ylab = "Prevalence",
     xlab = "Cycle",
     main = "Disease prevalence")
```

# 07 Compute Cost-Effectiveness Outcomes

Good mathematical notation here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\Applied Cost Effectiveness Modeling with R\\rcea-master\\rcea-master\\vignettes

## 07.1 Mean Costs and QALYs for each strategy

```{r}


# Calculate the costs and QALYs per cycle by multiplying m_M (the Markov trace) with the cost/utility vectors for the different states

# per cycle
# calculate expected costs by multiplying cohort trace with the cost vector for the different health states

# Basically, you take the cohort trace over time for each strategy [m_M_SoC] and multiply this by a vector of our costs for each state: [c(c_H, c_S, c_D)] -> So, basically the number of people in each state at each cycle multiplied by the cost of being in that state [cost of healthy, cost of sick and cost of dead] for each strategy that we look at (standard of care, treatment A and treatment B).

# Bear in mind we are doing matrix multiplication [%*%], because what this does is for each cycle [row in the matrix], take the vector of costs, and multiply it be the distribution in that cycle [breakdown of proportions in the states in that row] and add it all together, to get the total costs for that cycle [row]. This gives us a vector of the costs accrued in this cohort of individuals ending up in these different states for each cycle on a per person basis, because it's a cohort distribution (it always sums to 1).

# So, in cycle 1 everyone is in the OS state, so they only incur the OS cost, as more and more time passes and more and more people get sick, the costs increases due to more people being in the PFS state, but over time this falls again as more and more people go into the dead state, which has no costs as we don't treat corpses.

# v_tc_SoC  <- m_M_SoC  %*% c(c_H, c_S, c_D)  
# v_tc_trtA <- m_M_trtA %*% c(c_H + c_trtA, c_S, c_D)  
# v_tc_trtB <- m_M_trtB %*% c(c_H + c_trtB, c_S, c_D)  

v_tc_SoC <- m_M_SoC %*% c(c_F_SoC, c_AE1, c_AE2, c_AE3, c_P, c_D)
v_tc_Exp <- m_M_Exp %*% c(c_F_Exp, c_AE1, c_AE2, c_AE3, c_P, c_D)

v_tc_SoC
v_tc_Exp

# calculate expected QALYs by multiplying cohort trace with the utilities for the different health states 

# The three vectors of utilities are basically built in the exact same way as the three vectors of costs above:

# v_tu_SoC  <- m_M_SoC  %*% c(u_H, u_S, u_D)  
# v_tu_trtA <- m_M_trtA %*% c(u_H, u_S, u_D) 
# v_tu_trtB <- m_M_trtB %*% c(u_H, u_S, u_D) 

# The file I am mirroring has the following qoute:

# "

# - note that to obtain QALYs, the utility needs to be mutiplied by the cycle length as well

# v_tu_SoC <- m_M_SoC %*% c(u_F, u_P, u_D) * t_cycle
# v_tu_Exp <- m_M_Exp %*% c(u_F, u_P, u_D) * t_cycle

# To get the QALY's we not only need to multiply the state occupancy with the utilities, but also with the duration of the time cycle, because QALY's are 2-dimensional in that they combine the duration of time and the health utility.

# "

# Maybe that's because the utility originally came from a year in the disease state, and we want it to reflect the proportion of a year that is our cycle length?

# So in the example t_cycle <- 1/4 # cycle length of 3 months (in years) - so maybe their utility originally came from a years utility and they wanted to decrease it to 3 months, aka the cycle lengths, utility.

# If that was the case it would be:

# A year of utility in this state is 0.75, so 3 months should be a quarter of this, should be 0.25 of this. So, t_cycle (0.25) * u_F (0.75) = 0.1875 i.e. your utility for 3 months if u_F is your utility for 12 months.

# v_tu_SoC <- m_M_SoC %*% c(u_F, u_P, u_D) * t_cycle

# I did the maths on his approach, and without *t_cycle the first cycle of each utility value is 0.8, but after *t_cycle it is 0.2, i.e. a quarter of what it was before. Which is why I think again that he is just making the yearly utility lower to match the 3 monthly cycles, i.e. quartering a yearly utility. A quarter of utility for a quarter of a year.

# So, I think he's just trying to generate the QALYs per cycle in both states.

# And slide 25 of C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models kind of proves that.

# In it they say: cycles are 6 months.

# Their R code in: 

# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models\markov_smoking_deterministic.R

# Says:

# Now define the QALYS associated with the states per cycle

# QALY associated with 1 - year in the smoking state is Normal(mean = 0.95,  SD = 0.01)
# Divide by 2 as cycle length is 6 months
# state_qalys["Smoking"] <-  0.95 / 2 [I know they divide by 2 but we could have also multiplied by a half, i.e. *0.05].

# QALY associated with 1 - year in the not smoking state is 1 (no uncertainty)
# So all PSA samples have the same value
# Again divide by 2 as cycle length is 6 months
# state_qalys["Not smoking"] <-  1.0 / 2

# I like their approach, they define utility at the start, then they can do the following:

		# Now use the cohort vectors to calculate the 
		# total QALYs for each cycle
		# cycle_qalys[i_treatment, ] <-  cohort_vectors[i_treatment, , ] %*% state_qalys[]

# i.e., take the cohort (or Markov trace) for each of the treatment options and multiply it by the state qalys.

# So, I could take where he says: "QALY's are 2-dimensional in that they combine the duration of time and the health utility." as saying "we need to give the patient a utility for this health state that matches how long they were in this health state, i.e., if the utility of one year in this state is x and we know the patient was in this health state for 2 weeks, then we need to give them 2 weeks of x as their utility."


# Andrew calculated QALYs as: 
#
# QALYs.SP0 <- trace.SP0%*%state.utilities
# QALYs.SP0
# 
# undisc.QALYs.SP0 <- colSums(QALYs.SP0)
# undisc.QALYs.SP0

# However, he described the values that he started off with as:

# The quality of life utilities of a year spent in each of the model health states has been
# estimated to be 0.85, 0.3 and 0.75 for the Successful primary, Revision, and Successful revision health states respectively.

# and said the cycle length is one year, so maybe that's why he doesnt need to multiply by the time passed, because his utilities are for a year spent in this state, whereas when we start our utilies may not necessarily match.

# So, what I think this means is that provided utilities are for the period of the cycle we can do the below, and if they are not for the period of the cycle then we can just convert them like in the smoking file above and then apply them as though they are for the period of the cycle:

v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)
v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D)

v_tu_SoC
v_tu_Exp

# The above displays the amount of QALY's gathered in each cycle.

# These are quality adjusted cycles, these are quality adjusted life years where cycles are annual, so I need to consider what this means for utility where cycles are monthly... I suppose it's just quality adjusted monthly utility.

# You can see above that there are no utility differences between the different treatments considered: c(u_H, u_S, u_D), it's just different utilities per the health states people are in.

# If we did want to do different utilities for the health state you are in per the treatment you are on, we could define this in the input parameters and then add this in above when creating the vector of utilities for that treatment.







```

```{r}

# There's probably a more elegant way to do this, but if I wanted to add in the once off cost of the COSLOSSUS test, I could do something like the below:

# v_tc_trtA <-  v_tc_trtA+10
# v_tc_trtB <-  v_tc_trtB+100

# I just need to think about this and does it make sense, because it's adding to the cost each cycle, so probably the best indicator would be whether the cost outputted in 07.2 is only higher by the cost of the test when we do this, or if it is higher by a number of other costs, so I can check that below when adding costs in.

# Page 17 of An Introductory Tutorial on Cohort State-Transition Models in R Using a Cost-Effectiveness Analysis Example https://arxiv.org/pdf/2001.07824.pdf suggests that these are total costs expected per individual, and with testing being a one off cost, maybe you could just add it onto the costs you have at the end? Table 4: Total expected discounted QALYs and costs per average individual in the cohort of the Sick-Sicker model by strategy accounting for within-cycle correction. Costs QALYs Standard of care $151,580 20.711 Strategy A $284,805 21.499 Strategy B $259,100 22.184 Strategy AB $378,875 23.137 The total expected discounted QALYs and costs for the Sick-Sicker model under the four strategies accounting for within-cycle correction are shown in Table 4.


```

## 07.2 Discounted Mean Costs and QALYs

```{r}

# Finally, we'll aggregate these costs and utilities into overall discounted mean (average) costs and utilities.

# Obtain the discounted costs and QALYs by multiplying the vectors of total cost and total utility we created above by the discount rate for each cycle:

# - note first the discount rate for each cycle needs to be defined accounting for the cycle length, as below:

v_dwc <- 1 / ((1 + d_c) ^ ((0:(n_cycle-1)) * t_cycle)) 
v_dwe <- 1 / ((1 + d_e) ^ ((0:(n_cycle-1)) * t_cycle))

# So, below we take the vector of costs, transposing it [the t() bit] to make it a 1 row matrix and using matrix multiplication [%*%] to multiply it by that discount factor vector, which is what you multiply by the outcome in each cycle to get the discounted value of the outcome for that cycle, and then it will all be summed all together across all cycles [across all cells of the 1 row matrix]. Giving you tc_d_SoC which is a scalar, or a single value, which is the lifetime expected cost for an average person under standard of care in this cohort. 

# Discount costs by multiplying the cost vector with discount weights (v_dwc) 
# tc_d_SoC  <-  t(v_tc_SoC)  %*% v_dwc
# tc_d_trtA <-  t(v_tc_trtA) %*% v_dwc
# tc_d_trtB <-  t(v_tc_trtB) %*% v_dwc

tc_d_SoC <-  t(v_tc_SoC) %*% v_dwc 
tc_d_Exp <-  t(v_tc_Exp) %*% v_dwc


# So, now we have the average cost per person for treatment with standard of care, and the experimental treatment. 


# Discount QALYS by multiplying the QALYs vector with discount weights (v_dwe) [probably utilities would be a better term here, as it's monthly health state quality of life, rather than yearly health state quality of life]
# tu_d_SoC  <-  t(v_tu_SoC)  %*% v_dwe
# tu_d_trtA <-  t(v_tu_trtA) %*% v_dwe
# tu_d_trtB <-  t(v_tu_trtB) %*% v_dwe


tu_d_SoC <-  t(v_tu_SoC) %*% v_dwe
tu_d_Exp <-  t(v_tu_Exp) %*% v_dwe


# Store them into a vector -> So, we'll take the single values for cost for an average person under standard of care and the experimental treatment and store them in a vector v_tc_d:
v_tc_d <- c(tc_d_SoC, tc_d_Exp)
v_tu_d <- c(tu_d_SoC, tu_d_Exp)

v_tc_d
v_tu_d

# To make things a little easier to read we might name these values what they are costs for, so we can use the vector of strategy names [v_names_str] to name the values:

names (v_tc_d) <- v_names_strats
v_tc_d

names (v_tu_d) <- v_names_strats
v_tu_d


# For utility, the utility values aren't different for the different states depending on the treatment strategy, i.e. SOC, Experimental Treatment, but the time spent in the states with the associated utility is different due to the treatment you're on, so your utility value will be higher if the treatment keeps you well for longer so that you stay in a higher utility state for longer than a lower utility state, i.e., progression.


# Dataframe with discounted costs and effectiveness

# So then we aggregate them into a dataframe with our discounted costs and utilities, and then we use this to calculate ICERs in: ## 07.3 Compute ICERs of the Markov model

# df_ce <- data.frame(Strategy = v_names_strats,
#                     Cost     = v_tc_d, 
#                     Effect   = v_tu_d)
# df_ce
```

## 07.3 Compute ICERs of the Markov model

I think somewhere in all my example files there must be some example code on generating life years, etc., and reporting them as part of your results table.
I basically want to be able to report the following per page 5 of Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colorectal Cancer <file:///C:/Users/jonathanbriody/Downloads/goldstein2014.pdf>:

Results In the base case analysis, effectiveness and costs were compared between the PK and BSA group.
PK FOLFOX provided 2.03 QALYs at a cost of \$50,177 compared with BSA FOLFOX with 1.46 QALYs at a cost of \$37,173.
The ICER for PK FOLFOX was \$22,695 per QALY.
In terms of LYs, PK FOLFOX provided 2.60 LYs compared with BSA FOLFOX with 1.99 LYs.
The ICER of PK 5-FU was \$21,423 per LY.
All numerical results are summarized in Table 5.

(If I wanted to take a Net Monetary Benefit (NMB) approach instead, I could go to the 35 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_24\Live Session Recording\Live Session Recording August 24th.mp4)

```{r}

# The discounted costs and QALYs can be summarized and visualized using functions from the 'dampack' package
(df_cea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
                           effect     = c(tu_d_SoC, tu_d_Exp),
                           strategies = v_names_strats))
df_cea

# df_cea <- calculate_icers(cost       = df_ce$Cost,
#                           effect     = df_ce$Effect,
#                           strategies = df_ce$Strategy
#                           )
# df_cea

# The above uses the DARTHtools package to calculate our ICERS, incremental cost and incremental effectiveness, and also describes dominance status:

# This uses the "calculate_icers function", which does all the sorting, all the prioritization, and then computes the dominance, and not dominance, etc., and there's a publication on the methods behind this, based on a method from colleagues in Stanford.

# The default view is ordered by dominance status (ND = non-dominated, ED = extended/weak dominance, or D= strong dominance), and then ascending by cost per: https://cran.r-project.org/web/packages/dampack/vignettes/basic_cea.html


# The icer object can be easily formatted into a publication quality table using the kableExtra package.

# library(kableExtra)
# library(dplyr)
# df_cea %>%
#  kable() %>%
#  kable_styling()


```

## Results Table

> A good way to report your results can be seen in Table 4 of the following:
>
> Rivera, F., Valladares, M., Gea, S., & López-Martínez, N.
> (2017).
> Cost-effectiveness analysis in the Spanish setting of the PEAK trial of panitumumab plus mFOLFOX6 compared with bevacizumab plus mFOLFOX6 for first-line treatment of patients with wild-type RAS metastatic colorectal cancer.
> Journal of Medical Economics, 20(6), 574-584.
> <https://sci-hub.st/10.1080/13696998.2017.1285780> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Evidence Synthesis\\Economic Models\\Rivera et al_2017_Cost-effectiveness analysis in the Spanish setting of the PEAK trial of.pdf

## 07.4 Plot frontier of the Markov model

```{r}
plot(df_cea, effect_units = "QALYs", label = "all")

# plot(df_cea, effect_units = "QALYs")


# When we plot it we have 2 strategies it is possible that something would be off the frontier, would be weakly dominated or strongly dominated, with just a few strategies it's not necessarily that impressive, but with lots of strategies then dampack can be helpful.

```

The bottom axis of the above diagram shows you how effective the intervention is, the y axis shows you how costly the intervention is, here standard of care is cheaper but less effective than treatment B, which is more effective but more expensive, treatment A is not on the frontier, it has been dominated.

The frontier is also discussed here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Training Resources\\R_HTA_LMIC_Intro_to_RHTA_modeling

(If I wanted to see an example where the frontier doesnt exist, I could go to the 38 minute mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_24\Live Session Recording\Live Session Recording August 24th.mp4)

<https://www.google.ie/search?q=icer+fronter&hl=en&dcr=0&ei=nerzYYS_AciHhbIPlqyMuAk&ved=0ahUKEwjEgar1wdT1AhXIQ0EAHRYWA5cQ4dUDCA4&uact=5&oq=icer+fronter&gs_lcp=Cgdnd3Mtd2l6EAMyBwghEAoQoAE6BwgAEEcQsAM6BQgAEJECOgsIABCABBCxAxCDAToOCC4QgAQQsQMQxwEQowI6CwguELEDEMcBEK8BOg4ILhCABBCxAxDHARDRAzoICAAQsQMQgwE6BQguEJECOgQIABBDOgcIABCxAxBDOggIABCABBCxAzoOCC4QgAQQsQMQxwEQrwE6CAguEIAEELEDOgcIABDJAxBDOgUIABCSAzoFCAAQgAQ6CwguEIAEEMcBENEDOgUILhCABDoLCC4QgAQQxwEQrwE6BAgAEAo6CgguEMcBENEDEAo6BAguEAo6BwgAEMkDEAo6BggAEBYQHjoICAAQFhAKEB46BAgAEA06BQghEKABSgQIQRgASgQIRhgAUKIJWJYZYNEbaANwAngAgAGMAYgB6AiSAQM4LjSYAQCgAQHIAQjAAQE&sclient=gws-wiz>

<https://yhec.co.uk/glossary/cost-effectiveness-frontier/>

[\<https://www.hiqa.ie/sites/default/files/2017-01/Revised_Economic_Guidelines_posted_100714.pdf>](https://www.hiqa.ie/sites/default/files/2017-01/Revised_Economic_Guidelines_posted_100714.pdf){.uri} [also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\GitHub\\COLOSSUS_Model\\Revised_Economic_Guidelines_posted_100714.pdf]

<https://researchonline.lshtm.ac.uk/id/eprint/4648686/1/The%20efficiency-frontier%20approach%20for%20health_GREEN%20AAM.pdf> also saved here:C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS\_Model\The efficiency-frontier approach for health_GREEN AAM.pdf

The efficiency frontier is described on page 277 [in the textbook] of: [file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine\_%20integrating%20evidence%20and%20values.pdf](file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/R%20Code/GitHub/COLOSSUS_Model/(Cambridge%20medicine)%20Hunink,%20M.%20G.%20Myriam_Weinstein,%20Milton%20C%20-%20Decision%20making%20in%20health%20and%20medicine_%20integrating%20evidence%20and%20values.pdf)

# **Sensitivity Analysis Below:**

I've put detailed notes in:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_27\\3_SA_material\markov\_sick-sicker_SA_solutions

Which should be read in tangent with: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Decision Modelling - Advanced Course\A2\_Making Models Probabilistic\A2.1.2 Distributions for parameters\notes.txt

when doing the sensitivity analysis below:

There is similarly relevant information at the 1:38 hour mark of: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Cost-Effectiveness and Decision Modeling using R Workshop \_ DARTH\August\_25\Live Session Recording\Live Session Recording August 25th WITH CHAT.mkv

And in this document:

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Briggs et al 2012 model parameter estimation and uncertainty.pdf

**Negative ICER's:**

In presenting 1-way uncertainty analysis, reporting negative incremental cost-effectiveness ratios (ICERs) should be avoided as they are meaningless.26,27 Instead, the ICER range should be limited to results corresponding to positive incremental health consequences and costs---quadrant I in the cost-effectiveness plane.
Results for which incremental costs are positive and health consequences negative should be indicated qualitatively as ''dominated'' and those with negative incremental costs and positive health consequences as ''dominant.'' ICERs corresponding to negative incremental costs and health consequences---quadrant III---should be distinguished from ICERs in quadrant I.
C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf

# 08 Deterministic Sensitivity Analysis

When i'm thinking about the range around the point estimate, i.e., the max and min, I'll have to return to my uncertainty notepad file and see how daniel did it to make his table in the goldstein paper because I know he used the same range there for the DSA and PSA based on how he described the results of the tornado diagram, i.e.:

Also I can look at the following study and their Table:

We then assessed the impact of varying single parameters on incremental costs, QALYs, and ICUR.
We used the 95% CI of the hazard ratios for death and tumor progression reported in the trial to vary the survival in each health state.
We varied utilities in stable disease states from the worst case scenario (cough, dyspnea, and pain) to the best case scenario (treatment response with no symptoms) [15,16].
We varied the utility of PD and FN states from the worst health state utility in our study (severe bleeding) to the utility of stable disease on treatment state [15,17].
We varied the utility of severe bleeding by 10%.
We varied the number of bevacizumab cycles from a minimum of 4 to a maximum of 15 cycles, to reflect a range of cycles going from the minimally recommended number of platinum-doublet cycles with no maintenance [4]to continuation of bevacizumab for 3 months after tumor progression [5,15].
We varied all other unit costs by 20% of the base case.
Discount rates varied from 0% to 6% (Table 2A).
We repeated the one-way sensitivity analysis assuming a dose of 7.5 mg/kg of bevacizumab while varying the other individual parameters within the same ranges used for the dose of 15 mg/kg.
This allowed us to estimate the cost-effectiveness and model robustness of bevacizumab at the dose used in the AVAiL trial [22].
Goulart, B., & Ramsey, S.
(2011).
A trial-based assessment of the cost-utility of bevacizumab and chemotherapy versus chemotherapy alone for advanced non-small cell lung cancer.
Value in Health, 14(6), 836-845.
sciencedirect.com/science/article/pii/S1098301511014124

A number of sensitivity analyses were conducted to address uncertainty in parameter estimation and assess robustness of the model results.

A one-way sensitivity analysis evaluates the relative weight of each parameter in our model on overall uncertainty of cost-effectiveness [@hamdyelsisi2019].

We varied each model parameter one at a time, while holding the other parameters constant, subsequently we assessed the impact on incremental costs, QALYs, and ICER.

The mean, range and distributions applied in these analyses are described in Table X [big table that has the base-case, max, min, reference and distribution].

Goldstein says "Utilities were varied over their 95% confidence intervals." so I can think about that.

The below supports my thoughts on the above, that I use the same range for DSA and PSA in the descriptive Table like Goldstein did.

"The general principle remains that assumptions for specifying the distribution and/or defining the interval for uncertainty analysis should follow standard statistical methods (e.g., beta distributions are a natural match for binomial data; gamma or log normal for right skew parameters; log normal for relative risks or hazard ratios; logistic for odds ratios15). These distributions can be used directly in PSA or to define the interval (plausible range) for a DSA." <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

"This is true whatever the uncertainty analysis's technical specification. For a 1-way DSA, it is necessary to specify the parameter's point estimate and a defensible range; these may be taken directly from the estimation process, with the latter based, for example, on a 95% confidence interval. Representation of uncertainty depends on the uncertainty analysis planned. For DSA, an interval estimate representing beliefs about the parameter's plausible range is required. For PSA, a distribution is specified via its parameters." <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

For cost parameters, a template was completed....

Costs, utilities and probabilities were varied within +/- 20% of baseline values, per [@goldstein2014a; @goulart2011]....
[these are just guys who have done the 20% varying, if I find better online I can use that].

To study the influence of a range of transition risks, the PFS and OS curves \$S(t)\$ in the model were altered for both treatment strategies via a hazard ratio adjustment \$\\gamma\$, as \$[S(t)\^{\\gamma}]\$.
Subsequently, transition risks for each cycle were calculated from these adjusted survival curves [@goldstein2014a].
The adjustment hazard ratio was 1 +/- 0.2, i.e., the base case varied within plus or minus 20%.

We individually consider the influence of each parameter on the ICER in a one way deterministic sensitivity analysis.

**Describing Tornado diagram results:**

We create a tornado plot of the one-way sensitivity analysis .
In Figure X, the parameters with the largest effect on the ICER differed between treatment arms.
Results remained robust to adjustments to X, with an ICER range between \$X,000 to X,000 per QALY.
There was a limited influence of variation in other parameters on the ICER of THE NOVEL THERAPY (\< \$X,000 per QALY) .
A parameter of interest was X, which - when varied between \$X00 and \$X00 - placed the ICER range between \$X,000 and X,000 per QALY .

[- read this in the context of this article and mirror their display, particularly putting the max and the min on either end of the tornado diagram bars: Goldstein, D. A., Chen, Q., Ayer, T., Howard, D. H., Lipscomb, J., Harvey, R. D., ... & Flowers, C. R.
(2014).
Cost effectiveness analysis of pharmacokinetically-guided 5-fluorouracil in FOLFOX chemotherapy for metastatic colorectal cancer.
Clinical colorectal cancer, 13(4), 219-225.]

Across a range of parameter values, the ICER was above X per QALY, supporting the robustness of results.

[

Results suggest that SoC is dominated by the novel therapy, which is more cost-effective in almost all situations.
(Review Hamdy Elsisi, G., Nada, Y., Rashad, N., & Carapinha, J. (2019).
Cost-effectiveness of sorafenib versus best supportive care in advanced hepatocellular carcinoma in Egypt.
Journal of Medical Economics, 22(2), 163-168.
<https://www-tandfonline-com.proxy.library.rcsi.ie/doi/pdf/10.1080/13696998.2018.1552432> to make sure you are reporting this right)

I think I could just manually look at the ICERs when they are calculated in R to see what is and isnt dominated.

More description in this vein:

Sensitivity analysis Figure 3 shows the one-way sensitivity analysis represented by tornado diagrams for incremental QALYs, total costs, and the ICUR, respectively.
Incremental QALYs were most sensitive, in a decreasing order, to the survival in the stable disease on treatment state (maximum range 0.07-- 0.20 incremental QALYs favoring bevacizumab), the utility of the stable disease on treatment state, and the survival in the progressive disease state.
Incremental costs were most sensitive, in a decreasing order, to the number of bevacizumab cycles (maximum range US\$25,000 --\$95,000 for the bevacizumab group), the acquisition costs of bevacizumab (range \$58,000 --\$86,000), and the discount rate.
The ICUR was most sensitive to the survival in the stable disease on treatment state for the bevacizumab group (max range \$363,000 -- \$1,012,000/QALY), the number of bevacizumab cycles (range \$194,000 --\$743,000/QALY), and utility of the stable disease on treatment state (range \$393,000 --\$910,000/QALY).
We calculated a weighted average of the health utility states in the CPB group using the mean time spent in each health state as the weights.
The weighted average health utility in the CPB group was 0.534.
This allowed us to calculate the mean overall survival benefit that bevacizumab would have to generate in order to result in an ICUR of \$100,000/QALY.
The addition of bevacizumab to chemotherapy would have to increase the mean overall survival by 1.3 years in order to result in an ICUR of US\$100,000/QALY, while holding other parameters constant.
The acquisition cost of bevacizumab would have to be approximately \$885 for the addition of bevacizumab to result in an ICUR of US\$100,000/QALY, other parameters held constant.
Goulart, B., & Ramsey, S.
(2011).
A trial-based assessment of the cost-utility of bevacizumab and chemotherapy versus chemotherapy alone for advanced non-small cell lung cancer.
Value in Health, 14(6), 836-845.
<https://sci-hub.ru/10.1016/j.jval.2011.04.004>

]

"One-way DSA may be reported using a tornado diagram (Figure 1). The horizontal axis is the outcome; along the vertical axis, parameters are arrayed and horizontal bars represent the outcome range associated with the specified parameter's range. The outcome point estimate corresponding to base-case values is indicated by a vertical line cutting through all horizontal bars. Commonly, the longest bar (reflecting the parameter generating the widest uncertainty) is placed at the top, and the other bars are arrayed in descending order of length. A tornado diagram should be accompanied by a legend or table indicating the upper and lower bounds of values for each parameter, with their justification in terms of the evidence base. A table may be used instead of a tornado diagram or the results ranges provide in the text of the report (e.g., the text might state that ''the outcome ranged from X to Y when parameter Z was varied from A to B''). It is important that the range A to B represents a defensible range for parameter Z, not an arbitrary one."

Results of 1-way threshold analyses are easily reported in text (e.g., ''The ICER remains less than Y as long as the value of X is greater than A,'' or ''Alternative 1 dominates 2 if the value of Z is less than B'').

<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

The tornado plot in Figure X describes which parameters are driving the majority of the variation in the outcome.
In the figure, "Parameter Level High/Low" corresponds to parameter values that are above or below the median point estimate.
Subsequently, it is possible to highlight where high or low expected outcome values result from variation in parameter values.

*For example, the tornado plot below tells us that the parameter `"muDieCancer"` has the most leverage in affecting the effectiveness model outcome, and that values below the median value of `"muDieCancer"` in the one-way sensitivity analysis are associated with higher expected effectiveness outcomes.* [Helpful in interpreting my own tornado diagram with NMB].

*It is important to note that some important information is obscured by tornado plots and caution should be exercised when interpreting it. As the parameter of interest varies across its range in the one-way sensitivity, the strategy that maximizes the outcome of interest can also change across this range. The plot is not showing how the expected outcome changes for a single strategy, but how the expected outcome of the optimal strategy changes. The designation of which strategy is optimal is liable to alternate over the range of the parameter of interest, and this is hidden in a tornado plot.* [Important just generally to note].

*For owsa objects that contain many parameters that have minimal effect on the parameter of interest, you may want to consider producing a plot that highlights only the most influential parameters. Using the `min_rel_diff` argument, you can instruct `owsa_tornado` to exclude all parameters that fail to produce a relative change in the outcome below a specific fraction.*

*owsa_tornado(o,*

*min_rel_diff = 0.05)*

*In order to attain the data.frame used to produce the tornado plot, use the `return` argument to change the type of object returned by the `owsa_tornado` function.*

*owsa_tornado(o,*

*return = "data")*

*- <https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>*

Page 26 of <https://cran.r-project.org/web/packages/dampack/dampack.pdf> has information on making changing to the visual parts of Tornado plots, putting things in black and white, etc.,.

Very simple tornado diagram explanation: <https://mbounthavong.com/blog/2018/5/26/communicating-data-effectively-with-data-visualizations-tornado-diagram>

At the moment, my tornado plot describes a NMB.
If I wanted to change this to describe an ICER, it looks like according to the below I could do this just by adding the code from above to generate an ICER to the oncologySemiMarkov_function.R, storing it in df_cea, make sure it equals the ICER generated above and change:

outcomes = c("NMB"), \# output to do the OWSA on

to

outcomes = c("ICER"), \# output to do the OWSA on

But, at the moment I have code above where doing a tornado diagram on effectiveness as an outcome is described, so I can take their approach and apply my code to NMB if I like.

C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\R Code\\GitHub\\COLOSSUS_Model\\Deterministic Sensitivity Analysis\_ Generation.pdf

If a reviewer wants me to do a more unusual deterministic sensitivity analysis, which includes regression, I can find information on doing that here: <https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html>

## 08.1 Load PFS-PFSer Markov model function

To make the function work for my health states I rename "health" and "sick" in it to "PFS" and "OS" respectively.

One thing that concerns me is that the first line of the function sets willingness to pay at 10,000 and this is then used to build the net monetary benefit, I wonder if this needs to be updated for different willingess to pay values directly in the function or if I can do it in this Markov_3state.Rmd file?

oncologySemiMarkov \<- function(l_params_all, n_wtp = 10000) {

It may just be creating an wtp for the NMB calculation in the function, because there is none for use in this function and one is needed to calculate:

v_nmb_d \<- v_tu_d \* n_wtp - v_tc_d

but probably a good idea to ensure this is the case (i.e., change the n_wtp to see how it changes the nmb) and also ensure that I am happy with the NMB value currently under use (10,000 above), i.e. do a little reading and see if another n_wtp may be more appropriate and maybe reach out to darth and see if there is a reason their wtp's don't match.

```{r}


# I've written up an explanation of tornado diagram probabilities and creating boxes in Tornado diagrams, etc., here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\Tornado Diagram Probabilities Explained.R


# Using ICERs in the tornado diagram:

# Typically, it's very easy to get the tornado plot to work with ICERS rather than with NMB, you just change   outcomes         = c("NMB"), to   outcomes         = c("DSAICER"), and then change the function to include the following code, basically creating ICERs the same way you do in this Rmd file and then an insertion of DSAICER  = DSA_ICER into the df_ce so that you have an ICER to include under outcomes         = c("") as above: 


    # v_nmb_d   <- v_tu_d * n_wtp - v_tc_d
    # 
    # (df_DSAcea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
    #                               effect     = c(tu_d_SoC, tu_d_Exp),
    #                               strategies = v_names_strats))
    # df_DSAcea
    # 
    # DSA_ICER    <- c(df_DSAcea[2,6])
    # 
    # 
    # df_ce <- data.frame(Strategy = v_names_strats,
    #                     Cost     = v_tc_d,
    #                     Effect   = v_tu_d,
    #                     NMB      = v_nmb_d,
    #                     DSAICER  = DSA_ICER)
    # 
    # return(df_ce)


# However, I was having some trouble getting my tornado plot to generate. I think I've tracked the issue down to ggplot, which the dampack tornado plot code uses. I suspect that this was because some of my values were very small, because the exact issue I was getting was:

#Error in if (zero_range(as.numeric(limits))) { :
#missing value where TRUE/FALSE needed 

# and per the following webpage https://stackoverflow.com/questions/12462479/r-ggplot2-simple-plot-cannot-specify-log-axis-limits other people have had the same issue and talked about ggplot and how this error might mean it won't accept certain issues. Also here: https://stackoverflow.com/questions/28474630/missing-value-where-true-false-needed-error-in-if-statement-in-r and here:https://stackoverflow.com/questions/55867994/ggplot-has-error-missing-value-where-true-false-needed and here: https://stackoverflow.com/questions/67810309/pot-histograms-and-had-error-missing-value-where-true-false-needed and here: https://stackoverflow.com/questions/53052874/define-new-scales-axis-tranform-for-ggplot and https://github.com/tidyverse/ggplot2/issues/2907 and  https://github.com/tidyverse/ggplot2/issues/930 and https://community.rstudio.com/t/can-i-transform-scales-axes-from-log-to-linear-scale-in-ggplot2/69628/7 and https://groups.google.com/g/ggplot2/c/Z5dt6ZBnGII?pli=1

# I should check if they build tornado diagrams without ggplot in the York code, and if they do at least I can see the ggplot in action and remedy the issue using one of the solutions suggested at the above links.


# If I can't figure out how to get a tornado diagram to work with ICER's rather than NMB, I could follow the approach here of doing a probabilistic analysis and then creating another tornado diagram with ICERS on the probabilistic analysis. These examples explain the technicalities of how this is done in R but also from a theoretical viewpoint how this works:

# https://github.com/DARTH-git/dampack/blob/master/R/owsa.R

# Also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\doingtornadoprobabilsticallyexample.R

# https://cran.r-project.org/web/packages/dampack/vignettes/psa_analysis.html also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\cran-r-project-org-web-packages-dampack-vignettes-psa_analysis-html.pdf

# Alternatively, I could take the following approach described here: 

# https://rpubs.com/mbounthavong/decision_tree_model_tutorial also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\rpubs-com-mbounthavong-decision_tree_model_tutorial.pdf with the code that it calls saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\R Code\GitHub\COLOSSUS_Model\tornado_diagram_code.R

# I've actually coded up the above solution in quite a bit of detail. To get it working though I'll have to change oncologySemiMarkov_function.R to be:

#    
#     v_nmb_d   <- v_tu_d * n_wtp - v_tc_d
#     
#     (df_DSAcea <- calculate_icers(cost       = c(tc_d_SoC, tc_d_Exp),
#                                   effect     = c(tu_d_SoC, tu_d_Exp),
#                                   strategies = v_names_strats))
#     df_DSAcea
#     
#     DSA_ICER    <- c(df_DSAcea[2,6])
#     
#     # I'm picking the row and column where the ICER value appears in the df_DSAcea dataframe created by calculate_icers.
#     
#     # Generate the ouput
#     return(c(v_names_strats, v_tc_d, v_tu_d, DSA_ICER))
#   })
#   
# }
# 
    
    
# Or I could try the below:

# https://cran.r-project.org/web/packages/tornado/vignettes/tornadoVignette.html also saved here: https://cran.r-project.org/web/packages/tornado/vignettes/tornadoVignette.html

# But if these don't work, because of using ggplot also, I could see if I can find a package or manual creation code that doesnt use ggplot, or seeing as the ggplot part of the code in the mbounthavong code happens somewhere I can see it and change it, I might be able to see if one of the fixes mentioned in the stack overflow links about can be applied and let the tornado plot be built.

# Worst case scenario, I create the tornado diagram and two-way sensitivity analysis as NMB and report NMB's in the league table of results so it doesnt look too out of place.



#Normal chrome Session buddy has all the tabs on describing tornado diagrams that I need to read saved. Canary session buddy has all the tabs saved on doing a tornado diagram outside of the darth framework.

# A CODE CHUNK WHERE IT IS EASY TO RUN ALL THE CODE IN ONE FILE, WHICH WILL BE TAKEN AND ADDED TO THE ACTUAL TORNADO CODE CHUNKS LOWER DOWN.

p_PD  <- 0.05
p_FD  <- 0.02      
u_F <- 0.5
p_FD_SoC  <- 0.05
p_FD_Exp  <- 0.05

p_FA1_SoC  <- p_FA1_STD
p_FA2_SoC  <- p_FA2_STD
p_FA3_SoC  <- p_FA3_STD

# Because of how p_FA1_SoC is made, [i.e., p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD means that I will get a different value of p_FA1_SoC for each cycle based on multiplying a static value of p_FA1_STD by the varying value of p_PFS_SoC] I will get the following error if I include it as it was created. I can address this just like the hazard ratio problem, but at the moment I havent thought about probabilities and the whole (p_PFS_SoC) * p_FA1_STD part too deeply, so instead, I decide to make it non-varying by including just the static value that is multiplied by the changing p_PFS_SoC value as above. This all also holds for p_FA2_SoC and p_FA3_SoC.


# Error in data.frame(pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC",  : 
#  arguments imply differing number of rows: 29, 386


# p_A1F_SoC = 0.01


# p_A1D_SoC <- 0.00100000000000000000


# I was having some trouble with the A1D probabilities - with the basecase not actually being between the min and the max. My assumptions was that because the numbers were so small they were being rounded and thus moved to a value that wasnt bigger or smaller than the base case value. Although I don't think this should fix things, as this is just the format in which things are printed to the screen, rather than the value that is saved in the parameter, I still set R to display as many digits as possible (22 is the max) per: https://stackoverflow.com/questions/6669681/preventing-r-from-rounding, as suspected things are unchanged whether options(digits=2) of options(digits=22)

# I realised that the issue was that when the parameter was set equal to a value in the list below, this only changed the parameter value in the list, not outside the list, so when I create my min and max using that parameter, it is using it's initial value, not the value assigned in the list, the problem with this is that if the parameter value in the list and the value in the min and max calculation don't align, then the basecase value of the parameter from the list is unlikely to lie between the min and max created from this parameter value outside the list. I have a github commit where I get into this in greater detail here: https://github.com/JonathanBriody/COLOSSUS_Model/commit/3b0cac700598e8d111c96ab39e13ce656ae80210 if the link doesnt work it's a commit called "the problem was this" around 5.15pm on the 20/07/22

options(digits=22)

## 4.1 Initialization ----

# Load the model as a function that is defined in the supporting script
# source("Functions_markov_3state.R")
# Test function
# calculate_ce_out(l_params_all)

source(file = "oncologySemiMarkov_function.R")
# If I change any code in this main model code file, I will also need to update the function that I call.


# Create list l_params_all with all input probabilities, costs, utilities, etc.,

# Test whether the function works (and generates the same results)
# - to do so, first a list of parameter values needs to be generated

# In Koen's code they manually set the value of things here, i.e.   HR_FP_Exp = 0.6, however, to support replicability I just set the variables equal to themselves for this list, and that way if I change something like HR_FP_Exp = 0.6, to HR_FP_Exp = 0.9 earlier in this R code it will automatically update here as well.

# I know that the values they used in this section were the same as their earlier values as they gave identical results when changed from manual values to setting the variable names equal to themselves, apart from .01 of a difference for NMB, but that's probably a rounding thing:



# 
# > l_params_all <- list(
# +   coef_weibull_shape_SoC = 0.7700246,
# +   coef_weibull_scale_SoC = 1.742534,
# +   HR_FP_Exp = 0.6,
# +   p_FD      = 0.02,      
# +   p_PD      = 0.1,      
# +   c_F_SoC   = 400,  
# +   c_F_Exp   = 800,
# +   c_P       = 1000,  
# +   c_D       = 0,     
# +   u_F       = 0.8,   
# +   u_P       = 0.5,   
# +   u_D       = 0,  
# +   d_e       = 0.03,  
# +   d_c       = 0.03,
# +   n_cycle   = 60,
# +   t_cycle   = 0.25
# + )
# > 
# > oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)
#           Strategy     Cost   Effect      NMB
# 1 Standard of Care 11493.14 3.725848 63023.81
# 2     Experimental 18913.35 4.148299 64052.63
# > l_params_all <- list(
# +   coef_weibull_shape_SoC = coef_weibull_shape_SoC,
# +   coef_weibull_scale_SoC = coef_weibull_scale_SoC,
# +   HR_FP_Exp = HR_FP_Exp,
# +   p_FD      = p_FD,      
# +   p_PD      = p_PD,      
# +   c_F_SoC   = c_F_SoC,  
# +   c_F_Exp   = c_F_Exp,
# +   c_P       = c_P,  
# +   c_D       = c_D,     
# +   u_F       = u_F,   
# +   u_P       = u_P,   
# +   u_D       = u_D,  
# +   d_e       = d_e,  
# +   d_c       = d_c,
# +   n_cycle   = n_cycle,
# +   t_cycle   = t_cycle
# + )
# > oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)
#           Strategy     Cost   Effect      NMB
# 1 Standard of Care 11493.14 3.725848 63023.82
# 2     Experimental 18913.35 4.148299 64052.64

# Now I update this list with the variables I have:


l_params_all <- list(
  coef_weibull_shape_SoC = coef_weibull_shape_SoC,
  coef_weibull_scale_SoC = coef_weibull_scale_SoC,
  HR_FP_Exp = HR_FP_Exp,
  HR_FP_SoC = HR_FP_SoC,
  p_FD      = p_FD,      
  p_PD      = p_PD,
  p_A1F_SoC = p_A1F_SoC,
  p_A1D_SoC = p_A1D_SoC,
  p_A2F_SoC = p_A2F_SoC,
  p_A2D_SoC = p_A2D_SoC,
  p_A3F_SoC = p_A3F_SoC,
  p_A3D_SoC = p_A3D_SoC,
  p_FD_SoC  = p_FD_SoC,
  p_FD_Exp  = p_FD_Exp,
  p_PD_SoC = p_PD_SoC,
  p_PD_Exp = p_PD_Exp,
  p_FA1_SoC = p_FA1_SoC,
  p_FA2_SoC = p_FA2_SoC,
  p_FA3_SoC = p_FA3_SoC,
  c_F_SoC   = c_F_SoC,  
  c_F_Exp   = c_F_Exp,
  c_P       = c_P,  
  c_D       = c_D,    
  c_AE1 = c_AE1,
  c_AE2 = c_AE2,
  c_AE3 = c_AE3,
  u_F = u_F,   
  u_P = u_P,   
  u_D = u_D,  
  u_AE1 = u_AE1,
  u_AE2 = u_AE2,
  u_AE3 = u_AE3,
  d_e       = d_e,  
  d_c       = d_c,
  n_cycle   = n_cycle,
  t_cycle   = t_cycle
)

 
      #######################################################################
    
#    v_tu_SoC <- m_M_SoC %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D) *t_cycle
#    v_tu_Exp <- m_M_Exp %*% c(u_F, u_AE1, u_AE2, u_AE3, u_P, u_D) *t_cycle
    
     
     ## ~~ In the example code, they multiply utilities by t_cycle to adjust the utility by the time spent in that state, this makes the utility values smaller in their code. In my own code I prefer to set the utility values at the start of the file, for the reasons I explain earlier in Markov_3state.Rmd, however, I will decrease the size of the utilies in my code temporarily so that I can get a box in the tornado diagram for hr_fp_soc and make other adjustments as necessary to the other parameters so that everything I'm including in my code will appear on the tornado diagram and then I will know that my code works properly, once I have actual values to plug in I will no longer do this. SO REMEMBER TO DELETE THESE EDITS TO PARAMETERS WHEN I HAVE MY FINAL VALUES OR THE COST-EFFECTIVENESS RESULTS WILL DIFFER TO THOSE IN THE PRE-SENSITIVITY ANALYSIS ABOVE!!!! ~~ ##
     
     
    #######################################################################
  
#    p_FA1_SoC  <- (p_PFS_SoC) * p_FA1_STD
#    p_FA2_SoC  <- (p_PFS_SoC) * p_FA2_STD
#    p_FA3_SoC  <- (p_PFS_SoC) * p_FA3_STD
    
    # I think that, to ensure the probability is a conditional probability for p_FA1_SoC, I need to define it as above, because I know that p_PFS_SoC changes due to the hazard ratio changes, BUT, if I want to alter the probability of the adverse event in my model, I should let: p_FA1_STD reflect my adverse event and include this in the OWSA code, rather than p_FA1_SoC - because p_FA1_STD can be changed by 20% and then this code above will create p_FA1_SoC conditional on the new probability for p_PFS_SoC as this changes with the changing hazard ratios in the senstivity analysis. So, p_FA1_SoC will be conditional on the updated p_PFS_SoC rather than the original p_PFS_SoC pre-hazard ratio changes to p_PFS_SoC.





# Test function

# Test whether the function works (and generates the same results)

oncologySemiMarkov(l_params_all = l_params_all, n_wtp = 20000)



# I can't use these probabilities because they are from Free to Progressed, but now those probabilities are time-sensitive, so if I try to change these here and include them in the tornado diagram I'll be including too many things, as I explain in my description on hazard ratios.

# Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
# Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC
# 
# 
# Minimum_p_FP_Exp <- p_FP_Exp - 0.20*p_FP_Exp
# Maximum_p_FP_Exp <- p_FP_Exp + 0.20*p_FP_Exp


# Hazard Ratios:


HR_FP_Exp
    
Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp



HR_FP_SoC
    
Minimum_HR_FP_SoC <- HR_FP_SoC - 0.20*HR_FP_SoC
Maximum_HR_FP_SoC <- HR_FP_SoC + 0.20*HR_FP_SoC

# Probability of progressive disease to death:

p_PD

Minimum_p_PD <- p_PD - 0.20*p_PD
Maximum_p_PD <- p_PD + 0.20*p_PD

# Under the assumption that everyone will get the same second line therapy, I give them all the same probability of going from progessed (i.e., OS) to dead, and thus only need to include p_PD here once - because it is applied in oncologySemiMarkov_function.R for both SoC and Exp.


# Probability of going from PFS to Death states under the standard of care treatment and the experimental treatment:

p_FD_SoC

Minimum_p_FD_SoC <- p_FD_SoC - 0.20*p_FD_SoC
Maximum_p_FD_SoC <- p_FD_SoC + 0.20*p_FD_SoC

p_FD_Exp

Minimum_p_FD_Exp<- p_FD_Exp - 0.20*p_FD_Exp
Maximum_p_FD_Exp <- p_FD_Exp + 0.20*p_FD_Exp



# Probability of Adverse Events, from PFS to AE, from AE to PFS and from AE to Death:

# Although these probabilities say _SoC, I make the assumption that everyone has the same probability of AE1, 2 or 3 regardless of what treatment they are under (i.e., SoC or the Experimental). If I decide to get more complicated with treatment specific AE probabilities in the future I can update this to be _SoC and _Exp.


p_FA1_SoC
Minimum_p_FA1_SoC <- p_FA1_SoC - 0.20*p_FA1_SoC
Maximum_p_FA1_SoC <- p_FA1_SoC + 0.20*p_FA1_SoC

p_A1F_SoC
Minimum_p_A1F_SoC <- p_A1F_SoC - 0.20*p_A1F_SoC
Maximum_p_A1F_SoC <- p_A1F_SoC + 0.20*p_A1F_SoC


p_A1D_SoC
Minimum_p_A1D_SoC <- p_A1D_SoC - 0.20*p_A1D_SoC
Maximum_p_A1D_SoC <- p_A1D_SoC + 0.20*p_A1D_SoC

p_FA2_SoC
Minimum_p_FA2_SoC <- p_FA2_SoC - 0.20*p_FA2_SoC
Maximum_p_FA2_SoC <- p_FA2_SoC + 0.20*p_FA2_SoC

p_A2F_SoC
Minimum_p_A2F_SoC <- p_A2F_SoC - 0.20*p_A2F_SoC
Maximum_p_A2F_SoC <- p_A2F_SoC + 0.20*p_A2F_SoC

p_A2D_SoC
Minimum_p_A2D_SoC <- p_A2D_SoC - 0.20*p_A2D_SoC
Maximum_p_A2D_SoC <- p_A2D_SoC + 0.20*p_A2D_SoC

p_FA3_SoC
Minimum_p_FA3_SoC <- p_FA3_SoC - 0.20*p_FA3_SoC
Maximum_p_FA3_SoC <- p_FA3_SoC + 0.20*p_FA3_SoC

p_A3F_SoC
Minimum_p_A3F_SoC <- p_A3F_SoC - 0.20*p_A3F_SoC
Maximum_p_A3F_SoC <- p_A3F_SoC + 0.20*p_A3F_SoC

p_A3D_SoC
Minimum_p_A3D_SoC <- p_A3D_SoC - 0.20*p_A3D_SoC
Maximum_p_A3D_SoC <- p_A3D_SoC + 0.20*p_A3D_SoC


#install.packages("Rmpfr")
#library(Rmpfr)

# 
# Minimum_p_A1D_SoC <- mpfr(Minimum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A1D_SoC <- mpfr(Maximum_p_A1D_SoC,200) # set arbitrary precision that's greater than R default
# 
# 
# Minimum_p_A2D_SoC <- mpfr(Minimum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A2D_SoC <- mpfr(Maximum_p_A2D_SoC,200) # set arbitrary precision that's greater than R default
# 
# Minimum_p_A3D_SoC <- mpfr(Minimum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default
# Maximum_p_A3D_SoC <- mpfr(Maximum_p_A3D_SoC,200) # set arbitrary precision that's greater than R default




# Cost:

# If I decide to include the cost of the test for patients I will also need to include this in the sensitivity analysis here:

c_F_SoC

Minimum_c_F_SoC <- c_F_SoC - 0.20*c_F_SoC
Maximum_c_F_SoC <- c_F_SoC + 0.20*c_F_SoC

c_F_Exp

Minimum_c_F_Exp  <- c_F_Exp - 0.20*c_F_Exp
Maximum_c_F_Exp  <- c_F_Exp + 0.20*c_F_Exp

c_P 

Minimum_c_P  <- c_P - 0.20*c_P
Maximum_c_P  <- c_P + 0.20*c_P

c_D  

Minimum_c_D  <- c_D - 0.20*c_D
Maximum_c_D  <- c_D + 0.20*c_D

c_AE1

Minimum_c_AE1  <- c_AE1 - 0.20*c_AE1
Maximum_c_AE1  <- c_AE1 + 0.20*c_AE1

c_AE2

Minimum_c_AE2  <- c_AE2 - 0.20*c_AE2
Maximum_c_AE2  <- c_AE2 + 0.20*c_AE2

c_AE3

Minimum_c_AE3  <- c_AE3 - 0.20*c_AE3
Maximum_c_AE3  <- c_AE3 + 0.20*c_AE3


# Utilities:


u_F

Minimum_u_F <- u_F - 0.20*u_F
Maximum_u_F <- u_F + 0.20*u_F 


u_P

Minimum_u_P <- u_P - 0.20*u_P
Maximum_u_P <- u_P + 0.20*u_P 


u_D

Minimum_u_D <- u_D - 0.20*u_D
Maximum_u_D <- u_D + 0.20*u_D 


u_AE1

Minimum_u_AE1 <- u_AE1 - 0.20*u_AE1
Maximum_u_AE1 <- u_AE1 + 0.20*u_AE1 


u_AE2

Minimum_u_AE2 <- u_AE2 - 0.20*u_AE2
Maximum_u_AE2 <- u_AE2 + 0.20*u_AE2 


u_AE3

Minimum_u_AE3 <- u_AE3 - 0.20*u_AE3
Maximum_u_AE3 <- u_AE3 + 0.20*u_AE3 
 
 
# Discount factor
# Cost Discount Factor
# Utility Discount Factor


d_e

Minimum_d_e <- d_e - 0.20*d_e
Maximum_d_e <- d_e + 0.20*d_e



d_c

Minimum_d_c <- d_c - 0.20*d_c
Maximum_d_c <- d_c + 0.20*d_c

# 
# df_params_OWSA <- data.frame(
#   pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A1D_SoC", "p_A2D_SoC", "p_A3D_SoC"),   # names of the parameters to be changed
#   min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_p_PD, Minimum_p_FD_SoC, Minimum_p_FD_Exp, Minimum_p_FA1_SoC, Minimum_p_A1F_SoC, Minimum_p_FA2_SoC, Minimum_p_A2F_SoC, Minimum_p_FA3_SoC, Minimum_p_A3F_SoC, Minimum_p_A1D_SoC, Minimum_p_A2D_SoC, Minimum_p_A3D_SoC),         # min parameter values
#   max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_p_PD, Maximum_p_FD_SoC, Maximum_p_FD_Exp, Maximum_p_FA1_SoC, Maximum_p_A1F_SoC, Maximum_p_FA2_SoC, Maximum_p_A2F_SoC, Maximum_p_FA3_SoC, Maximum_p_A3F_SoC, Maximum_p_A1D_SoC, Maximum_p_A2D_SoC, Maximum_p_A3D_SoC)          # max parameter values
# )

df_params_OWSA <- data.frame(
  pars = c("HR_FP_Exp", "HR_FP_SoC", "p_PD", "p_FD_SoC", "p_FD_Exp", "p_FA1_SoC", "p_A1F_SoC", "p_A1D_SoC", "p_FA2_SoC", "p_A2F_SoC", "p_A2D_SoC", "p_FA3_SoC", "p_A3F_SoC", "p_A3D_SoC", "c_F_SoC", "c_F_Exp", "c_P","c_AE1", "c_AE2", "c_AE3", "d_e", "d_c", "u_F", "u_P", "u_AE1", "u_AE2", "u_AE3"),   # names of the parameters to be changed
  min  = c(Minimum_HR_FP_Exp, Minimum_HR_FP_SoC, Minimum_p_PD, Minimum_p_FD_SoC, Minimum_p_FD_Exp, Minimum_p_FA1_SoC, Minimum_p_A1F_SoC, Minimum_p_A1D_SoC, Minimum_p_FA2_SoC, Minimum_p_A2F_SoC, Minimum_p_A2D_SoC, Minimum_p_FA3_SoC, Minimum_p_A3F_SoC, Minimum_p_A3D_SoC, Minimum_c_F_SoC, Minimum_c_F_Exp, Minimum_c_P, Minimum_c_AE1, Minimum_c_AE2, Minimum_c_AE3, Minimum_d_e, Minimum_d_c, Minimum_u_F, Minimum_u_P, Minimum_u_AE1, Minimum_u_AE2, Minimum_u_AE3),         # min parameter values
  max  = c(Maximum_HR_FP_Exp, Maximum_HR_FP_SoC, Maximum_p_PD, Maximum_p_FD_SoC, Maximum_p_FD_Exp, Maximum_p_FA1_SoC, Maximum_p_A1F_SoC, Maximum_p_A1D_SoC, Maximum_p_FA2_SoC, Maximum_p_A2F_SoC, Maximum_p_A2D_SoC, Maximum_p_FA3_SoC, Maximum_p_A3F_SoC, Maximum_p_A3D_SoC, Maximum_c_F_SoC, Maximum_c_F_Exp, Maximum_c_P, Maximum_c_AE1,  Maximum_c_AE2, Maximum_c_AE3, Maximum_d_e, Maximum_d_c, Maximum_u_F, Maximum_u_P, Maximum_u_AE1, Maximum_u_AE2, Maximum_u_AE3)          # max parameter values
)



# I made sure the names of the parameters to be varied and their mins and maxs are in the same order in all the brackets above in order to make sure that the min and max being applied are the min and the max of the parameter I want to consider a min and a max for.



OWSA_NMB  <- run_owsa_det(

# Arguments:
  
  params_range     = df_params_OWSA,     # dataframe with parameters for OWSA

  params_basecase  = l_params_all,       # list with all parameters

# params_basecase	

  nsamp            = 100,                # number of parameter values

# nsamp	

  FUN              = oncologySemiMarkov, # function to compute outputs

# FUN	
 

 outcomes         = c("NMB"),           # output to do the OWSA on

#  outcomes         = c("DSAICER"),           # output to do the OWSA on

# outcomes	

  strategies       = v_names_strats,       # names of the strategies

  progress = TRUE,


  n_wtp            = 20000               # extra argument to pass to FUN to specify the willingness to pay
)

owsa_tornado(owsa = OWSA_NMB, txtsize = 11)


# owsa_tornado(DSAICER,
# 
# min_rel_diff = 0.05)

# For owsa objects that contain many parameters that have minimal effect on the parameter of interest, you may want to consider producing a plot that highlights only the most influential parameters. Using the min_rel_diff argument, you can instruct owsa_tornado to exclude all parameters that fail to produce a relative change in the outcome below a specific fraction.
```

## 08.3 One-way sensitivity analysis (OWSA)

```{r}

## Defining and performing a one-way sensitivity analysis:




# code to generate max and min at 20% more and 20% less than the mean current value we assigned the parameter automatically. 

Maximum_c_F_Exp <- c_F_Exp + 0.20*c_F_Exp
Minimum_c_F_Exp <- c_F_Exp - 0.20*c_F_Exp

Maximum_c_F_Exp
Minimum_c_F_Exp


Maximum_u_F <- u_F + 0.20*u_F
Minimum_u_F <- u_F - 0.20*u_F

Maximum_u_F
Minimum_u_F


Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC


Minimum_p_FP_Exp <- p_FP_Exp - 0.20*p_FP_Exp
Maximum_p_FP_Exp <- p_FP_Exp + 0.20*p_FP_Exp


HR_FP_Exp
    
Minimum_HR_FP_Exp <- HR_FP_Exp - 0.20*HR_FP_Exp
Maximum_HR_FP_Exp <- HR_FP_Exp + 0.20*HR_FP_Exp



HR_FP_SoC
    
Minimum_HR_FP_SoC <- HR_FP_SoC - 0.20*HR_FP_SoC
Maximum_HR_FP_SoC <- HR_FP_SoC + 0.20*HR_FP_SoC









# A one-way sensitivity analysis (OWSA) can be defined by specifying the names of the parameters that are to be incuded and their minimum and maximum values.

# - here, the cost per cycle of the experimental treatment and utility for the progression-free state are included

df_params_OWSA <- data.frame(
  pars = c("c_F_Exp", "u_F", "HR_FP_Exp", "HR_FP_SoC"),   # names of the parameters to be changed
  min  = c(400,  0.70, Minimum_HR_FP_Exp, Minimum_HR_FP_SoC),         # min parameter values
  max  = c(1200, 0.90, Maximum_HR_FP_Exp, Maximum_HR_FP_SoC)          # max parameter values
)




# To vary transition probabilities is slightly complex in a time dependent model.


# FOR THE TRANSITION PROBABILITY FROM FREE TO PROGRESSION WE CAN MULTIPLY THAT PARAMETER BY 20% ONCE AND IT WILL CHANGE THE TRANSITION PROBABILITIES AT EACH CYCLE 
 
# > Minimum_p_FP_SoC <- p_FP_SoC - 0.20*p_FP_SoC
# > Maximum_p_FP_SoC <- p_FP_SoC + 0.20*p_FP_SoC

# > p_FP_SoC
#  ...0.249399924
# > Minimum_p_FP_SoC
#  ...0.1995199390
# > Maximum_p_FP_SoC
#  ...0.299279908

# 0.0499 is 20% of 0.249399924
# 0.249399924 - 0.0499 = 0.199499924
# 0.249399924 + 0.0499 = 0.299299924 

# So, this allows us to have the 20% variability on our time dependent transition probabilities


# WE DONT NEED TO HAVE A MAX (MEAN+20%) AND MIN (MEAN-20%) FOR THE HAZARD RATIO TOO IF WE HAVE THESE FOR TRANSITION PROBABILITIES FOR THE EXPERIMENTAL TREATMENT.

# There's a problem below, df_params_OWSA doesnt like the fact that a different probability for each cycle (from the time-dependent transition probabilities) gives 122 rows. 
 
# > df_params_OWSA <- data.frame(
# +   pars = c("c_F_Exp", "u_F", "p_FP_SoC", "p_FP_Exp"),   # names of the parameters to be changed
# +   min  = c(400,  0.70, Minimum_p_FP_SoC, Minimum_p_FP_Exp),         # min parameter values
# +   max  = c(1200, 0.90, Maximum_p_FP_SoC, Maximum_p_FP_Exp)          # max parameter values
# + )
# Error in data.frame(pars = c("c_F_Exp", "u_F", "p_FP_SoC", "p_FP_Exp"),  : 
#   arguments imply differing number of rows: 4, 122
# 

# To address this, I can go back to the Goldstein approach and change the hazard ratio instead. For the experimental strategy this is simple to do as there already is a hazard ratio in generating experimental strategy transition probabilities. 

# For the standard of care strategy I'll have to add a hazard ratio into oncologySemiMarkov_function that is equal to 1 and get's multiplied by the standard of care stuff first, then I can vary this by a min and a max as above.




























# We create a dataframe containing all parameters we want to do the sensitivity analysis on, and the min and max values of the parameters of interest 
# "min" and "max" are the mininum and maximum values of the parameters of interest.


# options(scipen = 999) # disabling scientific notation in R



# The OWSA is performed using the run_owsa_det function


# This function runs a deterministic one-way sensitivity analysis (OWSA) on a given function that produces outcomes. rdrr.io/github/DARTH-git/dampack/src/R/run_dsa.R


OWSA_NMB  <- run_owsa_det(
# run_owsa_det: https://rdrr.io/github/DARTH-git/dampack/man/run_owsa_det.html

# Arguments:
  
  params_range     = df_params_OWSA,     # dataframe with parameters for OWSA

# params_range	
# data.frame with 3 columns of parameters for OWSA in the following order: "pars", "min", and "max".
# The number of samples from this range is determined by nsamp. 
# "pars" are the parameters of interest and must be a subset of the parameters from params_basecase.


# Details
# params_range
 
# "pars" are the names of the input parameters of interest. These are the parameters that will be varied in the deterministic sensitivity analysis. variables in "pars" column must be a subset of variables in params_basecase
 

  
  params_basecase  = l_params_all,       # list with all parameters

# params_basecase	
# a named list of basecase values for input parameters needed by FUN, the user-defined function. So, I guess it takes the values that the parameters are equal to in l_params_all as the base case, so if cost is generated equal to 1,000 it'll take that as the base case, and then take the min and the max around this from the data.frame we created above.




  nsamp            = 100,                # number of parameter values

# nsamp	
# number of sets of parameter values to be generated. If NULL, 100 parameter values are used -> I think Eva Enns said these are automatically evenly spaced out values of the parameters.

  FUN              = oncologySemiMarkov, # function to compute outputs

# FUN	
# function that takes the basecase in params_basecase and runs the analysis in the function... to produce the outcome of interest. The FUN must return a dataframe where the first column are the strategy names and the rest of the columns must be outcomes.
# 


  outcomes         = c("NMB"),           # output to do the OWSA on

# outcomes	
# string vector with the outcomes of interest from FUN produced by nsamp
# This basically tells run_owsa_det what the name of the outcome of interest from the function we fed it is. Here our function had NMB, i.e., the net monetary benefit.

  strategies       = v_names_strats,       # names of the strategies

# strategies	
# Set it equal to a vector of strategy names. The default NULL will use strategy names in FUN (  strategies = NULL,)
# Here that's "Standard of Care" and "Experimental Treatment".

progress = TRUE,

# progress	
# TRUE or FALSE for whether or not function progress should be displayed in console, i.e., like 75% complete, 100%, etc.,



  n_wtp            = 20000               # extra argument to pass to FUN to specify the willingness to pay
)

# Value
# A list containing dataframes with the results of the sensitivity analyses. The list will contain a dataframe for each outcome specified. List elements can be visualized with plot.owsa, owsa_opt_strat and owsa_tornado from dampack

# Basically, run_owsa_det creates the above.






 
# Resources on this available here:


# https://rdrr.io/github/DARTH-git/dampack/man/run_owsa_det.html (also saved here: C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\sensitivity analysis help files\rdrr-io-github-DARTH-git-dampack-man-run_owsa_det-html.pdf)





owsa_tornado(owsa = OWSA_NMB, txtsize = 11)










```

## 08.3.1 Plot OWSA

```{r}
plot(OWSA_NMB, txtsize = 10, n_x_ticks = 4, 
     facet_scales = "free") +
  theme(legend.position = "bottom")
```

## 08.3.2 Optimal strategy with OWSA

```{r}
owsa_opt_strat(owsa = OWSA_NMB, txtsize = 10)
```

## 08.3.3 Tornado plot

```{r}
owsa_tornado(owsa = OWSA_NMB, txtsize = 11)

# Plotting the outcomes of the OWSA in a tornado plot
# - note that other plots can also be generated using the plot() and owsa_opt_strat() functions
```

## 08.3.4 Table Describing Parameters

To build a Table describing parameters, you click on the Table button at the top, and design it in there.
Then to include the parameter values automatically as they are updated you fill in the parameter name you want, highlight it in the cell and click \</> from above to put it in a code block.
You do the same for gamma, etc., put this in code blocks also.

Per: [Putting the value of a variable into a table in R Markdown, rather than it's name - Stack Overflow](https://stackoverflow.com/questions/72902548/putting-the-value-of-a-variable-into-a-table-in-r-markdown-rather-than-its-nam)

Line 320 of this: <https://github.com/DARTH-git/cohort-modeling-tutorial-intro/blob/main/manuscript/cSTM_Tutorial_Intro.Rmd> is informative when viewing the final document on page 7 here: <https://arxiv.org/pdf/2001.07824.pdf>

This also includes within-cycle correction (WCC) using Simpson's 1/3 rule, which I probably don't want to bother applying, but at least I know it's an option.

Some helpful tips on using math notation in R Markdown.

<https://rpruim.github.io/s341/S19/from-class/MathinRmd.html>

This paper says in technical terms which distributions fit and why:

Model Parameter Estimation and Uncertainty Analysis: A Report of the ISPOR-SMDM Modeling Good Research Practices Task Force Working Group--6 <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

## 08.4 Two-way sensitivity analysis (TWSA)

A 2-way uncertainty analysis will be more useful if informed by the covariance between the 2 parameters of interest or on the logical relationship between them (e.g., a 2-way uncertainty analysis might be represented by the control intervention event rate and the hazard ratio with the new treatment).
Representation of uncertaint <file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

**Describing two-way sensitivity analysis:**

"Results of 2- and multiway uncertainty analysis require graphical or tabular displays (Figure 2). The axes represent possible values, and the quadrant is partitioned into regions corresponding to various ICERs, the boundaries representing specified ICER thresholds, or thresholds of dominance. As in 1- way analyses, it is important to specify which alternative dominates and which comparator is more effective and costly when an ICER threshold is indicated. Three-way threshold analyses may be superimposed on 2-way graphs by overlaying threshold curves (Figure 3), but this often leads to visual overload and confusion and will work only if the third parameter can be represented as taking on discrete values. Threshold analyses are especially useful, perhaps necessary, when reporting DSA involving 3 or more comparators. In those situations, the relevant question may be,''Which alternative is cost-effective at a threshold of X?'' To portray the answer for 2 parameters, partition the quadrant to show which alternative is cost-effective at various ICER thresholds and for different combinations of parameters (Figures 4A and 4B). Results for different decision criteria (e.g., ICER thresholds or dominance) are best presented in separate panels of a graphical display. *When the base-case result of an analysis strongly favors one alternative, a threshold analysis may be presented as a worst-case or ''even if'' analysis (e.g., ''Even if the risk reduction is as low as X, the ICER remains below Y,'' or ''Even if the relative risk reduction with alternative A is as low as X and the cost of treatment is as high as Y, alternative A dominates B'').* Threshold values can easily be combined with the tornado presentation by marking them on the horizontal bars."

<file:///C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf>

```{r}

#' Run deterministic two-way sensitivity analysis (TWSA) https://rdrr.io/github/DARTH-git/dampack/src/R/run_dsa.R


# dataframe containing all parameters, their basecase values, and the min and 
# max values of the parameters of interest
df_params_twsa <- data.frame(pars = c("c_trtA", "c_trtB"),
                             min  = c(300, 500),  # min parameter values
                             max  = c(1200, 2000) # max parameter values
)

twsa_nmb <- run_twsa_det(params_range    = df_params_twsa,    # dataframe with parameters for TWSA
                         params_basecase = l_params_all,      # list with all parameters
                         nsamp           = 40,                # number of parameter values
                         FUN             = calculate_ce_out,  # function to compute outputs
                         outcomes        = "NMB",             # output to do the TWSA on
                         strategies      = v_names_str,       # names of the strategies
                         n_wtp           = 5000)              # extra argument to pass to FUN
```

## 08.4.1 Plot TWSA

```{r}
plot(twsa_nmb)
```

# 09 Probabilistic Sensitivity Analysis (PSA)

Go back to the resources and papers you reviewed for the deterministic sensitivity analysis.

In probabilistic sensitivity analysis, parameters were varied simultaneously according to their sampling distributions.
We used g distribution for cost parameters, b distribution for parameters bounded on the 0 to 1 interval (eg, AE incidence estimates and utilities), and the uniform distribution for the discount factor.
The range for the discount factor was 0.00 to 0.05.
In multivariate analysis, we ran 10,000 replications in the probabilistic sensitivity analysis.
The baseline values, ranges, and distributions for model parameters are presented in Table 4.
Per: Cost Effectiveness Analysis of Pharmacokinetically-Guided 5-Fluorouracil in FOLFOX Chemotherapy for Metastatic Colo .
<file:///C:/Users/jonathanbriody/Downloads/goldstein2014.pdf>

```{r}

 
# According to:
#
# C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\Health Economic Modeling in R A Hands-on Introduction\Health-Eco\Markov models\markov_smoking_probabilistic.R
# 
# 
# It is OK to do the following:
# 
# # QALY associated with 1-year in the smoking state is Normal(mean = 0_95, SD = 0_01)
# # Divide by 2 as cycle length is 6 months
# state_qalys[, "Smoking"] <- rnorm(n_samples, mean = 0.95, sd = 0.01) / 2
#   
# So, when doing probabilistic sensitivity analysis and I need my mean and sd for the method of moments this may be useful information when drawing these from population norms.
# This will also be useful for utility and PSA in the adverse event setting.




# Function to generate PSA input dataset
gen_psa <- function(n_sim = 1000, seed = 071818){
  set.seed(seed) # set a seed to be able to reproduce the same results
  df_psa <- data.frame(
    # Transition probabilities (per cycle), conditional on surviving
    # probability to become PFS when OS
    # probability of dying when OS
    p_HD       = rbeta(n_sim, shape1 = 4,  shape2 = 391),
    p_HS_SoC   = rbeta(n_sim, shape1 = 24, shape2 = 450),  # under standard of care
    p_HS_trtA  = rbeta(n_sim, shape1 = 15, shape2 = 368),  # under treatment A
    p_HS_trtB  = rbeta(n_sim, shape1 = 16, shape2 = 767),  # under treatment B
    
    # probability of dying when PFS
    p_SD       = rbeta(n_sim, shape1 = 22.4, shape2 = 201.6), 
    
    # Cost vectors with length n_sim
    # cost of remaining one cycle in state H
    c_H        = rgamma(n_sim, shape = 16, scale = 25), 
    # cost of remaining one cycle in state S1
    c_S        = rgamma(n_sim, shape = 100, scale = 10), 
    # cost of being in the death state
    c_D        = 0, 
    # cost of treatment (per cycle)
    c_trtA    = rgamma(n_sim, shape = 64, scale = 12.5),
    # cost of treatment (per cycle)
    c_trtB    = rgamma(n_sim, shape = 225, scale = 6.67),
    
    # Utility vectors with length n_sim 
    # utility when OS
    u_H        = rbeta(n_sim, shape1 =  1.5, shape2 = 0.0015), 
    # utility when PFS
    u_S        = rbeta(n_sim, shape1 = 49.5, shape2 = 49.5), 
    # utility when dead
    u_D        = 0                                              
  )
  return(df_psa)
}


# Try it
gen_psa(10) 

# Number of simulations
n_sim <- 1000

# Generate PSA input dataset
df_psa_input <- gen_psa(n_sim = n_sim)
# First six observations
head(df_psa_input)

# Save the dataframe
# save dataframe
#save(df_psa_input, file = "df_psa_input.rda")


# Histogram of parameters
ggplot(melt(df_psa_input, variable.name = "Parameter"), aes(x = value)) +
  facet_wrap(~Parameter, scales = "free") +
  geom_histogram(aes(y = ..density..)) +
  theme_bw(base_size = 16) + 
  theme(axis.text = element_text(size=8))

# Initialize dataframes with PSA output 
# Dataframe of costs
df_c <- as.data.frame(matrix(0, 
                             nrow = n_sim,
                             ncol = n_str))
colnames(df_c) <- v_names_str
# Dataframe of effectiveness
df_e <- as.data.frame(matrix(0, 
                             nrow = n_sim,
                             ncol = n_str))
colnames(df_e) <- v_names_str
```

I need to fix the "# Histogram of parameters" section above, the below gives advice on doing this:

<https://stackoverflow.com/questions/68416435/rcpp-package-doesnt-include-rcpp-precious-remove>

[https://www.mail-archive.com/rcpp-devel\@lists.r-forge.r-project.org/msg10226.html](https://www.mail-archive.com/rcpp-devel@lists.r-forge.r-project.org/msg10226.html){.uri}

<https://statisticsglobe.com/warning-cannot-remove-prior-installation-in-r>

The above approach worked, I manually deleted the package, installed darthtools from github, chose option 4, i.e. replace rccp and then I was done.

## 09.1 Conduct probabilistic sensitivity analysis

We apply deterministic methods (point estimates with an appropriate range) and probabilistic methods (parameterized distributions) to conduct a sensitivity analysis of our results.
C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf

```{r}
# Run Markov model on each parameter set of PSA input dataset
for(i in 1:n_sim){
  l_out_temp <- calculate_ce_out(df_psa_input[i, ])
  df_c[i, ] <- l_out_temp$Cost
  df_e[i, ] <- l_out_temp$Effect
  # Display simulation progress
  if(i/(n_sim/10) == round(i/(n_sim/10), 0)) { # display progress every 10%
    cat('\r', paste(i/n_sim * 100, "% done", sep = " "))
  }
}
```

## 09.2 Create PSA object for dampack

```{r}
l_psa <- make_psa_obj(cost          = df_c, 
                      effectiveness = df_e, 
                      parameters    = df_psa_input, 
                      strategies    = v_names_str)
```

## 09.2.1 Save PSA objects

```{r}
save(df_psa_input, df_c, df_e, v_names_str, n_str, l_psa,
     file = "markov_3state_PSA_dataset.RData")
```

Vector with willingness-to-pay (WTP) thresholds.

```{r}
v_wtp <- seq(0, 30000, by = 1000)
```

## 09.3.1 Cost-Effectiveness Scatter plot

```{r}
plot(l_psa, xlim = c(9.5, 22.5))
```

## 09.4 Conduct CEA with probabilistic output

```{r}
# Compute expected costs and effects for each strategy from the PSA
df_out_ce_psa <- summary(l_psa)

# Calculate incremental cost-effectiveness ratios (ICERs)
df_cea_psa <- calculate_icers(cost       = df_out_ce_psa$meanCost, 
                              effect     = df_out_ce_psa$meanEffect,
                              strategies = df_out_ce_psa$Strategy)
df_cea_psa

# Save CEA table with ICERs
# As .RData
save(df_cea_psa, 
     file = "markov_3state_probabilistic_CEA_results.RData")
# As .csv
write.csv(df_cea_psa, 
          file = "markov_3state_probabilistic_CEA_results.csv")
```

## 09.4.1 Plot cost-effectiveness frontier

```{r}
plot(df_cea_psa)
```

## 09.4.2 Cost-effectiveness acceptability curves (CEACs) and frontier (CEAF)

```{r}
ceac_obj <- ceac(wtp = v_wtp, psa = l_psa)
# Regions of highest probability of cost-effectiveness for each strategy
summary(ceac_obj)
# CEAC & CEAF plot
plot(ceac_obj)
```

## 09.4.3 Expected Loss Curves (ELCs)

The expected loss is the the quantification of the foregone benefits when choosing a suboptimal strategy given current evidence.

```{r}
elc_obj <- calc_exp_loss(wtp = v_wtp, psa = l_psa)
elc_obj
# ELC plot
plot(elc_obj, log_y = FALSE)
```

## 09.4.4 Expected value of perfect information (EVPI)

Value of information is discussed in the York course and below:

C:\Users\Jonathan\OneDrive - Royal College of Surgeons in Ireland\COLOSSUS\Training Resources\CDC\_Exclusive_Decision Modeling for Public Health_DARTH

```{r}
evpi <- calc_evpi(wtp = v_wtp, psa = l_psa)
# EVPI plot
plot(evpi, effect_units = "QALY")
```

## 09.4.5 Scenario Analysis

> The below study includes scenario analysis at the end:
>
> Rivera, F., Valladares, M., Gea, S., & López-Martínez, N.
> (2017).
> Cost-effectiveness analysis in the Spanish setting of the PEAK trial of panitumumab plus mFOLFOX6 compared with bevacizumab plus mFOLFOX6 for first-line treatment of patients with wild-type RAS metastatic colorectal cancer.
> Journal of Medical Economics, 20(6), 574-584.
> <https://sci-hub.st/10.1080/13696998.2017.1285780> also saved here: C:\\Users\\Jonathan\\OneDrive - Royal College of Surgeons in Ireland\\COLOSSUS\\Evidence Synthesis\\Economic Models\\Rivera et al_2017_Cost-effectiveness analysis in the Spanish setting of the PEAK trial of.pdf

Scenario Analysis: "Often, uncertainty in a parameter may be represented by several discrete values, instead of a continuous range, sometimes called scenario analyses (e.g., evidence from clinical studies, utility surveys, or cost data sets may lead to different values). It is acceptable to report alternative outcomes under each of these discrete assumptions to complement other uncertainty analysis." - Per: Model Parameter Estimation and Uncertainty Analysis C:/Users/Jonathan/OneDrive%20-%20Royal%20College%20of%20Surgeons%20in%20Ireland/COLOSSUS/Briggs%20et%20al%202012%20model%20parameter%20estimation%20and%20uncertainty.pdf

## 09.4.6 Calibration

> The following has a calibration section: Lawrence, D., Maschio, M., Leahy, K. J., Yunger, S., Easaw, J. C., & Weinstein, M. C.
> (2013).
> Economic analysis of bevacizumab, cetuximab, and panitumumab with fluoropyrimidine-based chemotherapy in the first-line treatment of KRAS wild-type metastatic colorectal cancer (mCRC).
> Journal of medical economics, 16(12), 1387-1398.
> <https://www.tandfonline.com/doi/pdf/10.3111/13696998.2013.852097?needAccess=true>

## References:

Generated automatically when the document is knitted.

Useful Darth publications to cite when using this code:

-   Jalal H, Pechlivanoglou P, Krijkamp E, Alarid-Escudero F, Enns E, Hunink MG. An Overview of R in Health Decision Sciences.
    Med Decis Making.
    2017; 37(3): 735-746.
    <https://journals.sagepub.com/doi/abs/10.1177/0272989X16686559>

-   Alarid-Escudero F, Krijkamp EM, Enns EA, Yang A, Hunink MGM Pechlivanoglou P, Jalal H. Cohort State-Transition Models in R: A Tutorial.
    arXiv:200107824v2.
    2020:1-48.
    <http://arxiv.org/abs/2001.07824>

-   Krijkamp EM, Alarid-Escudero F, Enns EA, Jalal HJ, Hunink MGM, Pechlivanoglou P. Microsimulation modeling for health decision sciences using R: A tutorial.
    Med Decis Making.
    2018;38(3):400--22.
    <https://journals.sagepub.com/doi/abs/10.1177/0272989X18754513>

-   Krijkamp EM, Alarid-Escudero F, Enns E, Pechlivanoglou P, Hunink MM, Jalal H. A Multidimensional Array Representation of State-Transition Model Dynamics.
    Med Decis Making.
    Online First <https://doi.org/10.1177/0272989X19893973>
